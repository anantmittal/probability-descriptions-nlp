             Stat Trek  Teach yourself statistics                Home    Tutorials    AP statistics    Stat tables    Stat tools    Calculators    Books    Help        Overview  AP statistics  Beyond AP statistics  Simple linear regression  Probability sampling  Matrix algebra    AP tutorial  Test preparation  Practice test  AP formulas  FAQ  AP study guides  AP calculators    Binomial  Chi-square  f Dist  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Dist    Random numbers  Probability  Bayes rule  Combinations/permutations  Factorial  Event counter  Sample planning    Graphing  Scientific  Financial    Statistics  AP study guides  Probability  Survey sampling    Statistics dictionary  Problems and solutions  Formulas  Notation               Home    Tutorials   Overview  AP statistics  Beyond AP statistics  Simple Linear regression  Probability sampling  Matrix algebra     AP statistics   Test preparation  AP tutorial  Practice test  AP formulas  FAQ  AP study guides  AP calculators     Statistical tables   Binomial  Chi-square  f Distribution  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Distribution     Statistical tools   Random number generator  Probability calculator  Bayes rule calculator  Combinations/permutations  Factorial calculator  Event counter  Sample planning wizard     Handheld calculators   Graphing  Scientific  Financial     Books   Statistics  AP study guides  Probability  Survey sampling     Help   Statistics dictionary  Problems and solutions  Formulas  Notation                       Beyond AP Statistics   Probability Basics   Sets and subsets  Statistical experiment  Counting data points  Probability problems  Bayes theorem   Small Samples   Estimate proportion  Test proportion   Distributions   Probability distribution  Discrete/continuous  Hypergeometric  Multinomial  Poisson  f Distribution   Power   Region of acceptance  Power of a test   How to find power            Beyond AP Statistics   Probability Basics   Sets and Subsets  Statistical Experiments  Counting Data Points  Probability Problems  Bayes Theorem   Small Samples   Estimate Proportion  Test Proportion   Distributions   Probability Distribution  Discrete/Continuous  Hypergeometric  Multinomial  Poisson  f Distribution   Power   Region of Acceptance  Power of a Test   How to Find Power        What is a Probability Distribution?  A probability distribution is a table or an equation that links each outcome 
    of a statistical experiment with its probability of occurrence.  Probability Distribution Prerequisites  To understand probability distributions, it is important to understand variables. 
	    random variables, and some notation.   A variable is a symbol ( A , B , x , y , 
	    etc.) that can take on any of a specified set of values.  When the value of a variable is the outcome of a statistical experiment , that variable is a random variable .   Generally, statisticians use a capital letter to represent a random variable and 
	    a lower-case letter, to represent one of its values. For example,   X represents the random variable X.  P(X) represents the probability of X.  P(X = x) refers to the probability that the random variable X is equal to a 
		    particular value, denoted by x. As an example, P(X = 1) refers to the 
		    probability that the random variable X is equal to 1.   Probability Distributions  An example will make clear the relationship between random variables and 
        probability distributions. Suppose you flip a coin two times. This simple 
	    statistical experiment can have four possible outcomes: HH, HT, TH, and TT. 
	    Now, let the variable X represent the number of Heads that result from this 
	    experiment. The variable X can take on the values 0, 1, or 2. In this example, 
	    X is a random variable; because its value is determined by the outcome of a 
	    statistical experiment.  A probability distribution is a table or an equation that links 
	    each outcome of a statistical experiment with its probability of occurrence. 
	    Consider the coin flip experiment described above. The table below, which 
	    associates each outcome with its probability, is an example of a probability 
	    distribution.    Number of heads  Probability    0  0.25    1  0.50    2  0.25    The above table represents the probability distribution of the random variable 
	    X.        Cumulative Probability Distributions  A cumulative probability refers to the probability that the 
	    value of a random variable falls within a specified range.  Let us return to the coin flip experiment. If we flip a coin two times, we might 
	    ask: What is the probability that the coin flips would result in one or fewer 
	    heads? The answer would be a cumulative probability. It would be the 
	    probability that the coin flip experiment results in zero heads plus the 
	    probability that the experiment results in one head.  P(X < 1) = P(X = 0) + P(X = 1) = 0.25 + 0.50 = 0.75  Like a probability distribution, a cumulative probability distribution can be 
	    represented by a table or an equation. In the table below, the cumulative 
	    probability refers to the probability than the random variable X is less than 
	    or equal to x.    Number of heads: x  Probability: P(X = x) Cumulative Probability: P(X < x)  0  0.25  0.25   1  0.50  0.75   2  0.25  1.00      Uniform Probability Distribution The simplest probability distribution occurs when all of the values of a 
	    random variable occur with equal probability. This probability 
	    distribution is called the uniform distribution .  Uniform Distribution. Suppose the 
		    random variable X can assume k different values. Suppose also that the P(X = x k ) 
		    is constant. Then,  P(X = x k ) = 1/k  Example 1 Suppose a die is tossed. What is the probability that the die will land on 5? Solution: When a die is tossed, there are 6 possible outcomes represented 
	    by: S = { 1, 2, 3, 4, 5, 6 }. Each possible outcome is a random variable (X), 
	    and each outcome is equally likely to occur. Thus, we have a uniform 
	    distribution. Therefore, the P(X = 5) = 1/6. Example 2 Suppose we repeat the dice tossing experiment described in Example 1. This 
	    time, we ask what is the probability that the die will land on a number that is 
	    smaller than 5? Solution: When a die is tossed, there are 6 possible outcomes represented 
	    by: S = { 1, 2, 3, 4, 5, 6 }. Each possible outcome is equally likely to occur. 
	    Thus, we have a uniform distribution. This problem involves a cumulative probability. The probability that the die 
	    will land on a number smaller than 5 is equal to: P( X < 5 ) = P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4) P( X < 5 ) = 1/6 + 1/6 + 1/6 + 1/6 = 2/3           Bestsellers Handheld Calculators Updated daily 1. Texas Instruments TI-84 Plus CE Lightning Graphing Calculator $150.00  $150.00 2. Texas Instruments Ti-84 plus Graphing calculator - Black $108.99  $108.99 3. Texas Instruments VOY200/PWB Graphing Calculator $200.00  4. Sharp EL-W535B WriteView Scientific Calculator $24.99  $39.99 Today's Bargain Book Understandable Statistics $319.95  $65.00  80% off See more Statistics books ... Bestsellers Statistics and Probability Updated daily 1. Barron's AP Statistics, 9th Edition $18.99  $12.91 2. Naked Statistics: Stripping the Dread from the Data $16.95  $13.70 3. Statistics For Dummies (For Dummies (Math & Science)) $19.99  $13.11 4. How to Lie with Statistics $13.95  $8.98           About  Contact  Privacy  Terms  Advertising    Advertise on Stat Trek  Copyright © 2018 StatTrek.com             Search       Statistics How To   Statistics for the rest of us!       Home  Tables   Binomial Distribution Table  F Table  PPMC Critical Values  T-Distribution Table (One Tail and Two-Tails)  Chi Squared Table (Right Tail)  Z-table (Right of Curve or Left)    Probability and Statistics   Binomials  Chi-Square Statistic  Expected Value  Hypothesis Testing  Non Normal Distribution  Normal Distributions  Probability  Regression Analysis  Statistics Basics  T-Distribution  Multivariate Analysis  Sampling    Calculators   Variance and Standard Deviation Calculator  Tdist Calculator  Permutation Calculator / Combination Calculator  Interquartile Range Calculator  Linear Regression Calculator  Expected Value Calculator  Binomial Distribution Calculator    Statistics Blog  Calculus   Derivatives  Integrals  Limits    Matrices  Experimental Design  Practically Cheating Statistics Handbook  Navigation             Find the Mean of the Probability Distribution / Binomial    Binomial Theorem > How to find the mean of the probability distribution  Contents:   Mean of a probability distribution  Mean of a binomial (by hand or TI83)   Mean of a probability distribution How to find the mean of the probability distribution : Overview A normal distribution curve is one kind of probability distribution. Finding the mean of a probability distribution is easy in probability and statistics — if you know how. This how to will guide you through a few simple steps necessary to find the mean of the probability distribution or binomial distribution . You’ll often find these types of questions in textbook chapters on binomial probability distribution. The binomial distribution is just a simple trial where there are two outcomes: success or failure. For example, if you are counting how many times you draw an Ace from a deck of cards, you could assign “Success” to “Drawing an Ace” and “Failure” to drawing any other card. You can find the mean of the probability distribution by creating a probability table. How to find the mean of the probability distribution: Steps Sample question : “A grocery store has determined that in crates of tomatoes,  95% carry no rotten tomatoes, 2% carry one rotten tomato, 2% carry two rotten tomatoes, and 1% carry three rotten tomatoes. Find the mean number of rotten tomatoes in the crates.”  Step 1:  Convert all the percentages to decimal probabilities . For example: 95% = .95 2% = .02 2% = .02 1% = .01  Step 2:  Construct a probability distribution table . (If you don’t know how to do this, see how to construct a probability distribution ) .)   Step 3:  Multiply the values in each column. (In other words, multiply each value of X by each probability P(X).) Referring to our probability distribution table: 0 × .95 = 0 1 × .02  = .02 2 × .02  = .04 3 × .01 = .03  Step 4:  Add the results from step 3 together . 0 + .02 + .04 + .03 = .09 is the mean.  You’re done finding the mean for a probability distribution! Mean of Binomial Distribution A coin toss is a simple binomial experiment . A binomial distribution represents the results from a simple experiment where there is “success” or “failure.” For example, if you are polling voters to see who is voting Democrat, the voters that say they will vote Democrat is a “success” and anything else is a failure. One of the simplest binomial experiments you can perform is a coin toss, where “heads” could equal “success” and “tails” could equal “failure.” The mean of binomial distribution is much like the mean of anything else. It answers the question “If you perform this experiment many times, what’s the likely (the average) result?. Formula for Mean of Binomial Distribution The formula for the mean of binomial distribution is:  μ = n *p Where “n” is the number of trials and “p” is the probability of success. For example: if you tossed a coin 10 times to see how many heads come up, your probability is .5 (i.e. you have a 50 percent chance of getting a heads and 50 percent chance of a tails) and “n” is how many trials — 10. Therefore, the mean of this particular binomial distribution is: 10 * .5 = 5. This makes sense: if you toss a coin ten times you would expect heads to show up on average, 5 times. Mean for a Binomial Distribution on the TI-83 Sample problem : Find the mean for a binomial distribution with n = 5 and p = 0.12. Again, the TI 83 doesn’t have a function for this. But if you know the formula (n*p), it’s pretty easy to enter it on the home screen.  Step 1: Multiply n by p. 5 * .12 ENTER  =.6 Hey, that was easy! Something to think about: You may be wondering why it was so easy to calculate the mean. After all being asked to “calculate the mean for a binomial distribution” sounds scary. If you think about what a mean (or average ) is, then you’ll see why it was so easy. In the sample question, n = 5 and p = 0.12. What is “n”? That’s the number of items. So imagine a list of 5 items with a certain score: 1 = 0.12 2 = 0.12 3 = 0.12 4 = 0.12 5 = 0.12 If you were asked to find the average score for those five items, you wouldn’t even have to do the math: it’s just 0.12, right? Finding the mean for a binomial distribution is just a little different: you add up all of the probabilities (0.12 + 0.12 + 0.12 + 0.12 + 0.12). Or a faster way, just multiply n by p. Check out our Youtube channel for more help and stats tips!     If you prefer an online interactive environment to learn R and statistics, this free R Tutorial by Datacamp is a great way to get started. If you're are somewhat comfortable with R and are interested in going deeper into Statistics, try this Statistics with R track . Facebook page Find the Mean of the Probability Distribution / Binomial was last modified: October 15th, 2017 by Stephanie   By Stephanie | August 24, 2009 | Statistics How To |   ← Construct a probability distribution in Easy Steps  Binomial experiment: How to figure out what is and what isn’t →   4 thoughts on “ Find the Mean of the Probability Distribution / Binomial ”      Selin Asar May 30, 2016 at 9:20 am    A building venice is designed by taking 20  years return period water level. a) Calculate the probability of water level exceeding the design value less than 2 times in 20 years with Binomial distrubition. b)Calculate the probability of water level exceeding the design value more than 2 times in 60 years with Poisson distrubition.          Andale May 30, 2016 at 10:39 am    Were you given any probabilities for the water level exceeding the design value? Without this info, the question is impossible to answer.          M N Choudhary May 5, 2017 at 3:48 am    Assume a banks 95% value at risk model is perfectly accurate,If daily losses are independent, what is the probability that the number of daily losses exceeds the Var on exactly 5 days out of the previous 100 trading days.          Madison September 27, 2017 at 7:17 pm    This helps so much! Thanks!         Find an article   Search       Feel like "cheating" at Statistics? Check out the grade-increasing book that's recommended reading at top universities!                 Privacy policy.     Copyright © 2018 Statistics How To Theme by: Theme Horse Powered by: WordPress    Back to Top             Search       Statistics How To   Statistics for the rest of us!       Home  Tables   Binomial Distribution Table  F Table  PPMC Critical Values  T-Distribution Table (One Tail and Two-Tails)  Chi Squared Table (Right Tail)  Z-table (Right of Curve or Left)    Probability and Statistics   Binomials  Chi-Square Statistic  Expected Value  Hypothesis Testing  Non Normal Distribution  Normal Distributions  Probability  Regression Analysis  Statistics Basics  T-Distribution  Multivariate Analysis  Sampling    Calculators   Variance and Standard Deviation Calculator  Tdist Calculator  Permutation Calculator / Combination Calculator  Interquartile Range Calculator  Linear Regression Calculator  Expected Value Calculator  Binomial Distribution Calculator    Statistics Blog  Calculus   Derivatives  Integrals  Limits    Matrices  Experimental Design  Practically Cheating Statistics Handbook  Navigation             Construct a probability distribution in Easy Steps    Probability and Statistics > Probability > Construct a probability distribution   Watch the video or read the steps below:   Construct a probability distribution: Overview  A normal distribution curve is one kind of probability distribution. A probability distribution is basically a chart of what the probability of an event happening is. It’s a way of quickly viewing the event, and the probability of that event. Probability distribution charts can get quite complex in statistics. For example, a normal distribution or t-distribution chart usually requires some form of technology (like Microsoft Excel or the TI-83 calculator ) to create. However, you can construct a basic probability distribution showing events and probabilities in a few easy steps.  Construct a probability distribution: Steps  Sample question : Construct a probability distribution for the following scenario: the probability of a sausage making machine producing 0, 1, 2, 3, 4, or 5 misshapen sausages per day are 0.09, 0.07, 0.1, 0.04,0.12, and 0.02.   Step 1:  Write down the number of widgets (things, items, products or other named thing) given on one horizontal line . In this case, the widgets in this question are the “misshapen sausages”.  Making the first line of the probability distribution chart.  Step 2:  Directly underneath the first line, write the probability of the event happening.   The probability of the sausage machine producing 0 misshapen sausages a day is 0.09. Write “0.09” directly under “0”.  The probability of the sausage machine producing 1 misshapen sausages a day is 0.07. Write “0.07” directly under “1”.  The probability of the sausage machine producing 2 misshapen sausages a day is 0.1. Write “0.1” directly under “2”.  The probability of the sausage machine producing 3 misshapen sausages a day is 0.04. Write “0.04” directly under “3”.  The probability of the sausage machine producing 4 misshapen sausages a day is 0.12. Write “0.12” directly under “4”.  The probability of the sausage machine producing 5 misshapen sausages a day is 0.02. Write “0.02” directly under “5”.     That’s how to Construct a probability distribution!  Check out our YouTube Channel for hundreds more statistics how to videos!      ------------------------------------------------------------------------------ If you prefer an online interactive environment to learn R and statistics, this free R Tutorial by Datacamp is a great way to get started. If you're are somewhat comfortable with R and are interested in going deeper into Statistics, try this Statistics with R track . Comments are now closed for this post. Need help or want to post a correction? Please post a comment on our Facebook page and I'll do my best to help! Construct a probability distribution in Easy Steps was last modified: October 12th, 2017 by Stephanie     By Stephanie | August 24, 2009 | Statistics How To |    ← Probability of selecting a person from a group or committee  Find the Mean of the Probability Distribution / Binomial →         Find an article   Search       Feel like "cheating" at Statistics? Check out the grade-increasing book that's recommended reading at top universities!                 Privacy policy.       Copyright © 2018 Statistics How To Theme by: Theme Horse Powered by: WordPress    Back to Top                   Stat Trek  Teach yourself statistics                Home    Tutorials    AP statistics    Stat tables    Stat tools    Calculators    Books    Help        Overview  AP statistics  Beyond AP statistics  Simple linear regression  Probability sampling  Matrix algebra    AP tutorial  Test preparation  Practice test  AP formulas  FAQ  AP study guides  AP calculators    Binomial  Chi-square  f Dist  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Dist    Random numbers  Probability  Bayes rule  Combinations/permutations  Factorial  Event counter  Sample planning    Graphing  Scientific  Financial    Statistics  AP study guides  Probability  Survey sampling    Statistics dictionary  Problems and solutions  Formulas  Notation               Home    Tutorials   Overview  AP statistics  Beyond AP statistics  Simple Linear regression  Probability sampling  Matrix algebra     AP statistics   Test preparation  AP tutorial  Practice test  AP formulas  FAQ  AP study guides  AP calculators     Statistical tables   Binomial  Chi-square  f Distribution  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Distribution     Statistical tools   Random number generator  Probability calculator  Bayes rule calculator  Combinations/permutations  Factorial calculator  Event counter  Sample planning wizard     Handheld calculators   Graphing  Scientific  Financial     Books   Statistics  AP study guides  Probability  Survey sampling     Help   Statistics dictionary  Problems and solutions  Formulas  Notation                      AP Statistics Tutorial   Exploring Data    The basics   Variables  Population vs sample  Mean and median  Variability  Position     Charts and graphs   Patterns in data  Dotplots  Histograms  Stemplots  Boxplots  Cumulative plots  Scatterplots  Comparing data sets     Regression   Measurement scales  Linear correlation  Linear regression  Regression example  Residual analysis  Transformations  Influential points     Categorical data   One-way tables  Two-way tables     Experimentation    Surveys   Data collection  Sampling methods  Bias in surveys     Experiments   Intro to experiments  Experimental design     Anticipating Patterns    Probability   Probability intro  Rules of probability     Random variables   Introduction  Distributions  Mean and variance  Independence  Combinations  Transformations  Simulation of events     Discrete variables   Binomial distribution  Negative binomial     Continuous variables   Normal distribution  Standard normal  t Distribution  Chi-square     Sampling distributions   Sampling distribution  Diff between props  Diff between means     Statistical Inference    Estimation   Estimation problems  Standard error  Margin of error  Confidence interval     Confidence intervals   Proportions  Diff between props  Means  Diff between means  Diff between pairs  Regression slope     Hypothesis testing   Hypothesis test intro  Power of test  How to test     Hypothesis tests   Proportions  Diff between props  Mean  Diff between means  Diff between pairs  Goodness of fit test  Homogeneity  Independence  Regression slope     Appendices    ■ Practice exam    ■ Notation    ■ AP stat formulas             AP Statistics Lessons   The basics   Variables  Population vs sample  Mean and median  Variability  Position   Charts and graphs   Patterns in data  Dotplots  Histograms  Stemplots  Boxplots  Cumulative plots  Scatterplots  Comparing data sets   Regression   Measurement scales  Linear correlation  Linear regression  Regression example  Residual analysis  Transformations  Influential points   Categorical data   One-way tables  Two-way tables   Surveys   Data collection  Sampling methods  Bias in surveys   Experiments   Intro to experiments  Experimental design   Probability   Probability intro  Rules of probability   Random variables   Introduction  Distributions  Mean and variance  Independence  Combinations  Transformations  Simulation of events   Discrete variables   Binomial distribution  Negative binomial   Continuous variables   Normal distribution  Standard normal  t Distribution  Chi-square   Sampling distributions   Sampling distribution  Diff between props  Diff between means   Estimation   Estimation problems  Standard error  Margin of error  Confidence interval   Confidence intervals   Proportions  Diff between props  Means  Diff between means  Diff between pairs  Regression slope   Hypothesis testing   Hypothesis test intro  Power of test  How to test   Hypothesis tests   Proportions  Diff between props  Mean  Diff between means  Diff between pairs  Goodness of fit test  Homogeneity  Independence  Regression slope   Appendices   Practice exam  Notation  AP stat formulas        Binomial Probability Distribution  To understand binomial distributions and binomial probability, it helps to 
        understand binomial experiments and some associated notation; so we 
        cover those topics first.  Binomial Experiment  A binomial experiment is a statistical experiment that has the following properties:   The experiment consists of n repeated trials.  Each trial can result in just two possible outcomes. We call one of these 
        outcomes a success and the other, a failure.  The probability of success, denoted by P , is the same on every 
        trial.  The trials are independent ; 
	        that is, the outcome on one trial does not affect the outcome on other trials.   Consider the following statistical experiment. You flip a coin 2 times and count 
        the number of times the coin lands on heads. This is a binomial experiment 
        because:   The experiment consists of repeated trials. 
            We flip a coin 2 times.  Each trial can result in just two possible 
            outcomes - heads or tails.  The probability of success is 
            constant - 0.5 on every trial.  The trials are independent; that is, getting heads on one trial does not affect 
	        whether we get heads on other trials.   Notation  The following notation is helpful, when we talk about binomial probability.   x : The number of successes that result 
            from the binomial experiment.  n : The number of trials in the 
            binomial experiment.  P : The probability of success on an 
            individual trial.  Q : The probability of failure on an 
            individual trial. 
	        (This is equal to 1 - P .)  n! : The factorial of n (also known as n factorial).  b( x ; n, P ): Binomial 
            probability - the probability that an n -trial 
	        binomial experiment results in exactly  x successes, 
	        when the 
	        probability of success on an individual trial is P .  n C r : The number of combinations of n things, taken r at a time.        Binomial Distribution  A binomial random variable is the number of successes x in n repeated trials of a binomial experiment. The probability distribution of a binomial random variable is called 
	        a binomial distribution .  Suppose we flip a coin two times and count the number of heads (successes). The 
        binomial random variable is the number of heads, which can take on values of 0, 
        1, or 2. The binomial distribution is presented below.    Number of heads  Probability    0  0.25    1  0.50    2  0.25    The binomial distribution has the following properties:   The mean of the distribution (μ x ) is equal to n * P .  The variance (σ 2 x ) is n * P * ( 1 - P ).  The standard deviation (σ x ) is 
            sqrt[ n * P * ( 1 - P ) ].   Binomial Formula and Binomial Probability  The binomial probability refers to the probability that a 
        binomial experiment results in exactly  x successes. For example, 
        in the above table, we see that the binomial probability of getting exactly one 
        head in two coin flips is 0.50.  Given x , n , and P , we can compute the binomial probability 
        based on the binomial formula:   Binomial Formula. Suppose a binomial 
	        experiment consists of n trials and results in x successes. If 
	        the probability of success on an individual trial is P , then the 
	        binomial probability is:  b( x ; n, P ) = n C x * P x * (1 - P) n - x  or b( x ; n, P ) = { n! / [ x! (n - x)! ] } * P x * (1 - P) n - x     Example 1  Suppose a die is tossed 5 times. What is the probability of getting exactly 2 
        fours?  Solution: This is a binomial experiment in which the number of trials is 
        equal to 5, the number of successes is equal to 2, and the probability of 
        success on a single trial is 1/6 or about 0.167. Therefore, the binomial 
        probability is:  b(2; 5, 0.167) = 5 C 2 * (0.167) 2 * (0.833) 3  b(2; 5, 0.167) = 0.161  Cumulative Binomial Probability  A cumulative binomial probability refers to the probability 
        that the binomial random variable falls within a specified range (e.g., 
        is greater than or equal to a stated lower limit and less than or 
        equal to a stated upper limit).  For example, we might be interested in the cumulative binomial probability of 
        obtaining 45 or fewer heads in 100 tosses of a coin (see Example 1 below). This 
        would be the sum of all these individual binomial probabilities.  b(x < 45; 100, 0.5) = b(x = 0; 100, 0.5) + b(x = 1; 100, 0.5) + ... + 
        b(x = 44; 100, 0.5) + b(x = 45; 100, 0.5)   Binomial Calculator  As you may have noticed, the binomial formula requires many time-consuming 
	        computations. The Binomial Calculator can do this work for you - quickly, 
	        easily, and error-free. Use the Binomial Calculator to compute binomial 
	        probabilities and cumulative binomial probabilities. The  
		    calculator is free. It can found in the Stat Trek
            main menu under the Stat Tools tab. Or you can tap the button below.  Binomial Calculator   Example 2  What is the probability of obtaining 45 or fewer heads in 100 tosses of a coin?  Solution: To solve this problem, we compute 46 individual probabilities, 
        using the binomial formula. The sum of all these probabilities is the answer we 
        seek. Thus,  b(x < 45; 100, 0.5) = b(x = 0; 100, 0.5) + b(x = 1; 100, 0.5) + . . . 
        + b(x = 45; 100, 0.5) b(x < 45; 100, 0.5) = 0.184           Example 3  The probability that a student is accepted to a prestigious college is 0.3. If 
        5 students from the same school apply, what is the probability that at most 2 
        are accepted?  Solution: To solve this problem, we compute 3 individual probabilities, 
        using the binomial formula. The sum of all these probabilities is the answer we 
        seek. Thus,  b(x < 2; 5, 0.3) = b(x = 0; 5, 0.3) + b(x = 1; 5, 0.3) + b(x = 2; 5, 
        0.3) b(x < 2; 5, 0.3) = 0.1681 + 0.3601 + 0.3087 b(x < 2; 5, 0.3) = 0.8369  Example 4  What is the probability that the world series will last 4 games?  5 games?
        6 games? 7 games? Assume that the teams are evenly matched.  Solution: This is a very tricky application of the binomial 
        distribution.  If you can follow the logic of this solution, you have
        a good understanding of the material covered in the tutorial, to this
        point.  In the world series, there are two baseball teams.  The series
        ends when the winning team wins 4 games.  Therefore, we define a success as a 
        win by the team that ultimately becomes the world series champion.  For the purpose of this analysis, we assume that the teams are evenly matched.  
        Therefore, the probability that a particular team wins a particular game is
        0.5.  Let's look first at the simplest case.  What is the probability that the series
        lasts only 4 games.  This can occur if one team wins the first 4 
        games.  The probability of the National League team winning 4 games 
        in a row is:  b(4; 4, 0.5) = 4 C 4 * (0.5) 4 * (0.5) 0 = 0.0625  Similarly, when we compute the probability of the American League team 
        winning 4 games in a row, we find that it is also 0.0625.  
        Therefore, probability that the series ends in
        four games would be 0.0625 + 0.0625 = 0.125; since the series would end if
        either the American or National League team won 4 games in a row.  Now let's tackle the question of finding probability that the world series 
        ends in 5 games.  The trick in finding this solution is to recognize that
        the series can only end in 5 games, if one team has 
        won 3 out of the first 4 games.  So let's first find the probability
        that the American League team wins exactly 3 of the first 4 games.  b(3; 4, 0.5) = 4 C 3 * (0.5) 3 * (0.5) 1 = 0.25  Okay, here comes some more tricky stuff, so listen up.  Given that the 
        American League  team has won 3 of the first 4 games, the American League team 
        has a 50/50 chance of winning the fifth game to end the series.
        Therefore, the probability of the American League team winning the 
        series in 5 games is 0.25 * 0.50 = 0.125.  Since the National League
        team could also win the series in 5 games, the probability that 
        the series ends in 5 games would be 0.125 + 0.125 = 0.25.  The rest of the problem would be solved in the same way.  You should find
        that the probability of the series ending in 6 games is 0.3125; and
        the probability of the series ending in 7 games is also 0.3125.               Bestsellers Advanced Placement Statistics Updated daily 1. Barron's AP Statistics, 9th Edition $18.99  $12.91 2. Barron's AP Statistics, 8th Edition $18.99  $20.00 3. Barron's AP Statistics, 7th Edition $18.99  $10.00 4. Cracking the AP Statistics Exam, 2017 Edition: Proven Techniques to Help You Score a 5 (College Test Preparation) $19.99  $19.99 Today's Bargain Book Understandable Statistics $319.95  $65.00  80% off See more Statistics books ... Bestsellers Handheld Calculators Updated daily 1. Texas Instruments TI-84 Plus CE Lightning Graphing Calculator $150.00  $150.00 2. Texas Instruments Ti-84 plus Graphing calculator - Black $108.99  $108.99 3. Texas Instruments VOY200/PWB Graphing Calculator $200.00  4. Sharp EL-W535B WriteView Scientific Calculator $24.99  $39.99 Today's Bargain Calculator Texas Instruments Nspire CX CAS Graphing Calculator $185.00  $143.49  22% off See more Graphing Calculators ... Bestsellers Statistics and Probability Updated daily 1. Barron's AP Statistics, 9th Edition $18.99  $12.91 2. Naked Statistics: Stripping the Dread from the Data $16.95  $13.70 3. Statistics For Dummies (For Dummies (Math & Science)) $19.99  $13.11 4. How to Lie with Statistics $13.95  $8.98           About  Contact  Privacy  Terms  Advertising    Advertise on Stat Trek  Copyright © 2018 StatTrek.com        Advanced          Show Ads  Hide Ads  About Ads                   Random Variables: Mean, Variance and Standard Deviation  A Random Variable is a set of possible values from  a random experiment.   Example: Tossing a coin: we could get Heads or Tails.  Let's give them the values Heads=0 and Tails=1 and we have a Random Variable "X":    So:   We have an experiment (like tossing a coin)  We give values to each event  The set of values is a Random Variable   Learn more at Random Variables .  Mean, Variance and Standard Deviation   Example: Tossing a single unfair  die  For fun, imagine a weighted die (cheating!) so we have these probabilities:    1  2  3  4  5  6    0.1  0.1  0.1  0.1  0.1  0.5       Mean or Expected Value: μ  When we know the probability p of every value x we can calculate the Expected Value (Mean) of X:   μ = Σxp   Note: Σ is Sigma Notation , and means to sum up.  To calculate the Expected Value:   multiply each value by its probability  sum them up     Example continued:    x  1  2  3  4  5  6    p  0.1  0.1  0.1  0.1  0.1  0.5    xp  0.1  0.2  0.3  0.4  0.5  3    μ = Σxp The expected value is 4.5 Note: this  is a weighted mean :  values with higher probability have higher contribution to the mean.   Variance: Var(X) The Variance is:  Var(X) = Σx 2 p − μ 2  To calculate the Variance :  square each value and multiply by its probability  sum them up and we get Σx 2 p  then subtract the square of the Expected Value μ 2    Example continued:    x  1  2  3  4  5  6    p  0.1  0.1  0.1  0.1  0.1  0.5    x 2 p  0.1  0.4  0.9  1.6  2.5  18    Σx 2 p = 0.1+0.4+0.9+1.6+2.5+18 = 23.5  Var(X) = Σx 2 p − μ 2 = 23.5 - 4.5 2 = 3.25  The variance is 3.25   Standard Deviation: σ The Standard Deviation is the square root of the Variance:  σ = √Var(X)    Example continued:    x  1  2  3  4  5  6    p  0.1  0.1  0.1  0.1  0.1  0.5    x 2 p  0.1  0.4  0.9  1.6  2.5  18    σ = √Var(X) = √3.25 = 1.803...  The Standard Deviation is 1.803...   Let's have another example! (Note that we run the table downwards instead of along this time.)   You plan to open a new McDougals Fried Chicken, and found these stats for similar restaurants:    Percent  Year's Earnings    20%  $50,000 Loss    30%  $0    40%  $50,000 Profit    10%  $150,000 Profit    Using that as probabilities for your new restaurant's profit, what is the Expected Value and Standard Deviation?    The Random Variable is X = 'possible profit'.  Sum up xp and x 2 p :     Probability p  Earnings ($'000s) x  xp  x 2 p    0.2  -50  -10  500    0.3  0  0  0    0.4  50  20  1000    0.1  150  15  2250    Σp = 1    Σxp = 25   Σx 2 p = 3750       μ  = Σxp = 25  Var(X) = Σx 2 p − μ 2  = 3750 − 25 2  = 3750 − 625 = 3125  σ = √3125 = 56 (to nearest whole number)  But remember these are in thousands of dollars, so:   μ = $25,000  σ = $56,000   So you might expect to make $25,000, but with a very wide deviation possible.  Let's try that again, but with a much higher probability for $50,000:  Example (continued):  Now with different probabilities (the $50,000 value has a high probability of 0.7 now):     Probability p  Earnings ($'000s) x  xp  x 2 p    0.1  -50  -5  250    0.1  0  0  0    0.7  50  35  1750    0.1  150  15  2250    Σp = 1  Sums:  Σxp = 45  Σx 2 p = 4250       μ  = Σxp = 45  Var(X) = Σx 2 p − μ 2  = 4250 − 45 2  = 4250 − 2025 = 2225  σ = √2225 = 47 (to nearest whole number)  In thousands of dollars:   μ = $45,000  σ = $47,000   The mean is now much closer to the most probable value.  And the standard deviation is a little smaller (showing that the values are more central.)   Continuous Random Variables can be either Discrete 
	or Continuous :  Discrete Data can only take certain values (such as 1,2,3,4,5)  Continuous Data can take any value within a range (such as a person's height)  Here we looked only at discrete data, as finding the Mean, Variance and Standard Deviation of continuous data  needs Integration .  Summary   A Random 
			Variable is a variable whose possible values are numerical outcomes 
			of a random experiment.  The Mean (Expected 
			Value) is: μ = Σxp  The Variance is: Var(X) = Σx 2 p − μ 2  The Standard Deviation is: σ = √Var(X)      Random Variables Data Index     Copyright © 2016 MathsIsFun.com                        If you're seeing this message, it means we're having trouble loading external resources on our website.  If you're behind a web filter, please make sure that the domains *.kastatic.org and *.kasandbox.org are unblocked.        Main content       To log in and use all the features of Khan Academy, please enable JavaScript in your browser.                                                                                      If you're seeing this message, it means we're having trouble loading external resources on our website.  If you're behind a web filter, please make sure that the domains *.kastatic.org and *.kasandbox.org are unblocked.        Main content       To log in and use all the features of Khan Academy, please enable JavaScript in your browser.                                                                     Stat Trek  Teach yourself statistics                Home    Tutorials    AP statistics    Stat tables    Stat tools    Calculators    Books    Help        Overview  AP statistics  Beyond AP statistics  Simple linear regression  Probability sampling  Matrix algebra    AP tutorial  Test preparation  Practice test  AP formulas  FAQ  AP study guides  AP calculators    Binomial  Chi-square  f Dist  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Dist    Random numbers  Probability  Bayes rule  Combinations/permutations  Factorial  Event counter  Sample planning    Graphing  Scientific  Financial    Statistics  AP study guides  Probability  Survey sampling    Statistics dictionary  Problems and solutions  Formulas  Notation               Home    Tutorials   Overview  AP statistics  Beyond AP statistics  Simple Linear regression  Probability sampling  Matrix algebra     AP statistics   Test preparation  AP tutorial  Practice test  AP formulas  FAQ  AP study guides  AP calculators     Statistical tables   Binomial  Chi-square  f Distribution  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Distribution     Statistical tools   Random number generator  Probability calculator  Bayes rule calculator  Combinations/permutations  Factorial calculator  Event counter  Sample planning wizard     Handheld calculators   Graphing  Scientific  Financial     Books   Statistics  AP study guides  Probability  Survey sampling     Help   Statistics dictionary  Problems and solutions  Formulas  Notation                       Beyond AP Statistics   Probability Basics   Sets and subsets  Statistical experiment  Counting data points  Probability problems  Bayes theorem   Small Samples   Estimate proportion  Test proportion   Distributions   Probability distribution  Discrete/continuous  Hypergeometric  Multinomial  Poisson  f Distribution   Power   Region of acceptance  Power of a test   How to find power            Beyond AP Statistics   Probability Basics   Sets and Subsets  Statistical Experiments  Counting Data Points  Probability Problems  Bayes Theorem   Small Samples   Estimate Proportion  Test Proportion   Distributions   Probability Distribution  Discrete/Continuous  Hypergeometric  Multinomial  Poisson  f Distribution   Power   Region of Acceptance  Power of a Test   How to Find Power        What is a Probability Distribution?  A probability distribution is a table or an equation that links each outcome 
    of a statistical experiment with its probability of occurrence.  Probability Distribution Prerequisites  To understand probability distributions, it is important to understand variables. 
	    random variables, and some notation.   A variable is a symbol ( A , B , x , y , 
	    etc.) that can take on any of a specified set of values.  When the value of a variable is the outcome of a statistical experiment , that variable is a random variable .   Generally, statisticians use a capital letter to represent a random variable and 
	    a lower-case letter, to represent one of its values. For example,   X represents the random variable X.  P(X) represents the probability of X.  P(X = x) refers to the probability that the random variable X is equal to a 
		    particular value, denoted by x. As an example, P(X = 1) refers to the 
		    probability that the random variable X is equal to 1.   Probability Distributions  An example will make clear the relationship between random variables and 
        probability distributions. Suppose you flip a coin two times. This simple 
	    statistical experiment can have four possible outcomes: HH, HT, TH, and TT. 
	    Now, let the variable X represent the number of Heads that result from this 
	    experiment. The variable X can take on the values 0, 1, or 2. In this example, 
	    X is a random variable; because its value is determined by the outcome of a 
	    statistical experiment.  A probability distribution is a table or an equation that links 
	    each outcome of a statistical experiment with its probability of occurrence. 
	    Consider the coin flip experiment described above. The table below, which 
	    associates each outcome with its probability, is an example of a probability 
	    distribution.    Number of heads  Probability    0  0.25    1  0.50    2  0.25    The above table represents the probability distribution of the random variable 
	    X.        Cumulative Probability Distributions  A cumulative probability refers to the probability that the 
	    value of a random variable falls within a specified range.  Let us return to the coin flip experiment. If we flip a coin two times, we might 
	    ask: What is the probability that the coin flips would result in one or fewer 
	    heads? The answer would be a cumulative probability. It would be the 
	    probability that the coin flip experiment results in zero heads plus the 
	    probability that the experiment results in one head.  P(X < 1) = P(X = 0) + P(X = 1) = 0.25 + 0.50 = 0.75  Like a probability distribution, a cumulative probability distribution can be 
	    represented by a table or an equation. In the table below, the cumulative 
	    probability refers to the probability than the random variable X is less than 
	    or equal to x.    Number of heads: x  Probability: P(X = x) Cumulative Probability: P(X < x)  0  0.25  0.25   1  0.50  0.75   2  0.25  1.00      Uniform Probability Distribution The simplest probability distribution occurs when all of the values of a 
	    random variable occur with equal probability. This probability 
	    distribution is called the uniform distribution .  Uniform Distribution. Suppose the 
		    random variable X can assume k different values. Suppose also that the P(X = x k ) 
		    is constant. Then,  P(X = x k ) = 1/k  Example 1 Suppose a die is tossed. What is the probability that the die will land on 5? Solution: When a die is tossed, there are 6 possible outcomes represented 
	    by: S = { 1, 2, 3, 4, 5, 6 }. Each possible outcome is a random variable (X), 
	    and each outcome is equally likely to occur. Thus, we have a uniform 
	    distribution. Therefore, the P(X = 5) = 1/6. Example 2 Suppose we repeat the dice tossing experiment described in Example 1. This 
	    time, we ask what is the probability that the die will land on a number that is 
	    smaller than 5? Solution: When a die is tossed, there are 6 possible outcomes represented 
	    by: S = { 1, 2, 3, 4, 5, 6 }. Each possible outcome is equally likely to occur. 
	    Thus, we have a uniform distribution. This problem involves a cumulative probability. The probability that the die 
	    will land on a number smaller than 5 is equal to: P( X < 5 ) = P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4) P( X < 5 ) = 1/6 + 1/6 + 1/6 + 1/6 = 2/3           Bestsellers Handheld Calculators Updated daily 1. Texas Instruments TI-84 Plus CE Lightning Graphing Calculator $150.00  $150.00 2. Texas Instruments Ti-84 plus Graphing calculator - Black $108.99  $108.99 3. Texas Instruments VOY200/PWB Graphing Calculator $200.00  4. Sharp EL-W535B WriteView Scientific Calculator $24.99  $39.99 Today's Bargain Book Understandable Statistics $319.95  $65.00  80% off See more Statistics books ... Bestsellers Statistics and Probability Updated daily 1. Barron's AP Statistics, 9th Edition $18.99  $12.91 2. Naked Statistics: Stripping the Dread from the Data $16.95  $13.70 3. Statistics For Dummies (For Dummies (Math & Science)) $19.99  $13.11 4. How to Lie with Statistics $13.95  $8.98           About  Contact  Privacy  Terms  Advertising    Advertise on Stat Trek  Copyright © 2018 StatTrek.com              Stack Exchange Network  Stack Exchange network consists of 173 Q&A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.  Visit Stack Exchange                   current community        help  chat    Mathematics        Mathematics Meta      your communities     Sign up or log in to customize your list.    more stack exchange communities   company blog           Tour Start here for a quick overview of the site     Help Center Detailed answers to any questions you might have     Meta Discuss the workings and policies of this site     About Us Learn more about Stack Overflow the company     Business Learn more about hiring developers or posting ads with us                                Log In  Sign Up           Mathematics       Questions    Tags    Users    Badges    Unanswered        Ask Question            _  Mathematics Stack Exchange is a question and answer site for people studying math at any level and professionals in related fields. Join them; it only takes a minute:   Sign up    Here's how it works:   Anybody can ask a question  Anybody can answer  The best answers are voted up and rise to the top         Find the probability distribution of X?         up vote  1  down vote  favorite       Suppose Nokia store places 20 of its cell phones on a clearance sale, unknown to anyone 5 of these cell phones are defective. A customer selects 3 cell phones at random for inspection. Let X be the number of defective cell phones in the sample. Find the probability distribution of X?  My attempt:  $$
        \begin{matrix}
        x & 0 & 1 & 2 & 3 \\
        P(X=x) & 0.015625 & 0.140625 & 0.421875 & 0.421875 \\
        \\
        \end{matrix}
$$  I calculated the values using the binomial distribution: $$
P(X = x) = nCx \ p^{n-x} (1-p)^x$$  Where, $n$=5, $x$=0,1,2,3 and $p$=(.25) [from 5/20]  Is this the correct way to do this?    probability     share | cite | improve this question     asked Nov 25 '12 at 8:12       Siyanda   972 3 16 35              1      IF $p$ is the probability of detecting a faulty item ('success'), then it should be $\binom{n}{x}p^{x}(1-p)^{n-x}$ – Alex  Nov 25 '12 at 8:17            @Alex for sampling with replacement – Henry  Nov 25 '12 at 8:34            Still, important to point out that even though Binomial is inappropriate, Binomial was mishandled. – André Nicolas  Nov 25 '12 at 10:03        add a comment |           2 Answers 2     active  oldest  votes            up vote  4  down vote      No it is not correct.  In particular with only a quarter of the phones defective, getting three defectives out of three in the sample should have a low probability.  A second point is that the customer is presumably looking at three different phones (sampling without replacement ), so you not be using the binomial distribution.  As an example, the probability of three phones defective, sampling without replacement, is  $${3 \choose 3} \times \frac{5}{20} \times \frac{4}{19} \times \frac{3}{18} \approx 0.00877$$ rather than your ${3 \choose 3}0.25^0 0.75^3 = 0.421875.$     share | cite | improve this answer     answered Nov 25 '12 at 8:30       Henry   87.9k 3 63 133              add a comment |             up vote  3  down vote      This experiment is described by a hypergeometric distribution . For every $0\leqslant k\leqslant 3$,
$$\mathbb P(X=k)=\frac{\color{red}{{5\choose k}}\color{green}{{15\choose 3-k}}}{{20\choose 3}}$$
hence
$$
\mathbb P(X=3)=\frac{\color{red}{5\cdot4\cdot3}}{20\cdot19\cdot18},
\qquad
\mathbb P(X=2)=\frac{\color{red}{5\cdot4}\cdot3\cdot\color{green}{15}}{20\cdot19\cdot18},
$$
$$
\mathbb P(X=1)=\frac{\color{red}{5}\cdot3\cdot\color{green}{14\cdot15}}{20\cdot19\cdot18},
\qquad
\mathbb P(X=0)=\frac{\color{green}{13\cdot14\cdot15}}{20\cdot19\cdot18}.$$     share | cite | improve this answer      edited Nov 25 '12 at 9:19             answered Nov 25 '12 at 8:43       Did   239k 23 201 429                  (a) I thought you preferred hints rather than the full answer. (b) Your four probabilities do not add up to 1. – Henry  Nov 25 '12 at 9:04             @Henry (a) Maybe so. A naked link to the WP page might have been more educational here. (b) Thanks, misprint corrected. – Did  Nov 25 '12 at 9:25            OK - I will delete those comments – Henry  Nov 25 '12 at 12:09        add a comment |            Your Answer             draft saved  draft discarded             Sign up or log in    Sign up using Google   Sign up using Facebook   Sign up using Email and Password        Post as a guest       Name     Email             Post as a guest       Name     Email            discard  By posting your answer, you agree to the privacy policy and terms of service .     Not the answer you're looking for?                            Browse other questions tagged probability or ask your own question .         asked    5 years, 4 months ago      viewed     17,906 times       active    5 years, 4 months ago       Visit Chat      Linked     2   Assume you toss a fair coin 25 times. How many completed runs do you expect to observe?      Related   2 Probability and Counting 1 Probability $1$ item is defective out of a sample of $10$? 1 A Conditional probability problem and binomial formula 0 Binomial distribution question regarding one after another selection 0 discrete probability distributions; Dish problem 2 Simple calculations of mean, standard deviation, and probability 0 probability of a proportion point estimate 0 Probability of a Normally Distributed Random Sample 0 Find Probability Distribution Parameters For Sample With Percentage Given 1 Clarifying whether hypergeometric distribution can be used here      Hot Network Questions     What are healthy, productive ways to encourage students to progress to more advanced constructs as opposed to staying with the familiar?    To switch back to reg oil (from semi-synthetic) do i have to (or should I) flush the engine 1st?    Why was UNIX never backported to the PDP-7?    How can I give out my telephone number without implying anything?    Why didn't Voldemort create a seventh Horcrux not knowing Harry was one?    What type of aircraft is depicted in the Taylor Swift music video "Look What You Made Me Do"?    A Mathematical Paradox About Probabilities    Story about a spaceship "emergency program" that simulates a "perfect woman"    Does writing matter a lot in research?    Would being hollow solve the weight problem of giant swords?    Is the word 'Hitherto' outdated?    Single Word : Cannot be resolved by waiting and trying again    How to build a trap to last the ages?    Why will the BFS reenter broadside rather than engine first?    Evil Campaigns:How to explain the difference between being evil and being a jerk?    What's the probable cause for extremely low inbound traffic and high outbound traffic?    Why do we need so many classes in design patterns    How to ensure two standalone documents have same dimensions    How to make a Tree diagram using images as vertices, that goes left to right    Steaming with oil instead of water    Splitting equilateral triangle with shortest curve    Two lasers between two mirrors    Why should a software QA engineer need to learn JavaScript?    Why is this shared-neutral wiring bad?    more hot questions     question feed             Mathematics   Tour  Help  Chat  Contact  Feedback  Mobile     Company   Stack Overflow  Stack Overflow Business  Developer Jobs  About  Press  Legal  Privacy Policy       Stack Exchange Network   Technology  Life / Arts  Culture / Recreation  Science  Other        Stack Overflow  Server Fault  Super User  Web Applications  Ask Ubuntu  Webmasters  Game Development   TeX - LaTeX  Software Engineering  Unix & Linux  Ask Different (Apple)  WordPress Development  Geographic Information Systems  Electrical Engineering   Android Enthusiasts  Information Security  Database Administrators  Drupal Answers  SharePoint  User Experience  Mathematica   Salesforce  ExpressionEngine® Answers  Stack Overflow em Português  Blender  Network Engineering  Cryptography  Code Review   Magento  Software Recommendations  Signal Processing  Emacs  Raspberry Pi  Stack Overflow на русском  Programming Puzzles & Code Golf   Stack Overflow en español  Ethereum  Data Science  Arduino  Bitcoin    more (29)        Photography  Science Fiction & Fantasy  Graphic Design  Movies & TV  Music: Practice & Theory  Worldbuilding  Seasoned Advice (cooking)   Home Improvement  Personal Finance & Money  Academia  Law    more (15)        English Language & Usage  Skeptics  Mi Yodeya (Judaism)  Travel  Christianity  English Language Learners  Japanese Language   Arqade (gaming)  Bicycles  Role-playing Games  Anime & Manga  Puzzling  Motor Vehicle Maintenance & Repair    more (33)        MathOverflow  Mathematics  Cross Validated (stats)  Theoretical Computer Science  Physics  Chemistry  Biology   Computer Science  Philosophy    more (10)        Meta Stack Exchange  Stack Apps  API  Data  Area 51        Blog  Facebook  Twitter  LinkedIn   site design / logo © 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required . rev 2018.4.9.29787       Mathematics Stack Exchange works best with JavaScript enabled                  Stack Exchange Network  Stack Exchange network consists of 173 Q&A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.  Visit Stack Exchange                   current community        help  chat    Mathematics        Mathematics Meta      your communities     Sign up or log in to customize your list.    more stack exchange communities   company blog           Tour Start here for a quick overview of the site     Help Center Detailed answers to any questions you might have     Meta Discuss the workings and policies of this site     About Us Learn more about Stack Overflow the company     Business Learn more about hiring developers or posting ads with us                                Log In  Sign Up           Mathematics       Questions    Tags    Users    Badges    Unanswered        Ask Question            _  Mathematics Stack Exchange is a question and answer site for people studying math at any level and professionals in related fields. Join them; it only takes a minute:   Sign up    Here's how it works:   Anybody can ask a question  Anybody can answer  The best answers are voted up and rise to the top         Find the probability distribution of the random variable X.         up vote  2  down vote  favorite       A fair coin is flipped $3$ times. Consider a random variable $X$ which is the number of runs. Number of runs is the number of changes of letter $H$ and $T$. For example, $HHH$ has one run, $TTH$ has two runs and $THT$ has three runs. Find the probability distribution of the random variable $X$.  My work: I don't understand the phrasing of this question. In examples in my textbook and online $X$ is defined as the number of heads or tails. But I can't follow where the example in this question is going. I would think that $TTH$ and $THT$ would both have 2 runs since $HHH$ only has one. I don't know what zero runs would be either. Can anyone give me guidance on what exactly this question means? I'm pretty sure I can solve it once I understand what the number of runs means.  The outcomes would be:  $HHH$ $X=1$  $HTH$  $HHT$  $THH$  $TTH$ $X=2$  $HTT$  $THT$ $X=3$  $TTT$  I don't know what number of $X$ would correspond with each.    probability  probability-theory  statistics     share | cite | improve this question      edited Feb 28 '16 at 1:07             asked Feb 28 '16 at 1:00       Koalafications   45 9                  Why should you think that there must be an outcome described by the event zero runs?  In my interpretation, $\text{Onerun}=\{HHH,TTT\}, \text{Tworuns}=\{HHT,HTT,TTH,THH\}, \text{Threeruns}=\{HTH,THT\}$.  Something like $TTHHHHTTHTHHHT$ would have $7$ runs.  $\underbrace{TT}_1 \underbrace{HHHH}_2 \underbrace{TT}_3 \underbrace{H}_4 \underbrace{T}_5 \underbrace{HHH}_6 \underbrace{T}_7$ – JMoravitz  Feb 28 '16 at 1:13             Thank you that makes sense to me! – Koalafications  Feb 28 '16 at 1:20        add a comment |           1 Answer 1     active  oldest  votes            up vote  1  down vote  accepted      I think you are just missing what the book means by a "run". E.g.  HHHHHHHHHHHHTTTTTTHHHHHTTTTTT  has 4 runs: a run of heads, followed by a run of tails, followed by a run of heads, followed by a run of tails. All the H's or T's that are next to each other get lumped together to form a single "run", regardless of how many there are. Thus HHH and TTT have 1 run, HTT, HHT, THH, TTH have 2 runs, and HTH and THT have 3 runs.  Thus $P(X=1) = 2/8, P(X=2) = 4/8, P(X=3) = 2/8$ is the distribution of $X$.     share | cite | improve this answer     answered Feb 28 '16 at 1:17       nullUser   15.9k 4 33 96                  Yep that was it! Haha I was looking at it like some sort of pattern but it was really that simple. Guess I like to over complicate things. Thank you though! – Koalafications  Feb 28 '16 at 1:20        add a comment |            Your Answer             draft saved  draft discarded             Sign up or log in    Sign up using Google   Sign up using Facebook   Sign up using Email and Password        Post as a guest       Name     Email             Post as a guest       Name     Email            discard  By posting your answer, you agree to the privacy policy and terms of service .     Not the answer you're looking for?                            Browse other questions tagged probability  probability-theory  statistics or ask your own question .         asked    2 years, 1 month ago      viewed     174 times       active    2 years, 1 month ago       Related   8 Probability about a coin games 0 Chance on winning by throwing a head on first toss. 2 Conditional probability or Bayes' theorem 0 Random variables probability-infinite coin toss 2 Probability with coins 1 Confusion - Expected Waiting Times for pattern HHH 2 expected waiting time for HTT - alternative way 0 Possible outcomes on a three coin flip. 3 How to rationalize through coin flip probability? 1 Outcome space of 3 biased coins      Hot Network Questions     Up go the bits!    Should I prepare new homework exercises each year?    My contract is expiring and it won't be renewed yet I'm in the midst of a crucial project    How to reward students for learning from mistakes without penalizing those who didn't make mistakes in the first place?    My prefix ends fast    How can I avoid the awkwardness of a returning player who wears a low-cut shirt?    Automated way to create a directory tree    Why doesn’t the IRS just send me a bill for the taxes I owe based on the info they already have?    Count the number of lines in macro argument    Git clone only works with ssh://git@.. and not with git@    What type of aircraft is depicted in the Taylor Swift music video "Look What You Made Me Do"?    Steaming with oil instead of water    How many arguments were passed?    How to view MTU size in Wireshark?    How to build a trap to last the ages?    Crash during startup on a recent corporate computer    To switch back to reg oil (from semi-synthetic) do i have to (or should I) flush the engine 1st?    How do I talk about my abusive former advisor if people ask?    Single Word : Cannot be resolved by waiting and trying again    Why use baking powder instead of yeast?    What's the probable cause for extremely low inbound traffic and high outbound traffic?    How to ensure two standalone documents have same dimensions    I want to leave Islam, but they would execute and kill me. What do I do?    Why was UNIX never backported to the PDP-7?    more hot questions     question feed             Mathematics   Tour  Help  Chat  Contact  Feedback  Mobile     Company   Stack Overflow  Stack Overflow Business  Developer Jobs  About  Press  Legal  Privacy Policy       Stack Exchange Network   Technology  Life / Arts  Culture / Recreation  Science  Other        Stack Overflow  Server Fault  Super User  Web Applications  Ask Ubuntu  Webmasters  Game Development   TeX - LaTeX  Software Engineering  Unix & Linux  Ask Different (Apple)  WordPress Development  Geographic Information Systems  Electrical Engineering   Android Enthusiasts  Information Security  Database Administrators  Drupal Answers  SharePoint  User Experience  Mathematica   Salesforce  ExpressionEngine® Answers  Stack Overflow em Português  Blender  Network Engineering  Cryptography  Code Review   Magento  Software Recommendations  Signal Processing  Emacs  Raspberry Pi  Stack Overflow на русском  Programming Puzzles & Code Golf   Stack Overflow en español  Ethereum  Data Science  Arduino  Bitcoin    more (29)        Photography  Science Fiction & Fantasy  Graphic Design  Movies & TV  Music: Practice & Theory  Worldbuilding  Seasoned Advice (cooking)   Home Improvement  Personal Finance & Money  Academia  Law    more (15)        English Language & Usage  Skeptics  Mi Yodeya (Judaism)  Travel  Christianity  English Language Learners  Japanese Language   Arqade (gaming)  Bicycles  Role-playing Games  Anime & Manga  Puzzling  Motor Vehicle Maintenance & Repair    more (33)        MathOverflow  Mathematics  Cross Validated (stats)  Theoretical Computer Science  Physics  Chemistry  Biology   Computer Science  Philosophy    more (10)        Meta Stack Exchange  Stack Apps  API  Data  Area 51        Blog  Facebook  Twitter  LinkedIn   site design / logo © 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required . rev 2018.4.9.29787       Mathematics Stack Exchange works best with JavaScript enabled                  Stack Exchange Network  Stack Exchange network consists of 173 Q&A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.  Visit Stack Exchange                   current community        help  chat    Mathematics        Mathematics Meta      your communities     Sign up or log in to customize your list.    more stack exchange communities   company blog           Tour Start here for a quick overview of the site     Help Center Detailed answers to any questions you might have     Meta Discuss the workings and policies of this site     About Us Learn more about Stack Overflow the company     Business Learn more about hiring developers or posting ads with us                                Log In  Sign Up           Mathematics       Questions    Tags    Users    Badges    Unanswered        Ask Question            _  Mathematics Stack Exchange is a question and answer site for people studying math at any level and professionals in related fields. Join them; it only takes a minute:   Sign up    Here's how it works:   Anybody can ask a question  Anybody can answer  The best answers are voted up and rise to the top         find the mean value of x if The probability distribution of a discrete random variable x is given         up vote  0  down vote  favorite       The probability distribution of a discrete random variable x is
 $$f (x)=   \begin{pmatrix}3 \\ x \end{pmatrix}   (1/4)^x (3/4)^{3-x} $$   Find the mean value of x.
Construct a cumulative distribution function for f (x).
i find out$$ P(X=o) = 0.421875$$ $$ P(X=1) = 0.421875$$ $$ P(X=2) = 0.140625$$ $$ P(X=3) = 0.015625$$ put the value in Binomial distribution.    probability  statistics  probability-theory  probability-distributions     share | cite | improve this question      edited May 22 '14 at 8:15             asked May 21 '14 at 16:32       bill   37 1 5              3      The distribution is Binomial, the expected value (aka mean) is $np$, what is $p$ and what is $n$ in your formula? I'll give you a hint, $n  = 3,$ now find $p$ (probability of success) yourself. – Nameless  May 21 '14 at 16:35             The essence of the probability argument is that we have 3 independent trials, and on each trial, outcome 1 occurs with probability p and some other outcome with probability 1−p. help me for finding mean value – bill  May 21 '14 at 17:08        add a comment |           2 Answers 2     active  oldest  votes            up vote  1  down vote      "The essence of the probability argument is that we have 3 independent trials, and on each trial, outcome 1 occurs with probability p and some other outcome with probability 1−p. help me for finding mean value"  I would say that
x = {0,1}, P(0) = 1-p, P(1) = p, E(x) =0*(1-p)+1*p = p  When $X = x_1+x_2+\cdots + x_n$, then $E(X) = E(x_1)+E(x_2)+\cdots+E(x_n)=np$ (generally apply to both dependent and independent experiments).     share | cite | improve this answer     answered May 22 '14 at 8:06       georg   2,185 1 7 9              add a comment |             up vote  0  down vote      This discrete random variable follows Binomial distribution, i.e.
$$f(x)=\binom{n}{x}p^x(1-p)^{n-x}$$
where $n$ is the number of trials and $p$ is the proportion. What this distribution mean is that when there are $n$ trials and every trial follows $p$ proportion(success rate), the probability that $x$ successes occur is $f(x)$. And for Binomial distribution, the expect(mean) value and variance are
$$E(X)=np$$
$$Var(X)=np(1-p)$$
respectively.   From above, you can know how to calculate the mean value. For cumulative distribution function, it is just the the sum of probability of first k outcomes, which can be expressed as
$$F(k;n,p)=P(X\le \lfloor k\rfloor)=\sum_{i=0}^{\lfloor k \rfloor} \binom{n}{i}p^i(1-p)^{n-i}$$
where $\lfloor k\rfloor$ is the largest integer that is less than or equal to $k$. For this problem, I think this form is enough because of the small $n$ that equals to 3.     share | cite | improve this answer      edited May 21 '14 at 17:32             answered May 21 '14 at 17:15       sdg   161 1 7                  firsly find the probablity – bill  May 21 '14 at 17:18        add a comment |            Your Answer             draft saved  draft discarded             Sign up or log in    Sign up using Google   Sign up using Facebook   Sign up using Email and Password        Post as a guest       Name     Email             Post as a guest       Name     Email            discard  By posting your answer, you agree to the privacy policy and terms of service .     Not the answer you're looking for?                            Browse other questions tagged probability  statistics  probability-theory  probability-distributions or ask your own question .         asked    3 years, 10 months ago      viewed     3,777 times       active    3 years, 10 months ago       Visit Chat      Related   0 Construct a distribution for discrete random variable from its mean 0 Random variable X has the following discrete distribution 1 Discrete Random Variables Probability Exercise - How to approach this 0 Probability distribution of discrete random variable $X^2$ 0 Random variables/probability distribution table 0 Finding the expected value of a particular discrete random variable. 0 Pdf for discrete vs continuous random variables. 2 Given the cumulative distribution function find a random variable that has this distribution. 0 Given the joint distribution of discrete random variables $x$ and $y$ , find the probability distribution of a “centred” version of $x$. 1 Find the mean of the discrete random variable      Hot Network Questions     How to make a Tree diagram using images as vertices, that goes left to right    What to do about a colleague playing pranks on a manager?    Is there a way to avoid the recurring cost of microbes, when composting at home?    “GOD is real, unless declared integer.”    Hmm another Riley Riddle    Having trouble getting my friends to get invested in the game    Talmud passage relevant to #metoo    Is the word 'Hitherto' outdated?    Stat block from Animate Dead    Was the Twin Pines Mall scene at 1:15am for in-universe or out-of-universe reasons?    Distance between two points on the Moon    Why didn't Voldemort create a seventh Horcrux not knowing Harry was one?    Advantage of luminescent eggs?    What are healthy, productive ways to encourage students to progress to more advanced constructs as opposed to staying with the familiar?    Count the number of lines in macro argument    When to partition the hard drive?    Automated way to create a directory tree    Why would a god tolerate an impostor in his church?    In US universities, are the sport coaches typically considered tenured professors?    Git clone only works with ssh://git@.. and not with git@    What am I talking about here?    Is the quotient of a toric variety by a finite group still toric    Could a cave-in or avalanche in low gravity be dangerous?    Should Unity lifecycle methods be annotated with the UsedImplicitly attribute?    more hot questions     question feed             Mathematics   Tour  Help  Chat  Contact  Feedback  Mobile     Company   Stack Overflow  Stack Overflow Business  Developer Jobs  About  Press  Legal  Privacy Policy       Stack Exchange Network   Technology  Life / Arts  Culture / Recreation  Science  Other        Stack Overflow  Server Fault  Super User  Web Applications  Ask Ubuntu  Webmasters  Game Development   TeX - LaTeX  Software Engineering  Unix & Linux  Ask Different (Apple)  WordPress Development  Geographic Information Systems  Electrical Engineering   Android Enthusiasts  Information Security  Database Administrators  Drupal Answers  SharePoint  User Experience  Mathematica   Salesforce  ExpressionEngine® Answers  Stack Overflow em Português  Blender  Network Engineering  Cryptography  Code Review   Magento  Software Recommendations  Signal Processing  Emacs  Raspberry Pi  Stack Overflow на русском  Programming Puzzles & Code Golf   Stack Overflow en español  Ethereum  Data Science  Arduino  Bitcoin    more (29)        Photography  Science Fiction & Fantasy  Graphic Design  Movies & TV  Music: Practice & Theory  Worldbuilding  Seasoned Advice (cooking)   Home Improvement  Personal Finance & Money  Academia  Law    more (15)        English Language & Usage  Skeptics  Mi Yodeya (Judaism)  Travel  Christianity  English Language Learners  Japanese Language   Arqade (gaming)  Bicycles  Role-playing Games  Anime & Manga  Puzzling  Motor Vehicle Maintenance & Repair    more (33)        MathOverflow  Mathematics  Cross Validated (stats)  Theoretical Computer Science  Physics  Chemistry  Biology   Computer Science  Philosophy    more (10)        Meta Stack Exchange  Stack Apps  API  Data  Area 51        Blog  Facebook  Twitter  LinkedIn   site design / logo © 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required . rev 2018.4.9.29787       Mathematics Stack Exchange works best with JavaScript enabled                  Stack Exchange Network  Stack Exchange network consists of 173 Q&A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.  Visit Stack Exchange                   current community        help  chat    Mathematics        Mathematics Meta      your communities     Sign up or log in to customize your list.    more stack exchange communities   company blog           Tour Start here for a quick overview of the site     Help Center Detailed answers to any questions you might have     Meta Discuss the workings and policies of this site     About Us Learn more about Stack Overflow the company     Business Learn more about hiring developers or posting ads with us                                Log In  Sign Up           Mathematics       Questions    Tags    Users    Badges    Unanswered        Ask Question            _  Mathematics Stack Exchange is a question and answer site for people studying math at any level and professionals in related fields. Join them; it only takes a minute:   Sign up    Here's how it works:   Anybody can ask a question  Anybody can answer  The best answers are voted up and rise to the top         Probability $X$ is odd in a geometric distribution [duplicate]         up vote  1  down vote  favorite        This question already has an answer here:    Find the probability that a geometric random variable $X$ is an even number  2 answers     I have this problem:   Let $X$ be a Geometric distribution with parameter $p = \frac1{10}$.   What is the value of $P(x\text{ is odd})$?   What I got was $P(X = i) = pq^{i-1}$ where $p = \frac{1}{10}$ and $q = (1-p)$   Is this right?    probability  probability-distributions     share | cite | improve this question      edited Apr 24 '16 at 16:45       G. Sassatelli   22.4k 11 30 54        asked Mar 10 '14 at 0:10       userunknown   133 2 12           marked as duplicate by Meta, G. Sassatelli , Daniel Robert-Nicoud , Stefan4024 , Edward Jiang  Apr 24 '16 at 19:46   This question has been asked before and already has an answer. If those answers do not fully address your question, please ask a new question .          add a comment |           2 Answers 2     active  oldest  votes            up vote  3  down vote  accepted      $$P(\text{X is odd}) = P(X=1) + P(X=3) + P(X=5) + \ldots$$  The pmf of a geometric distribution is $q^{k-1}p$.  So $$P(\text{X is odd}) = q^{0} p + q^{2}p + q^{4} p + \ldots = p \sum_{k=0}^{\infty} q^{2k} = \frac{p}{1-q^{2}}$$  EDIT:  which equals $ \displaystyle\frac{1}{q+1}$ as pointed out by Dilip Sarwate in the comments     share | cite | improve this answer      edited Mar 10 '14 at 1:57             answered Mar 10 '14 at 1:43       Random Variable   25k 1 68 134              1      and $\displaystyle \frac{p}{1-q^2} = \frac{p}{(1-q)(1+q)} = \frac{1}{1+q}$ since $p = 1-q$. – Dilip Sarwate  Mar 10 '14 at 1:47         add a comment |             up vote  0  down vote      Imagine that if $X$ is odd then Alphonse wins the game, while if $X$ is even then Beti wins the game. It is clear that with probability $1$ one of Alphonse or Beti wins. Let $p$ be the probability A wins.  If A fails to win on the first trial, then the roles of A and B are reversed, and B has probability $p$ of winning. It follows that
$$p+\frac{9}{10}p=1.$$
Solve for $p$.     share | cite | improve this answer     answered Mar 10 '14 at 6:42       André Nicolas   442k 35 401 779              add a comment |       Not the answer you're looking for?                            Browse other questions tagged probability  probability-distributions or ask your own question .         asked    4 years, 1 month ago      viewed     3,330 times       active    1 year, 11 months ago       Visit Chat      Linked     3   Find the probability that a geometric random variable $X$ is an even number      Related   1 Calculate Expectancy of Poisson distribution with Poisson parameter 1 help on geometric probability distribution problem? 3 Probability, geometric distribution 0 Testing if distribution is geometric and finding is parameter. 0 Minimum of two geometric distributions 1 Probability and Name of Distribution 0 Finding the Probability of a multiple using geometric distribution 0 Tossing a coin. X is the number tosses. What is the probability that X is an odd number? 2 Simple Probability - Enumeration and Geometric Distributions 1 Interesting probability distribution of a mixed type random variable $Y$      Hot Network Questions     How to make people spread over the earth?    dividing words in equation    Why don't all objects bounce like rubber balls?    Why isn't the bracha "Al Achillat Matzah" said before eating the Afikoman?    What am I talking about here?    Automated way to create a directory tree    Why is first order logic not categorical, as Löwenheim-Skolem just reduces the infinity to infinitely countable    How to ensure two standalone documents have same dimensions    Can animated undead wear armor and use weapons?    What can I do to get models to take my small camera more seriously?    What type of aircraft is depicted in the Taylor Swift music video "Look What You Made Me Do"?    Hmm another Riley Riddle    What are healthy, productive ways to encourage students to progress to more advanced constructs as opposed to staying with the familiar?    When to partition the hard drive?    I have two siblings; we're locked in a war    Why was Fleur's little sister Gabrielle even at Hogwarts?    What's the probable cause for extremely low inbound traffic and high outbound traffic?    Story about a spaceship "emergency program" that simulates a "perfect woman"    Single Word : Cannot be resolved by waiting and trying again    In US universities, are the sport coaches typically considered tenured professors?    How does the title 'The Imitation Game' justify the story of the movie?    Splitting equilateral triangle with shortest curve    What is the most effective use of phoenixes in battle?    How can we teach good naming practice for students learning Java?    more hot questions             Mathematics   Tour  Help  Chat  Contact  Feedback  Mobile     Company   Stack Overflow  Stack Overflow Business  Developer Jobs  About  Press  Legal  Privacy Policy       Stack Exchange Network   Technology  Life / Arts  Culture / Recreation  Science  Other        Stack Overflow  Server Fault  Super User  Web Applications  Ask Ubuntu  Webmasters  Game Development   TeX - LaTeX  Software Engineering  Unix & Linux  Ask Different (Apple)  WordPress Development  Geographic Information Systems  Electrical Engineering   Android Enthusiasts  Information Security  Database Administrators  Drupal Answers  SharePoint  User Experience  Mathematica   Salesforce  ExpressionEngine® Answers  Stack Overflow em Português  Blender  Network Engineering  Cryptography  Code Review   Magento  Software Recommendations  Signal Processing  Emacs  Raspberry Pi  Stack Overflow на русском  Programming Puzzles & Code Golf   Stack Overflow en español  Ethereum  Data Science  Arduino  Bitcoin    more (29)        Photography  Science Fiction & Fantasy  Graphic Design  Movies & TV  Music: Practice & Theory  Worldbuilding  Seasoned Advice (cooking)   Home Improvement  Personal Finance & Money  Academia  Law    more (15)        English Language & Usage  Skeptics  Mi Yodeya (Judaism)  Travel  Christianity  English Language Learners  Japanese Language   Arqade (gaming)  Bicycles  Role-playing Games  Anime & Manga  Puzzling  Motor Vehicle Maintenance & Repair    more (33)        MathOverflow  Mathematics  Cross Validated (stats)  Theoretical Computer Science  Physics  Chemistry  Biology   Computer Science  Philosophy    more (10)        Meta Stack Exchange  Stack Apps  API  Data  Area 51        Blog  Facebook  Twitter  LinkedIn   site design / logo © 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required . rev 2018.4.9.29787       Mathematics Stack Exchange works best with JavaScript enabled                  Stack Exchange Network  Stack Exchange network consists of 173 Q&A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.  Visit Stack Exchange                   current community        help  chat    Mathematics        Mathematics Meta      your communities     Sign up or log in to customize your list.    more stack exchange communities   company blog           Tour Start here for a quick overview of the site     Help Center Detailed answers to any questions you might have     Meta Discuss the workings and policies of this site     About Us Learn more about Stack Overflow the company     Business Learn more about hiring developers or posting ads with us                                Log In  Sign Up           Mathematics       Questions    Tags    Users    Badges    Unanswered        Ask Question            _  Mathematics Stack Exchange is a question and answer site for people studying math at any level and professionals in related fields. Join them; it only takes a minute:   Sign up    Here's how it works:   Anybody can ask a question  Anybody can answer  The best answers are voted up and rise to the top         probability distribution $X, Y$ and $X+Y$         up vote  1  down vote  favorite       A box contains $5$ ticket, $\{ 0 , 0 , 0 , 4 , 4\}$.  Drawing two tickets at random w/o replacement.  $X$ be the sum of the first two draws and Y be the outcome of the first draw.  Question: Find distribution of $X + Y$?   What I did --
I found the distribution of $X$. There are only three possible sums: $0 , 4, 8$. And there are $10$ ways to get these sums:  $(0,0), (0,0), (0,4),(0,4),(0,0),(0,4),(0,4),(0,4),(0,4) (4,4)$. Therefore $P(0) = 3/10 , P(4) = 6/10, P(8) = 1/10$.   My other question:  Is there another way (different from what I did) to get the distribution of $X$?  And I have no idea how to get distribution of $Y$.    statistics  probability-distributions     share | cite | improve this question      edited Jan 30 '15 at 16:10       Daniel W. Farlow   17.1k 11 41 86        asked Feb 7 '13 at 5:46       hibc   48 2 9                  The distribution of $Y$ is given in the problem! I imagine you want the distribution of $X+Y$. – André Nicolas  Feb 7 '13 at 5:49             @AndréNicolas Yes I want to get X+Y, but I thought I need to first get the distribution of Y.  Y is given ?? really ?????  I'm confused,, – hibc  Feb 7 '13 at 5:51            Yes @hibc, it is given. There are 2 choices for the draw 0 or 4. It can be 0 with probabiliy _____ and 4 with probability _______. Also, it follows ________ distribution since it takes one of two values. – Inquest  Feb 7 '13 at 5:57        add a comment |           1 Answer 1     active  oldest  votes            up vote  1  down vote  accepted      For the distribution of $X$, I do not think there is a substantially better way than yours.  For the distribution of $X+Y$, we do something similar. If it helps to keep track of things, draw a tree diagram. Let $W=X+Y$.  Case $1$: Maybe the first pick was $0$. This has probability $\frac{3}{5}$. Given that this happened, the probability of picking a $0$ next is $\frac{2}{4}$. Then $W=0$. The probability of piking a $4$ is  $\frac{2}{4}$. Then $W=4$.  Case $2$: Maybe the first pick was a $4$. This has probability $\frac{2}{5}$. In that case, the probability of next getting $0$ is $\frac{3}{4}$, and we get $W=8$. The probability of getting a $4$ is $\frac{1}{4}$. In that case $W=12$.  So $W$ takes on values $0$, $4$, $8$, $12$.  For the probability that $W=0$, locate all the ways $W$ can be $0$. This only happens in one way, $0$ then $0$. We can see that the probability is $\frac{3}{5}\frac{2}{4}$.  Similarly, the probability that $W=4$ is $\frac{3}{5}\frac{2}{4}$.  Similarly, $\Pr(W=8)=\frac{2}{5}\frac{3}{4}$ and the probability that $W=12$ is $\frac{2}{5}\frac{1}{4}$.  As a partial correctness check, add up our four numbers. We should get $1$, and we do.     share | cite | improve this answer      edited Feb 7 '13 at 6:12             answered Feb 7 '13 at 6:06       André Nicolas   442k 35 401 779                  Thank you very much!! very detailed and clear ! – hibc  Feb 7 '13 at 6:09            You are welcome. I went into the gruesome detail in order to show that if one stays organized, things are likely to turn out well. – André Nicolas  Feb 7 '13 at 6:14        add a comment |            Your Answer             draft saved  draft discarded             Sign up or log in    Sign up using Google   Sign up using Facebook   Sign up using Email and Password        Post as a guest       Name     Email             Post as a guest       Name     Email            discard  By posting your answer, you agree to the privacy policy and terms of service .     Not the answer you're looking for?                            Browse other questions tagged statistics  probability-distributions or ask your own question .         asked    5 years, 2 months ago      viewed     733 times       active    3 years, 2 months ago       Related   2 Another textbook problem on probability 0 Hypergeometric Distribution Definition 1 Finding the distribution of a random vector in a conditional probability problem 1 Standard Error - Statistics 1 Statistics - Chebychev's Inequalities 0 Null Hypothesis and Binomial Distribution 0 Expected value and standard error question. 0 How to calculate the expected value in the following box? 0 Replacement Problem 1 Sampling distribution of random sample      Hot Network Questions     Crash during startup on a recent corporate computer    Why is the US Congress getting involved in the privacy issues at Facebook?    Count the number of lines in macro argument    Having trouble getting my friends to get invested in the game    What to do about a colleague playing pranks on a manager?    What to make with hundreds of 555 chips    How to reward students for learning from mistakes without penalizing those who didn't make mistakes in the first place?    Automated way to create a directory tree    Paths & Wasting Time    Distance between two points on the Moon    How can I avoid the awkwardness of a returning player who wears a low-cut shirt?    What am I talking about here?    The Folding Maze    Single Word : Cannot be resolved by waiting and trying again    How to build a trap to last the ages?    What should I play on a keyboard to accompany singing?    A Mathematical Paradox About Probabilities    How do I keep presenting progressively more challenging encounters to my PCs without making them think that the world is gaining Levels as they are?    What is the most effective use of phoenixes in battle?    Splitting equilateral triangle with shortest curve    Hmm another Riley Riddle    Why will the BFS reenter broadside rather than engine first?    Wiping an SSD with Parted Magic seemed too quick    Why do we need so many classes in design patterns    more hot questions     question feed             Mathematics   Tour  Help  Chat  Contact  Feedback  Mobile     Company   Stack Overflow  Stack Overflow Business  Developer Jobs  About  Press  Legal  Privacy Policy       Stack Exchange Network   Technology  Life / Arts  Culture / Recreation  Science  Other        Stack Overflow  Server Fault  Super User  Web Applications  Ask Ubuntu  Webmasters  Game Development   TeX - LaTeX  Software Engineering  Unix & Linux  Ask Different (Apple)  WordPress Development  Geographic Information Systems  Electrical Engineering   Android Enthusiasts  Information Security  Database Administrators  Drupal Answers  SharePoint  User Experience  Mathematica   Salesforce  ExpressionEngine® Answers  Stack Overflow em Português  Blender  Network Engineering  Cryptography  Code Review   Magento  Software Recommendations  Signal Processing  Emacs  Raspberry Pi  Stack Overflow на русском  Programming Puzzles & Code Golf   Stack Overflow en español  Ethereum  Data Science  Arduino  Bitcoin    more (29)        Photography  Science Fiction & Fantasy  Graphic Design  Movies & TV  Music: Practice & Theory  Worldbuilding  Seasoned Advice (cooking)   Home Improvement  Personal Finance & Money  Academia  Law    more (15)        English Language & Usage  Skeptics  Mi Yodeya (Judaism)  Travel  Christianity  English Language Learners  Japanese Language   Arqade (gaming)  Bicycles  Role-playing Games  Anime & Manga  Puzzling  Motor Vehicle Maintenance & Repair    more (33)        MathOverflow  Mathematics  Cross Validated (stats)  Theoretical Computer Science  Physics  Chemistry  Biology   Computer Science  Philosophy    more (10)        Meta Stack Exchange  Stack Apps  API  Data  Area 51        Blog  Facebook  Twitter  LinkedIn   site design / logo © 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required . rev 2018.4.9.29787       Mathematics Stack Exchange works best with JavaScript enabled     ]>>
startxref
0
%%EOF
             
466 0 obj< �v��w�)�c\H�2����	��� >stream
H���=o�0�w��S���M0<���!@�c3�ZE2$����{�7b���Z����xD�cA	�LA��/�HT����p��M�8�(�����&<��?}�~��	J��\g�ן���L*9�k%r77lF0����C���é	�=B膡n�����>>��[���pH���[���4Y���pa�̼|1>Iۿ���SZMDIK��%�]p�79VÈ1V��$�]��l��l5#X"��N��¦*6�k(�`�\ܯ��a��Z����۪X��wL�ȉ}ҩO�P KF4�h�&K;��Z�Ǣ��%��!H�c�M�U���?�̳">FVc2Ϳ!+����<��4"N�:�]x����3\_iy�#�8��"K� ���J�	��X+0��|!�Ӿv��~���ƻ���������w7��@��)�d��.e�fF��'0.��f��њQsAp�D̚|�?�e�Kt��E�f�= �0�����\P���G���>�N�C�� {B��k}�����'t����"/3��? ��su
endstreamendobj479 0 obj< >stream
H��UMk�@����ޙ��B�!�B�ѡ�^6��(�������	��U(�X���{o�}��l�	#�}�}�\�*�w� mx��Ĺ���ŕ$� �\�>�X<��I�������P��,D���8�&�q���^�zj�=LM��㸭�0ő1tпd}�d����8�k;5�+b��������!��n�'q�?����B)KnY�O!�퓡��1�)�Z8Y*\�g��%����a���q�~��	�ͻ�.��@�ĞWՎP�������?H��e��|�zf1�j�5�W�Q���qc#�C���w�{�j$��͈TY#��@�=�(�l'Q�9�CF�`�VY���LM\�}����X�(��Q'p�	�d��M���m����m�S[���Mq����#���O�F�+� ��p"
endstreamendobj482 0 obj< ���#:�
�v��yf����G�P/Y�1	�?E�qu��n������0�I��!���ŉwM[��%ҭ��]�!���	�{���6*�[���� �` ��jw
endstreamendobj487 0 obj< K���Jg����PC8�L�m6J:�E��R`.�Óp�$(W(��Ze�7E������9�Ið��R.�������u�Ϝ���E��+������5��%��;�;��{��'�״�v�y���#�ߨ|�6��Ͽ�������n�xsV�9�����C���3�V���z�R��eg�3B�r�F|�lKc��w���������\�4>q��ƍg��ħ#�0Y(��H	��vg��%�Eg6?h(��t-��	�2�����h>�ɫ���u%���9��j��^y����b��%��]Ʃ��6p�Pq�����\��2g<��V4W��̶���[��:?g���Y�������V�W,x����Ó�P������x�>���י�1��9��2��p��_7H�a�����s/w��N�-�/χ�����<�	�+n������꟥�B��tJ��n7n]T�Xy��{���X_ɕ���(�4�,���i�ꭵ�o=�2(S��؏��b���l���'L��R�[�)m{Dq���E{���]��Q�X��y�O���H�p�d]ԃ�M���>�/X��s�q�ĳ�w1�_Hh˴�U����թ"{�>`H<��-��0�|��4l𔆲�kȣO.]�Uu}a�o;�#�ܪ��_x�gI3�[`:�{��_���pzT�C���G��Z�\�����-�yך��%���ѳk�.���<;"�}q��4�=A,��9�6��Y�b2�L�¬*��m��X�������(�u�p�o�P8&�x�P�,�a0��Pcˢ�u����&�H8H�����(l�4�^�$-���"e��1'���8	i�k���/�C�$2�l�Pԁm���9
C�z��.��B�&�$":�%I�x:�#����^?�xS��B��Pa֑$�b�l28l$���#]g6e�(��
�¤|̍��
�i� �Wo��4�ңlV���{.j"�b�fJ�`���%��s�@m��/��o�G߬"'� �+��`��ʓ�o���M�/,E��S[pN��W��aWN�"���i��"�k�p��G��-�n�j[0�:,c���3�W5]���s�ǎ��[����e�\��'�))�l�j���ąh��]o^�����.��H?�s4`p����:��l�*#�Yt�M��������n�l_ޙvX�iÁ���Kﲆ����dK��Ew��|Թ_r?w����lY�sx_���Z����3��/�h��m�����U���=ne���vDe^|6��<K��m�.��7891'�\Ot�������'��brj��rZb�KK��%^EU=��6��?�5�'�s��PHN�}b~46��=b��_ߗ�@ӥ�h#���y�X�Ɏ��ѦNv R�	*f�(;�/
c������j�M$4ʔe2�(�z
�N6����Ad�j DPg̈́&���$����&e.䑹�鄁��M)#��_�^R:E7DԚ(�BX)81	�!�$�J0�H��d���4�׽�m �(��6M�V�-��A���A��$E�}gs��%����`�,2�Q�:�<�@�d[���!V�&"_�B#G`#�yi9���l#E7Iidd�� ���PM#H�CD��LdJ�B���qo�\�����R�q�"I�'+�\߯'��8j��V��(���i%LI@������+�Z%D�F��ڤIP�7^��Bm
m�KW�q��J����`�Z���
%�C��*-�M/�k4g��â8�����T���"j����1r���ˍ(0�0���	����BM!+芫+�5DMD�U?�x��\v���������Tu�������}q=��w����"�1Rك@p��vw}k�b���B��P!	��Y�zeh������=(�|'�E�Zu'�o'xۡ���v>������R1�k�Ů��"����$wE�����R���w�s��*���\����녨|G;;g��RP*2-N�~���Q��ܕȴ�
s?]�
���qX�B�U�6��#�B�݉`����	$KB���N�J�VaĨ��0!D��B�&	��BBb�30\ݕ3����G`�j-p����VO�oҼ�?J��������V�K?*W����M.d�K����*<�t���8?�$�I�~r����+�h^O�Y\��};qoM�"��-��x̅>]B�%�N�3�t��v��w�ў�v�H�m�_�ٴ���U�E�Sʾ�ug�J\���[�\h�����YI�y�u����]v_�Y2e���b��Y48O��n�s]��NG����n+���q��j�Ď���Ŝ�	I���h�ocΛ�M3�-ێ̚i{��_2L�2��X�� ty��`��7����nR(����c�Q�,`V�n,�1�Z�V����>E�J���!nj�Gy�����b���{�3|���|����᠄d��`PA��1_2A=^Mo�)��P�M�	a��k�~��H�eXZoP��movЏg�<�/�U�>*��V�{qé���y9����q���׍:�S�1���\Q���k-�?��W'��Z�}�)�AW��o]�ِ��WJ6*9{��c��`�QI	��U�6ߨ�o
?[sϑ��B�b�N���e�S�~;&���s��=�Z�d4f׌[���~'rI�B7z��ӏemzhsG��z[s}KԤg��[y��,��bZc�����Q����I�k��'{�6i\W�"T�;2R���ւ���s[2�^�;���|&������K2�5<�5��Bb�A�b�im��W�߿�RC8;�����7NK2��씕o�dX`�@���|7Nu{R�a�����ӾzX��]Cn5c��������n�U��.�0ة��?��Պ����3�S�{ps�+án�Mi5b��mܳ�[�3�����G�볅�E������i�/���:�xh� ��͗�eNMVs�����>5+ ��MѸtG�mFN�������K֏������=6_��ٺ�,N[�w����Ӱi����C8��x�P"�P^
�.U�
�w"�E�D���Zu�S~9X��0��l�w���5��c~5�hV����p�o��+LaX���8G&�7��A,���؟��<�G
F$���D� �a6g$V�;<�у�0�H�"`����s؀3dC,~/��d>! ������/���-�	]�B�
���@
��;�=�� �B)��bI�v�D����X�5`�	�mP���ej��Fb��G�"�F��u\������z�%k^�1b�0mF�Q��'��a�o� �*�h�f�8�0+�d��+�zȂ�����<$���
yBQ5�潥�R�՝ߊ��s\c �Ck�jHB�m�v�f	��'��$�Ć�W�C��&r;������������Y*ka�t�N/�n�5�[LB.	r.CO: �E��@|��F�rQ�2�r����PZ�!<���LB��ؐ�d�%�'$��&'I9G.�v�N��t6��>4����4���
ZM��_��9L���G��ղ��:k�[�q�\��+��:�����Œ��wиiĉ��,n�Q"�c�D0C<����Q"�X����mDD�`r�e�$T������:�͈�6��x��h��q�}b���#�(K�O�$��I.)D�+H%�yrQj���+h"M�[�n�����izBd��H����b�V�x��}�>a�X1�b�YG�9�7�m���	��k�n�rކ�A)�+��|�D_b,�.QJ���d�6|�P��s�d�!�>#m�cj�@�P]�D2���=0� �����h�{�:�I���� �$ ���� [ |Q2o
Jn7��A|��Q>�u�W�VB.]�Y*����$����4��4���ibJ-h��8�[���fst��u��C3�:z�~c�zc����������c�Ǳq�܇Ǆ������C�0?��',A�`j����L�\Ub�6iՄ:�N�u�Su,������i�(��Ci�����s_��thڳ��{�=��{�ﾛ���s��}�	w�ߕ�1�%���4t�K�*�W�I�d3�D�.�]������I-}`�b)L#���f�5�~�����F��񭑶v�'����o���~���q����K��Ύ��g���ڲ�y[�)�ذ��οEڧy��lv�j�5�U������Jm%�"K"~�BSL�Kj�.��:}�� ��)���LCQ��6LKZʹ�-ly�-���jKb�B
6i1]c�Fu-O&���M�ݵ�}/�Y�r��|h�ŜQ���c}S�X2���Jm=2n6A�V�l)r�F?�#5��bhM�3GA-Ǩ�K��X��!0�K����D,����`#��>�@�eV�Xn�a��F��Á�Z�i1�j���@٘>�:�`B��>*�7�j���q;wD��j�B&��x5�9��7k�>^�&����ߗ����W1��a��i3��4���H����7�Ǹ$yTc%z�>�9�Ĺqe��͹\�B��bZf$��X�[7S�͹J����5����`S�^���܆�E��|-3���8�9��C��%<"�9\LKkIB�1��b�2�vl��IЊ��L��H2c��rn�$�]�2�� ���%��D��?��u���P�³@�56�%�DpN1�n��l�i*O��q����۔�ٌ������0�vf0�\�`�=Fs�d4�5�+���\sfE�j��q%_�����u�����M��NF���z|Y��Z,�,�6>����o_�9�)�ܴ�Q�`iqQ^m�+�2&��/[�z,���*-	���=�{�4m>��i�/|̭,�ج&���w����,#`�b���dlku����>������nS_�Ҹ��&���*>��C����H� ~q�Y�%w�n�,t�nD�k��c�o�k��P@��ǈ&�0BC�"L�^�w��~�8��!nϩp�����x�C�.���n�-^eu��T�����P�x�M�X�+��͇ËP'��h�%MC5ƾu�R�ʇ��-��~~)��E�G��
� ���8N"��{C��� �����C��"Hc��B��?�1`+�<�6�M��ئm�߅�c��'pi3�{H�3\'?�KH�����&��7Dp��f'�
dd�lG� q_= �m�c�/�Pa��~��ӓh������b�9�p_*��:�P�pǮ�q�OCs��6����EH!�ǁ��#�]EtJW�Q��a� ͡x�m����6P��P�o1~�b�͘������& 8`x`��{��s,J.��	��-x:M�\DG�u�A_X��#�}��-���U�����up�T����U>o�=V��K�<����鶹78i��4 �b��ty[��P=��]_o�ޘ���D�U��}%��=�hFLa����m��?��c? ���g�����/��"Y,�g`�8�x�	~�"/�+��.Fi֒f�4kI�v1>�`���r��1�|��q�$��8"��OM8ٙQM�W�u�����qvU�2C�j��'�OquX���Tl$�;e�G��F8�������\��:w?Zq� �d��{̓Q�e#��?��3\��=�p�3�c��oy�M���@"�B����E�i��*��������ּu��/���\�R<����U�p0�U�`�j��+��/w��o��E��z/N���p�&��Ba���sx[&?g(?�����'��xJV�(�Ĵ 6YJu�(b�@��������-����C��K!�	-�8Z��*|~,pm�CMX|hH�9�8�|}��,;ފH|��j��B��t�m]v��C��ݽ��忖��*����}�z�A��; |(> l�3j�R��'O�e�_�L��w	|a�<=c�ʸ� ��\wHb��WQ���Dd�<� �3��,���8Q �U�l� 뱲K�B�s���u�]��r�ȭ�%b���F}UlT�e�񜳓���8Szg��4���қ��B�h8VG��?��\%8����V��fq���4����r�Z̓Y+��\�]��,��ୣ?��Ѽ`����<�i��U�Y���'�9m��W�[����v�2������s_�}�'6���؉oc���|�����{��8�`���t�܂T�L��<*�L���HQ�j�YP;���*� �}�(�5��ox�o�0��â��RF`�H���#�	fx�g��L��F�dZ��"M,*�VGS��J�ZJ��hhQ�ai��J�V�X<,��A<����k��L]�l;���z� ^�oc]P�G#��9-U��ad�����W�gF��qЈ�aÈ��6r�z����=��o/fLN���43D߄�	b�*30��ɡh3������"�蛔cn��:#�������>}cC�ه&�	m2i�o�we�Ev�RN�$�ES��l2ć:����~��C��H�h8��qfN�)�w�x�{1�������kX��&��bɾ�x��s�%�MgR�)��ڨ}%� 7�1u3���d/b��6���C���$�M�,��O�g3C�0�uE�a���Hx"�$�EnCuJ�x]Z�Ā{����N�"�J��
7n��^�~��~��)��kf��|<�{��fNV�K�`W=>�H��P�4�d1>��*n"�a� ��e3b�R�>EGD_F&���,DZ<�P;c�cӑ�"N�sm�o��m�(!V���<"�@O�� �p�(�g!�d�n0s%� ���ƿ�R,{�����<s.mDmJ*-�+���g�Y�uY�''����}F���^��,WG�#P F��p<���KӔ�� H9n���=�0�X�����sT;��gH�U��]ϔzK�yӵ�>)� V����<�{�6ۜ�k��>�}�^Y�,ty�L��x�tr $�Ut�b�ㄌQ>��	R�2��L�I�,�!�@�ږV���?��[x�3�*�2Ե6�%�t�:����"�J�3�&�B������u�<��]\G?;��;��{����-+�������w͊P���&
�^���V$�f�<:w �K��P'�=�Q<��V��S }�4�M��T%�'�d���G�D�'�t��<�����QaBrH��0#�洁=Ҭz��w� ϕ�y�j��A�i�נ�j~X_�����ǌ}��8:/�Z��F������L���ɋ��]W���ߗg�%��y祊s�滟��c��w�
h*�@�� ���

endstreamendobj489 0 obj< ����y�7�ȖK_m�#k����{�r�R5�$�7#.�y?F�RCXed��`p��-�靽�o�������������pv]mz���a���k{�'*�m "Jҽr���Һ!F+i�H�g�'�J>�oJ�����%%7B�y�.����Y`!|��D�B���t�����o���AF��+��vZ=�0z�)�9����j�i�mg��Xs(�Ms!�o����USAA�����=\yP]	Q�R-�M��6����2�+[��ɚ�r�_,��O�#��Pk�7�<� 0�����CQV��%JR+���n�N*GY�X�;��0�6����킍���s4'�-�F� 0�H�U����qyMLr���Fҩw���� �&����0�ǎ�#sce(lw��q�vԌxz7�[�n@�N��(Q�l��V��;��.T����?��A�8�E��0�HhH�A�0Һen��mHiM!�άCv�|ۙ��WƇ�R��8�F�·`�&�`�o��Ivo16�AW�,1�� �Sg��8c�s������*� *X0X۫-�O�iA�`D08i�^�=��",5"�� �4[��Cb��sm������t���>=�%:I%-�50̛�D���ʸJ+��u�Vd7� ��q:�Q�m�txF��dQ��*=$k�%���P��+���?�&�׉�밾*�Ǭ�QOF)1/�ٯv{�g�C4%HܸR�Q��}��n�Lm:	�R����qU�s/����,���?Od|���!K�A&���~W�O|�x���َ�.g�a��$>YvX����WҮ[Jm��������(W����ۧ�w�0��%?Qap(cs����}ʕQP?,l�1���r9�<�2�&E;"w�O�"��d�n�戮J�f>��P�ǚ�	�tbj��� �~t�4�y��CXdX�:RxZ;e��Xl�p��R!�|�L��g�)�����F;YrF9��{�w��r�yU���,�����O>���Hޚ8'�8ۙc@��o������n��8�wy�7���yT���z���H<�(6G1&Z;�s]��C -�8���Ou��B�QZ �����Y�>vzKo��N�>",�C��4q=�NX���~,F��d[���;[M�25,I�i7,|�!�����N��e�G	�"��eB��ݝ�P;�]�ΦfdY��'�ڞ5���ʏ�}B�����w(��x"vJ���o�m�=���,|�\"&�T>mNHM����"��zwf�������Va��f*�T(DwϦ�g�"-l���T�F��2)�ghG���H �
�z��V�O0/A�z�v̻.��5�Paēk���V�d}��r�dSV?|U��#��/�����gQ�>�-�rz���EPk����0i�#|�muf���kPC�`㩣ػy�o�*�ϐtMp8l�'t_pIAW@��d�΍�"r�m��Ϋ��i����Ab��؉��>�'�xZ'�u��4U�Mݿ�\;I��Ni������>��sI����Hv��?GH���gp�r��)Gh���`|m뼡��$w��2bgD+'�!lf����up7�j�pW��.�+We��.���ؑ��v%8P��b9a��r�Ly$���Hh�Y�)�(��ȶ������h��tt�4g�|���e�'�}�ZQ��s{�|Z/(��;
Aime��Z���{�Eh[���� @��kp��`<�B�5 ެ��&�?Ÿ�7�Y��e��DSP�85|��A��(`�C��4�@#S����S�A--y�i1�6��)<�8�3�v�/�ŏ�M�fW�C�v�bv~^��ͨ:�Z�	���p/�p�f���5d�x3;�����n�J̾_0T���_"0xE{���(�F��ƭ�hq��7��'{�/���M
!c���{�H�l�`_��0�P�jv�P���Qy��? oS}
endstreamendobj3 0 obj< S(Dѥ�XfL�����g�1�%8���x	�2�;���X��:��Yg4�*r\��fk8>��mZH�[UE���7D�Xk��{q�p�>��G���o�#β,��N�ju��]~V��1p�x�3�or�K�Hn)� ���;ddvOW7_2qbL�?�!}��מp�Q+�3W~g�c!���!ɼ����8adX�Q��T�]�_3�32��9�=Ծ�Q���Ӭv�WWl�����ۢ�vhtZ���4�Zs�WXU�n���k�"<�/�%�?���q�Y�1K#e�\��[�D�J��,��$�����c�� �;�r�dI�r�c	�&E;Ʊ���!TĒ�GŊ ��O��
ey�����Ly�0����s��-��^�!U�Ř���L
�"c��<��_tˉ@V��V��F*��JO[�_Z�,�G�/:�&�d ����[�L/��Y��4�OU��T��/z�_�3mhPK�}�|�ђ�"���Bʮ�99���*zoC}k��ˬ��ˣ�F=ݬ���?�N/''�`]@;��>�T�E���S��m������ 5���`���8�	"���HK���D�O��B�!m^Y�C	O�qxɨ\<_Z��e Y�"J�	8���G�Bh�����۠o��BDT�������/tM��E�Ǖ�yƟN��=3SV��mY[�ee~}S�������;�/�ʡ�g�5^ݔ��O��
�p��J�e���	���r�hU:��E���C�E��c�S��=�U�&�2�%[��X��=*m��|0��*i�׎�3�v���!�.׳��J:V���{�5ԻqWV��SFQ��iiWР�Y�hb �b8��k��]�f�^Wő�{�a�c_;G/��'/�*�9ѵ�O�#X�r�e\�!l$m�hDR��x[�5P|�+e�����@�j�-�Gpx�ڭ�����<"���O�'�rX ً.?-Y4�0>Iy������G��7��S��T����؋��������z�s�o{0��&���lA߸B�ֳ�~eo1�1T�:���Jt��k���r��D����ЈC5���6��4��3��=��2�}�h���u��Sʥ�XW������@f����n��7Kv�W���FJ$Q�!(��*9�)L��6�U���Wy �[?�ld;ߙB�ȼ3���}�!���B��NiH))�}"�Q	�M"ו��KIE�Ң�Oh�wP�-ߟ���Ǚ3�y�9s�<����o�O�o�#���F���G��	�o0�L\2s��R_r�WKʼ: ��H0ɀ��S$��$�Lb"��G���������/���zb�s�D�Pl�Ԓr�j����^�<���U/�d�h�|]���,�d�_п�&�hªn�����{�\�ZW��耥�n��ϗ�o�rQ�
���9��uQ�>��`�]�]ZU=��Vh� t���H9=ybb9ϯ�I� ړ-��uA�|;y�b�綉��+Z�u�9G��8H� �	�W���6�/�����;*cI�W���2�W ���+B>�3+y�:a,&#            Search       Statistics How To   Statistics for the rest of us!       Home  Tables   Binomial Distribution Table  F Table  PPMC Critical Values  T-Distribution Table (One Tail and Two-Tails)  Chi Squared Table (Right Tail)  Z-table (Right of Curve or Left)    Probability and Statistics   Binomials  Chi-Square Statistic  Expected Value  Hypothesis Testing  Non Normal Distribution  Normal Distributions  Probability  Regression Analysis  Statistics Basics  T-Distribution  Multivariate Analysis  Sampling    Calculators   Variance and Standard Deviation Calculator  Tdist Calculator  Permutation Calculator / Combination Calculator  Interquartile Range Calculator  Linear Regression Calculator  Expected Value Calculator  Binomial Distribution Calculator    Statistics Blog  Calculus   Derivatives  Integrals  Limits    Matrices  Experimental Design  Practically Cheating Statistics Handbook  Navigation             Find the Mean of the Probability Distribution / Binomial    Binomial Theorem > How to find the mean of the probability distribution  Contents:   Mean of a probability distribution  Mean of a binomial (by hand or TI83)   Mean of a probability distribution How to find the mean of the probability distribution : Overview A normal distribution curve is one kind of probability distribution. Finding the mean of a probability distribution is easy in probability and statistics — if you know how. This how to will guide you through a few simple steps necessary to find the mean of the probability distribution or binomial distribution . You’ll often find these types of questions in textbook chapters on binomial probability distribution. The binomial distribution is just a simple trial where there are two outcomes: success or failure. For example, if you are counting how many times you draw an Ace from a deck of cards, you could assign “Success” to “Drawing an Ace” and “Failure” to drawing any other card. You can find the mean of the probability distribution by creating a probability table. How to find the mean of the probability distribution: Steps Sample question : “A grocery store has determined that in crates of tomatoes,  95% carry no rotten tomatoes, 2% carry one rotten tomato, 2% carry two rotten tomatoes, and 1% carry three rotten tomatoes. Find the mean number of rotten tomatoes in the crates.”  Step 1:  Convert all the percentages to decimal probabilities . For example: 95% = .95 2% = .02 2% = .02 1% = .01  Step 2:  Construct a probability distribution table . (If you don’t know how to do this, see how to construct a probability distribution ) .)   Step 3:  Multiply the values in each column. (In other words, multiply each value of X by each probability P(X).) Referring to our probability distribution table: 0 × .95 = 0 1 × .02  = .02 2 × .02  = .04 3 × .01 = .03  Step 4:  Add the results from step 3 together . 0 + .02 + .04 + .03 = .09 is the mean.  You’re done finding the mean for a probability distribution! Mean of Binomial Distribution A coin toss is a simple binomial experiment . A binomial distribution represents the results from a simple experiment where there is “success” or “failure.” For example, if you are polling voters to see who is voting Democrat, the voters that say they will vote Democrat is a “success” and anything else is a failure. One of the simplest binomial experiments you can perform is a coin toss, where “heads” could equal “success” and “tails” could equal “failure.” The mean of binomial distribution is much like the mean of anything else. It answers the question “If you perform this experiment many times, what’s the likely (the average) result?. Formula for Mean of Binomial Distribution The formula for the mean of binomial distribution is:  μ = n *p Where “n” is the number of trials and “p” is the probability of success. For example: if you tossed a coin 10 times to see how many heads come up, your probability is .5 (i.e. you have a 50 percent chance of getting a heads and 50 percent chance of a tails) and “n” is how many trials — 10. Therefore, the mean of this particular binomial distribution is: 10 * .5 = 5. This makes sense: if you toss a coin ten times you would expect heads to show up on average, 5 times. Mean for a Binomial Distribution on the TI-83 Sample problem : Find the mean for a binomial distribution with n = 5 and p = 0.12. Again, the TI 83 doesn’t have a function for this. But if you know the formula (n*p), it’s pretty easy to enter it on the home screen.  Step 1: Multiply n by p. 5 * .12 ENTER  =.6 Hey, that was easy! Something to think about: You may be wondering why it was so easy to calculate the mean. After all being asked to “calculate the mean for a binomial distribution” sounds scary. If you think about what a mean (or average ) is, then you’ll see why it was so easy. In the sample question, n = 5 and p = 0.12. What is “n”? That’s the number of items. So imagine a list of 5 items with a certain score: 1 = 0.12 2 = 0.12 3 = 0.12 4 = 0.12 5 = 0.12 If you were asked to find the average score for those five items, you wouldn’t even have to do the math: it’s just 0.12, right? Finding the mean for a binomial distribution is just a little different: you add up all of the probabilities (0.12 + 0.12 + 0.12 + 0.12 + 0.12). Or a faster way, just multiply n by p. Check out our Youtube channel for more help and stats tips!     If you prefer an online interactive environment to learn R and statistics, this free R Tutorial by Datacamp is a great way to get started. If you're are somewhat comfortable with R and are interested in going deeper into Statistics, try this Statistics with R track . Facebook page Find the Mean of the Probability Distribution / Binomial was last modified: October 15th, 2017 by Stephanie   By Stephanie | August 24, 2009 | Statistics How To |   ← Construct a probability distribution in Easy Steps  Binomial experiment: How to figure out what is and what isn’t →   4 thoughts on “ Find the Mean of the Probability Distribution / Binomial ”      Selin Asar May 30, 2016 at 9:20 am    A building venice is designed by taking 20  years return period water level. a) Calculate the probability of water level exceeding the design value less than 2 times in 20 years with Binomial distrubition. b)Calculate the probability of water level exceeding the design value more than 2 times in 60 years with Poisson distrubition.          Andale May 30, 2016 at 10:39 am    Were you given any probabilities for the water level exceeding the design value? Without this info, the question is impossible to answer.          M N Choudhary May 5, 2017 at 3:48 am    Assume a banks 95% value at risk model is perfectly accurate,If daily losses are independent, what is the probability that the number of daily losses exceeds the Var on exactly 5 days out of the previous 100 trading days.          Madison September 27, 2017 at 7:17 pm    This helps so much! Thanks!         Find an article   Search       Feel like "cheating" at Statistics? Check out the grade-increasing book that's recommended reading at top universities!                 Privacy policy.     Copyright © 2018 Statistics How To Theme by: Theme Horse Powered by: WordPress    Back to Top          Probability distribution   From Wikipedia, the free encyclopedia   Jump to: navigation , search   This article is about probability distributions. For generalized functions in mathematical analysis, see Distribution (mathematics) . For other uses, see Distribution .         This article has multiple issues. Please help improve it or discuss these issues on the talk page . ( Learn how and when to remove these template messages )         This article includes a list of references , related reading or external links , but its sources remain unclear because it lacks inline citations . Please help to improve this article by introducing more precise citations.  (July 2011)  ( Learn how and when to remove this template message )           This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.  (July 2011)  ( Learn how and when to remove this template message )       ( Learn how and when to remove this template message )     In probability theory and statistics , a probability distribution is a mathematical function that, stated in simple terms, can be thought of as providing the probabilities of occurrence of different possible outcomes in an experiment . For instance, if the random variable  X is used to denote the outcome of a coin toss ("the experiment"), then the probability distribution of X would take the value 0.5 for X = heads , and 0.5 for X = tails (assuming the coin is fair).  In more technical terms, the probability distribution is a description of a random phenomenon in terms of the probabilities of events . Examples of random phenomena can include the results of an experiment or survey . A probability distribution is defined in terms of an underlying sample space , which is the set of all possible outcomes of the random phenomenon being observed. The sample space may be the set of real numbers or a higher-dimensional vector space , or it may be a list of non-numerical values; for example, the sample space of a coin flip would be {heads, tails} .  Probability distributions are generally divided into two classes. A discrete probability distribution (applicable to the scenarios where the set of possible outcomes is discrete , such as a coin toss or a roll of dice) can be encoded by a discrete list of the probabilities of the outcomes, known as a probability mass function . On the other hand, a continuous probability distribution (applicable to the scenarios where the set of possible outcomes can take on values in a continuous range (e.g. real numbers), such as the temperature on a given day) is typically described by probability density functions (with the probability of any individual outcome actually being 0). The normal distribution is a commonly encountered continuous probability distribution. More complex experiments, such as those involving stochastic processes defined in continuous time , may demand the use of more general probability measures .  A probability distribution whose sample space is the set of real numbers is called univariate , while a distribution whose sample space is a vector space is called multivariate . A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution ) gives the probabilities of a random vector – a list of two or more random variables – taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution , the hypergeometric distribution , and the normal distribution . The multivariate normal distribution is a commonly encountered multivariate distribution.     Contents    1  Introduction  2  Terminology   2.1  Basic terms    3  Cumulative distribution function  4  Discrete probability distribution   4.1  Measure theoretic formulation  4.2  Cumulative distribution function  4.3  Delta-function representation  4.4  Indicator-function representation    5  Continuous probability distribution  6  Some properties  7  Kolmogorov definition  8  Random number generation  9  Applications  10  Common probability distributions   10.1  Related to real-valued quantities that grow linearly (e.g. errors, offsets)  10.2  Related to positive real-valued quantities that grow exponentially (e.g. prices, incomes, populations)  10.3  Related to real-valued quantities that are assumed to be uniformly distributed over a (possibly unknown) region  10.4  Related to Bernoulli trials (yes/no events, with a given probability)  10.5  Related to categorical outcomes (events with K possible outcomes, with a given probability for each outcome)  10.6  Related to events in a Poisson process (events that occur independently with a given rate)  10.7  Related to the absolute values of vectors with normally distributed components  10.8  Related to normally distributed quantities operated with sum of squares (for hypothesis testing)  10.9  Useful as conjugate prior distributions in Bayesian inference    11  See also  12  References  13  External links     Introduction [ edit ]     The probability mass function (pmf) p ( S ) specifies the probability distribution for the sum S of counts from two dice . For example, the figure shows that p (11) = 1/18. The pmf allows the computation of probabilities of events such as P ( S > 9) = 1/12 + 1/18 + 1/36 = 1/6, and all other probabilities in the distribution.    To define probability distributions for the simplest cases, one needs to distinguish between discrete and continuous  random variables . In the discrete case, it is sufficient to specify a probability mass function      p    {\displaystyle p}   assigning a probability to each possible outcome: for example, when throwing a fair dice , each of the six values 1 to 6 has the probability 1/6. The probability of an event is then defined to be the sum of the probabilities of the outcomes that satisfy the event; for example, the probability of the event "the dice rolls an even value" is       p  (  2  )  +  p  (  4  )  +  p  (  6  )  =  1   /   6  +  1   /   6  +  1   /   6  =  1   /   2.    {\displaystyle p(2)+p(4)+p(6)=1/6+1/6+1/6=1/2.}     In contrast, when a random variable takes values from a continuum then typically, any individual outcome has probability zero and only events that include infinitely many outcomes, such as intervals, can have positive probability. For example, the probability that a given object weighs exactly 500 g is zero, because the probability of measuring exactly 500 g tends to zero as the accuracy of our measuring instruments increases. Nevertheless, in quality control one might demand that the probability of a "500 g" package containing between 490 g and 510 g should be no less than 98%, and this demand is less sensitive to the accuracy of measurement instruments.  Continuous probability distributions can be described in several ways. The probability density function describes the infinitesimal probability of any given value, and the probability that the outcome lies in a given interval can be computed by integrating the probability density function over that interval. On the other hand, the cumulative distribution function describes the probability that the random variable is no larger than a given value; the probability that the outcome lies in a given interval can be computed by taking the difference between the values of the cumulative distribution function at the endpoints of the interval. The cumulative distribution function is the antiderivative of the probability density function provided that the latter function exists.     The probability density function (pdf) of the normal distribution , also called Gaussian or "bell curve", the most important continuous random distribution. As notated on the figure, the probabilities of intervals of values correspond to the area under the curve.    Terminology [ edit ]  As probability theory is used in quite diverse applications, terminology is not uniform and sometimes confusing. The following terms are used for non-cumulative probability distribution functions:   Frequency distribution : A frequency distribution is a table that displays the frequency of various outcomes in a sample .  Relative frequency distribution : A frequency distribution where each value has been divided (normalized) by a number of outcomes in a sample i.e. sample size.  Probability distribution : Sometimes used as an alias for Relative frequency distribution but most books use it as a limit to which Relative frequency distribution tends when sample size tends to population size. It's a general term to indicate the way the total probability of 1 is distributed over all various possible outcomes (i.e. over entire population). It may for instance refer to a table that displays the probabilities of various outcomes in a finite population or to the probability density of an uncountably infinite population.  Cumulative distribution function : is a general functional form to describe a probability distribution.  Probability distribution function : somewhat ambiguous term sometimes referring to a functional form of probability distribution table. Could be called a "normalized frequency distribution function", where area under the graph equals to 1.  Probability mass , Probability mass function , p.m.f. , Discrete probability distribution function : for discrete random variables.  Categorical distribution : for discrete random variables with a finite set of values.  Probability density , Probability density function , p.d.f. , Continuous probability distribution function : most often reserved for continuous random variables.   The following terms are somewhat ambiguous as they can refer to non-cumulative or cumulative distributions, depending on authors' preferences:   Probability distribution function : continuous or discrete, non-cumulative or cumulative .  Probability function : even more ambiguous, can mean any of the above or other things.   Basic terms [ edit ]   Mode : for a discrete random variable, the value with highest probability (the location at which the probability mass function has its peak); for a continuous random variable, a location at which the probability density function has a local peak.  Support : the smallest closed set whose complement has probability zero.  Head : the range of values where the pmf or pdf is relatively high.  Tail : the complement of the head within the support; the large set of values where the pmf or pdf is relatively low.  Expected value or mean : the weighted average of the possible values, using their probabilities as their weights; or the continuous analog thereof.  Median : the value such that the set of values less than the median, and the set greater than the median, each have probabilities no greater than one-half.  Variance : the second moment of the pmf or pdf about the mean; an important measure of the dispersion of the distribution.  Standard deviation : the square root of the variance, and hence another measure of dispersion.  Symmetry : a property of some distributions in which the portion of the distribution to the left of a specific value is a mirror image of the portion to its right.  Skewness : a measure of the extent to which a pmf or pdf "leans" to one side of its mean. The third standardized moment of the distribution.  Kurtosis : a measure of the "fatness" of the tails of a pmf or pdf. The fourth standardized moment of the distribution.   Cumulative distribution function [ edit ]  Because a probability distribution P on the real line is determined by the probability of a scalar random variable X being in a half-open interval (−∞, x ], the probability distribution is completely characterized by its cumulative distribution function :       F  (  x  )  =  P  ⁡  [  X  ≤  x  ]    for all   x  ∈   R   .    {\displaystyle F(x)=\operatorname {P} [X\leq x]\qquad {\text{ for all }}x\in \mathbb {R} .}     Discrete probability distribution [ edit ]  See also: Probability mass function and Categorical distribution     The probability mass function of a discrete probability distribution. The probabilities of the singletons {1}, {3}, and {7} are respectively 0.2, 0.5, 0.3. A set not containing any of these points has probability zero.       The cdf of a discrete probability distribution, ...       ... of a continuous probability distribution, ...       ... of a distribution which has both a continuous part and a discrete part.    A discrete probability distribution is a probability distribution characterized by a probability mass function . Thus, the distribution of a random variable  X is discrete, and X is called a discrete random variable , if        ∑   u    P  ⁡  (  X  =  u  )  =  1    {\displaystyle \sum _{u}\operatorname {P} (X=u)=1}     as u runs through the set of all possible values of X . A discrete random variable can assume only a finite or countably infinite number of values. For the number of potential values to be countably infinite, even though their probabilities sum to 1, the probabilities have to decline to zero fast enough. For example, if     P  ⁡  (  X  =  n  )  =     1   2   n         {\displaystyle \operatorname {P} (X=n)={\tfrac {1}{2^{n}}}}   for n = 1, 2, ..., we have the sum of probabilities 1/2 + 1/4 + 1/8 + ... = 1.  Well-known discrete probability distributions used in statistical modeling include the Poisson distribution , the Bernoulli distribution , the binomial distribution , the geometric distribution , and the negative binomial distribution . Additionally, the discrete uniform distribution is commonly used in computer programs that make equal-probability random selections between a number of choices.  When a sample (a set of observations) is drawn from a larger population, the sample points have an empirical distribution that is discrete and that provides information about the population distribution.  Measure theoretic formulation [ edit ]  A measurable function      X  :  A  →  B    {\displaystyle X\colon A\to B}   between a probability space      (  A  ,    A    ,  P  )    {\displaystyle (A,{\mathcal {A}},P)}   and a measurable space      (  B  ,    B    )    {\displaystyle (B,{\mathcal {B}})}   is called a discrete random variable provided its image is a countable set and the pre-image of singleton sets are measurable, i.e.,      X   −  1    (  b  )  ∈    A      {\displaystyle X^{-1}(b)\in {\mathcal {A}}}   for all     b  ∈  B    {\displaystyle b\in B}   . The latter requirement induces a probability mass function       f   X    :  X  (  A  )  →   R     {\displaystyle f_{X}\colon X(A)\to \mathbb {R} }   via      f   X    (  b  )  :=  P  (   X   −  1    (  b  )  )    {\displaystyle f_{X}(b):=P(X^{-1}(b))}   . Since the pre-images of disjoint sets are disjoint        ∑   b  ∈  X  (  A  )     f   X    (  b  )  =   ∑   b  ∈  X  (  A  )    P  (   X   −  1    (  b  )  )  =  P   (    ⋃   b  ∈  X  (  A  )     X   −  1    (  b  )   )   =  P  (  A  )  =  1.    {\displaystyle \sum _{b\in X(A)}f_{X}(b)=\sum _{b\in X(A)}P(X^{-1}(b))=P\left(\bigcup _{b\in X(A)}X^{-1}(b)\right)=P(A)=1.}     This recovers the definition given above.  Cumulative distribution function [ edit ]  Equivalently to the above, a discrete random variable can be defined as a random variable whose cumulative distribution function (cdf) increases only by jump discontinuities —that is, its cdf increases only where it "jumps" to a higher value, and is constant between those jumps. The points where jumps occur are precisely the values which the random variable may take.  Delta-function representation [ edit ]  Consequently, a discrete probability distribution is often represented as a generalized probability density function involving Dirac delta functions , which substantially unifies the treatment of continuous and discrete distributions. This is especially useful when dealing with probability distributions involving both a continuous and a discrete part.  Indicator-function representation [ edit ]  For a discrete random variable X , let u 0 , u 1 , ... be the values it can take with non-zero probability. Denote        Ω   i    =   X   −  1    (   u   i    )  =  {  ω  :  X  (  ω  )  =   u   i    }  ,   i  =  0  ,  1  ,  2  ,  …    {\displaystyle \Omega _{i}=X^{-1}(u_{i})=\{\omega :X(\omega )=u_{i}\},\,i=0,1,2,\dots }     These are disjoint sets , and for such sets       P   (    ⋃   i     Ω   i     )   =   ∑   i    P  (   Ω   i    )  =   ∑   i    P  (  X  =   u   i    )  =  1.    {\displaystyle P\left(\bigcup _{i}\Omega _{i}\right)=\sum _{i}P(\Omega _{i})=\sum _{i}P(X=u_{i})=1.}     It follows that the probability that X takes any value except for u 0 , u 1 , ... is zero, and thus one can write X as       X  (  ω  )  =   ∑   i     u   i     1    Ω   i      (  ω  )    {\displaystyle X(\omega )=\sum _{i}u_{i}1_{\Omega _{i}}(\omega )}     except on a set of probability zero, where      1   A      {\displaystyle 1_{A}}   is the indicator function of A . This may serve as an alternative definition of discrete random variables.  Continuous probability distribution [ edit ]  See also: Probability density function  A continuous probability distribution is a probability distribution that has a cumulative distribution function that is continuous. Most often they are generated by having a probability density function . Mathematicians call distributions with probability density functions absolutely continuous , since their cumulative distribution function is absolutely continuous with respect to the Lebesgue measure  λ . If the distribution of X is continuous, then X is called a continuous random variable . There are many examples of continuous probability distributions: normal , uniform , chi-squared , and others .  Intuitively, a continuous random variable is the one which can take a continuous range of values —as opposed to a discrete distribution, where the set of possible values for the random variable is at most countable . While for a discrete distribution an event with probability zero is impossible [ citation needed ] (e.g., rolling π on a standard die has probability zero and is impossible), this is not so in the case of a continuous random variable. For example, if one measures the width of an oak leaf, the result of 3½ cm is possible; however, it has probability zero because uncountably many other potential values exist even between 3 cm and 4 cm. Each of these individual outcomes has probability zero, yet the probability that the outcome will fall into the interval  (3 cm, 4 cm) is nonzero. This apparent paradox is resolved by the fact that the probability that X attains some value within an infinite set, such as an interval, cannot be found by naively adding the probabilities for individual values. Formally, each value has an infinitesimally small probability, which statistically is equivalent to zero. [ citation needed ]  Formally, if X is a continuous random variable, then it has a probability density function  ƒ ( x ), and therefore its probability of falling into a given interval, say [ a , b ] is given by the integral       P  ⁡  [  a  ≤  X  ≤  b  ]  =   ∫   a    b    f  (  x  )   d  x    {\displaystyle \operatorname {P} [a\leq X\leq b]=\int _{a}^{b}f(x)\,dx}     In particular, the probability for X to take any single value a (that is a ≤ X ≤ a ) is zero, because an integral with coinciding upper and lower limits is always equal to zero.  The definition states that a continuous probability distribution must possess a density, or equivalently, its cumulative distribution function be absolutely continuous. This requirement is stronger than simple continuity of the cumulative distribution function, and there is a special class of distributions, singular distributions , which are neither continuous nor discrete nor a mixture of those. An example is given by the Cantor distribution . Such singular distributions however are never encountered in practice.  Note on terminology: some authors use the term "continuous distribution" to denote the distribution with continuous cumulative distribution function. Thus, their definition includes both the (absolutely) continuous and singular distributions.  By one convention, a probability distribution      μ    {\displaystyle \,\mu }   is called continuous if its cumulative distribution function     F  (  x  )  =  μ  (  −  ∞  ,  x  ]    {\displaystyle F(x)=\mu (-\infty ,x]}   is continuous and, therefore, the probability measure of singletons     μ  {  x  }   =   0    {\displaystyle \mu \{x\}\,=\,0}   for all      x    {\displaystyle \,x}   .  Another convention reserves the term continuous probability distribution for absolutely continuous distributions. These distributions can be characterized by a probability density function : a non-negative Lebesgue integrable function      f    {\displaystyle \,f}   defined on the real numbers such that       F  (  x  )  =  μ  (  −  ∞  ,  x  ]  =   ∫   −  ∞    x    f  (  t  )   d  t  .    {\displaystyle F(x)=\mu (-\infty ,x]=\int _{-\infty }^{x}f(t)\,dt.}     Discrete distributions and some continuous distributions (like the Cantor distribution ) do not admit such a density.  Some properties [ edit ]   The probability distribution of the sum of two independent random variables is the convolution of each of their distributions.  Probability distributions are not a vector space —they are not closed under linear combinations , as these do not preserve non-negativity or total integral 1—but they are closed under convex combination , thus forming a convex subset of the space of functions (or measures).   Kolmogorov definition [ edit ]  Main articles: Probability space and Probability measure  In the measure-theoretic formalization of probability theory , a random variable is defined as a measurable function  X from a probability space       (  Ω  ,    F    ,  P  )     {\displaystyle \scriptstyle (\Omega ,{\mathcal {F}},\operatorname {P} )}   to measurable space      (    X    ,    A    )     {\displaystyle \scriptstyle ({\mathcal {X}},{\mathcal {A}})}   . A probability distribution of X is the pushforward measure  X * P  of X , which is a probability measure on      (    X    ,    A    )     {\displaystyle \scriptstyle ({\mathcal {X}},{\mathcal {A}})}   satisfying X * P = P X  −1 .  Random number generation [ edit ]  Main article: Pseudo-random number sampling  A frequent problem in statistical simulations (the Monte Carlo method ) is the generation of pseudo-random numbers that are distributed in a given way. Most algorithms are based on a pseudorandom number generator that produces numbers X that are uniformly distributed in the half-open interval [0,1). These random variates  X are then transformed via some algorithm to create a new random variate having the required probability distribution.  Applications [ edit ]  The concept of the probability distribution and the random variables which they describe underlies the mathematical discipline of probability theory, and the science of statistics. There is spread or variability in almost any value that can be measured in a population (e.g. height of people, durability of a metal, sales growth, traffic flow, etc.); almost all measurements are made with some intrinsic error; in physics many processes are described probabilistically, from the kinetic properties of gases to the quantum mechanical description of fundamental particles . For these and many other reasons, simple numbers are often inadequate for describing a quantity, while probability distributions are often more appropriate.  As a more specific example of an application, the cache language models and other statistical language models used in natural language processing to assign probabilities to the occurrence of particular words and word sequences do so by means of probability distributions.  Common probability distributions [ edit ]  Main article: List of probability distributions  The following is a list of some of the most common probability distributions, grouped by the type of process that they are related to. For a more complete list, see list of probability distributions , which groups by the nature of the outcome being considered (discrete, continuous, multivariate, etc.)  Note also that all of the univariate distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. In practice, actually observed quantities may cluster around multiple values. Such quantities can be modeled using a mixture distribution .  Related to real-valued quantities that grow linearly (e.g. errors, offsets) [ edit ]   Normal distribution (Gaussian distribution), for a single such quantity; the most common continuous distribution   Related to positive real-valued quantities that grow exponentially (e.g. prices, incomes, populations) [ edit ]   Log-normal distribution , for a single such quantity whose log is normally distributed  Pareto distribution , for a single such quantity whose log is exponentially distributed; the prototypical power law distribution   Related to real-valued quantities that are assumed to be uniformly distributed over a (possibly unknown) region [ edit ]   Discrete uniform distribution , for a finite set of values (e.g. the outcome of a fair die)  Continuous uniform distribution , for continuously distributed values   Related to Bernoulli trials (yes/no events, with a given probability) [ edit ]   Basic distributions:  Bernoulli distribution , for the outcome of a single Bernoulli trial (e.g. success/failure, yes/no)  Binomial distribution , for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed total number of independent occurrences  Negative binomial distribution , for binomial-type observations but where the quantity of interest is the number of failures before a given number of successes occurs  Geometric distribution , for binomial-type observations but where the quantity of interest is the number of failures before the first success; a special case of the negative binomial distribution    Related to sampling schemes over a finite population:  Hypergeometric distribution , for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, using sampling without replacement  Beta-binomial distribution , for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, sampling using a Polya urn scheme (in some sense, the "opposite" of sampling without replacement )     Related to categorical outcomes (events with K possible outcomes, with a given probability for each outcome) [ edit ]   Categorical distribution , for a single categorical outcome (e.g. yes/no/maybe in a survey); a generalization of the Bernoulli distribution  Multinomial distribution , for the number of each type of categorical outcome, given a fixed number of total outcomes; a generalization of the binomial distribution  Multivariate hypergeometric distribution , similar to the multinomial distribution , but using sampling without replacement ; a generalization of the hypergeometric distribution   Related to events in a Poisson process (events that occur independently with a given rate) [ edit ]   Poisson distribution , for the number of occurrences of a Poisson-type event in a given period of time  Exponential distribution , for the time before the next Poisson-type event occurs  Gamma distribution , for the time before the next k Poisson-type events occur   Related to the absolute values of vectors with normally distributed components [ edit ]   Rayleigh distribution , for the distribution of vector magnitudes with Gaussian distributed orthogonal components. Rayleigh distributions are found in RF signals with Gaussian real and imaginary components.  Rice distribution , a generalization of the Rayleigh distributions for where there is a stationary background signal component. Found in Rician fading of radio signals due to multipath propagation and in MR images with noise corruption on non-zero NMR signals.   Related to normally distributed quantities operated with sum of squares (for hypothesis testing) [ edit ]   Chi-squared distribution , the distribution of a sum of squared standard normal variables; useful e.g. for inference regarding the sample variance of normally distributed samples (see chi-squared test )  Student's t distribution , the distribution of the ratio of a standard normal variable and the square root of a scaled chi squared variable; useful for inference regarding the mean of normally distributed samples with unknown variance (see Student's t-test )  F-distribution , the distribution of the ratio of two scaled chi squared variables; useful e.g. for inferences that involve comparing variances or involving R-squared (the squared correlation coefficient )   Useful as conjugate prior distributions in Bayesian inference [ edit ]  Main article: Conjugate prior   Beta distribution , for a single probability (real number between 0 and 1); conjugate to the Bernoulli distribution and binomial distribution  Gamma distribution , for a non-negative scaling parameter; conjugate to the rate parameter of a Poisson distribution or exponential distribution , the precision (inverse variance ) of a normal distribution , etc.  Dirichlet distribution , for a vector of probabilities that must sum to 1; conjugate to the categorical distribution and multinomial distribution ; generalization of the beta distribution  Wishart distribution , for a symmetric non-negative definite matrix; conjugate to the inverse of the covariance matrix of a multivariate normal distribution ; generalization of the gamma distribution   See also [ edit ]    Statistics portal     Copula (statistics)  Empirical probability  Histogram  Joint probability distribution  Likelihood function  List of statistical topics  Kirkwood approximation  Moment-generating function  Quasiprobability distribution  Riemann–Stieltjes integral application to probability theory   References [ edit ]    B. S. Everitt: The Cambridge Dictionary of Statistics , Cambridge University Press , Cambridge (3rd edition, 2006). ISBN  0-521-69027-7  Bishop: Pattern Recognition and Machine Learning , Springer , ISBN  0-387-31073-8  den Dekker A. J., Sijbers J., (2014) "Data distributions in magnetic resonance images: a review", Physica Medica , [1]   External links [ edit ]     Wikimedia Commons has media related to Probability distribution .     Hazewinkel, Michiel , ed. (2001) [1994], "Probability distribution" , Encyclopedia of Mathematics , Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN  978-1-55608-010-4   Field Guide to Continuous Probability Distributions , Gavin E. Crooks.         v  t  e    Probability distributions      List     Discrete univariate with finite support     Benford  Bernoulli  beta-binomial  binomial  categorical  hypergeometric  Poisson binomial  Rademacher  discrete uniform  Zipf  Zipf–Mandelbrot       Discrete univariate with infinite support     beta negative binomial  Borel  Conway–Maxwell–Poisson  discrete phase-type  Delaporte  extended negative binomial  Gauss–Kuzmin  geometric  logarithmic  negative binomial  parabolic fractal  Poisson  Skellam  Yule–Simon  zeta       Continuous univariate supported on a bounded interval     arcsine  ARGUS  Balding–Nichols  Bates  beta  beta rectangular  Irwin–Hall  Kumaraswamy  logit-normal  noncentral beta  raised cosine  reciprocal  triangular  U-quadratic  uniform  Wigner semicircle       Continuous univariate supported on a semi-infinite interval     Benini  Benktander 1st kind  Benktander 2nd kind  beta prime  Burr  chi-squared  chi  Dagum  Davis  exponential-logarithmic  Erlang  exponential  F  folded normal  Flory–Schulz  Fréchet  gamma  gamma/Gompertz  generalized inverse Gaussian  Gompertz  half-logistic  half-normal  Hotelling's T -squared  hyper-Erlang  hyperexponential  hypoexponential  inverse chi-squared   scaled inverse chi-squared    inverse Gaussian  inverse gamma  Kolmogorov  Lévy  log-Cauchy  log-Laplace  log-logistic  log-normal  Lomax  matrix-exponential  Maxwell–Boltzmann  Maxwell–Jüttner  Mittag-Leffler  Nakagami  noncentral chi-squared  Pareto  phase-type  poly-Weibull  Rayleigh  relativistic Breit–Wigner  Rice  shifted Gompertz  truncated normal  type-2 Gumbel  Weibull   Discrete Weibull    Wilks's lambda       Continuous univariate supported on the whole real line     Cauchy  exponential power  Fisher's z  Gaussian q  generalized normal  generalized hyperbolic  geometric stable  Gumbel  Holtsmark  hyperbolic secant  Johnson's S U  Landau  Laplace  asymmetric Laplace  logistic  noncentral t  normal (Gaussian)  normal-inverse Gaussian  skew normal  slash  stable  Student's t  type-1 Gumbel  Tracy–Widom  variance-gamma  Voigt       Continuous univariate with support whose type varies     generalized extreme value  generalized Pareto  Marchenko–Pastur  q-exponential  q-Gaussian  q-Weibull  shifted log-logistic  Tukey lambda       Mixed continuous-discrete univariate     rectified Gaussian       Multivariate (joint)     Discrete  Ewens  multinomial  Dirichlet-multinomial  negative multinomial  Continuous  Dirichlet  generalized Dirichlet  multivariate Laplace  multivariate normal  multivariate stable  multivariate t  normal-inverse-gamma  normal-gamma  Matrix-valued  inverse matrix gamma  inverse-Wishart  matrix normal  matrix t  matrix gamma  normal-inverse-Wishart  normal-Wishart  Wishart       Directional     Univariate (circular) directional  Circular uniform  univariate von Mises  wrapped normal  wrapped Cauchy  wrapped exponential  wrapped asymmetric Laplace  wrapped Lévy  Bivariate (spherical)  Kent  Bivariate (toroidal)  bivariate von Mises  Multivariate  von Mises–Fisher  Bingham       Degenerate and singular     Degenerate  Dirac delta function  Singular  Cantor       Families     Circular  compound Poisson  elliptical  exponential  natural exponential  location–scale  maximum entropy  mixture  Pearson  Tweedie  wrapped              v  t  e    Theory of probability distributions        probability mass function (pmf)  probability density function (pdf)  cumulative distribution function (cdf)  quantile function             raw moment  central moment  mean  variance  standard deviation  skewness  kurtosis  L-moment          moment-generating function (mgf)  characteristic function  probability-generating function (pgf)  cumulant  combinant              v  t  e    Statistics        Outline  Index            Descriptive statistics         Continuous data      Center     Mean   arithmetic  geometric  harmonic    Median  Mode       Dispersion     Variance  Standard deviation  Coefficient of variation  Percentile  Range  Interquartile range       Shape     Central limit theorem  Moments   Skewness  Kurtosis  L-moments            Count data     Index of dispersion       Summary tables     Grouped data  Frequency distribution  Contingency table       Dependence     Pearson product-moment correlation  Rank correlation   Spearman's rho  Kendall's tau    Partial correlation  Scatter plot       Graphics     Bar chart  Biplot  Box plot  Control chart  Correlogram  Fan chart  Forest plot  Histogram  Pie chart  Q–Q plot  Run chart  Scatter plot  Stem-and-leaf display  Radar chart                  Data collection         Study design     Population  Statistic  Effect size  Statistical power  Sample size determination  Missing data       Survey methodology     Sampling   stratified  cluster    Standard error  Opinion poll  Questionnaire       Controlled experiments     Design   control  optimal    Controlled trial  Randomized  Random assignment  Replication  Blocking  Interaction  Factorial experiment       Uncontrolled studies     Observational study  Natural experiment  Quasi-experiment                  Statistical inference         Statistical theory     Population  Statistic  Probability distribution  Sampling distribution   Order statistic    Empirical distribution   Density estimation    Statistical model   L p space    Parameter   location  scale  shape    Parametric family   Likelihood  (monotone)  Location–scale family  Exponential family    Completeness  Sufficiency  Statistical functional   Bootstrap  U  V    Optimal decision   loss function    Efficiency  Statistical distance   divergence    Asymptotics  Robustness       Frequentist inference      Point estimation     Estimating equations   Maximum likelihood  Method of moments  M-estimator  Minimum distance    Unbiased estimators   Mean-unbiased minimum-variance   Rao–Blackwellization  Lehmann–Scheffé theorem    Median unbiased    Plug-in       Interval estimation     Confidence interval  Pivot  Likelihood interval  Prediction interval  Tolerance interval  Resampling   Bootstrap  Jackknife         Testing hypotheses     1- & 2-tails  Power   Uniformly most powerful test    Permutation test   Randomization test    Multiple comparisons       Parametric tests     Likelihood-ratio  Wald  Score          Specific tests         Z -test (normal)  Student's t -test  F -test       Goodness of fit     Chi-squared  G -test  Kolmogorov–Smirnov  Anderson–Darling  Lilliefors  Jarque–Bera  Normality (Shapiro–Wilk)  Likelihood-ratio test  Model selection   Cross validation  AIC  BIC         Rank statistics     Sign   Sample median    Signed rank (Wilcoxon)   Hodges–Lehmann estimator    Rank sum (Mann–Whitney)  Nonparametric  anova   1-way (Kruskal–Wallis)  2-way (Friedman)  Ordered alternative (Jonckheere–Terpstra)            Bayesian inference     Bayesian probability   prior  posterior    Credible interval  Bayes factor  Bayesian estimator   Maximum posterior estimator                       Correlation  Regression analysis            Correlation     Pearson product-moment  Partial correlation  Confounding variable  Coefficient of determination       Regression analysis     Errors and residuals  Regression model validation  Mixed effects models  Simultaneous equations models  Multivariate adaptive regression splines (MARS)       Linear regression     Simple linear regression  Ordinary least squares  General linear model  Bayesian regression       Non-standard predictors     Nonlinear regression  Nonparametric  Semiparametric  Isotonic  Robust  Heteroscedasticity  Homoscedasticity       Generalized linear model     Exponential families  Logistic (Bernoulli) / Binomial / Poisson regressions       Partition of variance     Analysis of variance (ANOVA, anova)  Analysis of covariance  Multivariate ANOVA  Degrees of freedom                  Categorical / Multivariate / Time-series / Survival analysis         Categorical     Cohen's kappa  Contingency table  Graphical model  Log-linear model  McNemar's test       Multivariate     Regression  Manova  Principal components  Canonical correlation  Discriminant analysis  Cluster analysis  Classification  Structural equation model   Factor analysis    Multivariate distributions   Elliptical distributions   Normal           Time-series      General     Decomposition  Trend  Stationarity  Seasonal adjustment  Exponential smoothing  Cointegration  Structural break  Granger causality       Specific tests     Dickey–Fuller  Johansen  Q-statistic (Ljung–Box)  Durbin–Watson  Breusch–Godfrey       Time domain     Autocorrelation (ACF)   partial (PACF)    Cross-correlation (XCF)  ARMA model  ARIMA model (Box–Jenkins)  Autoregressive conditional heteroskedasticity (ARCH)  Vector autoregression (VAR)       Frequency domain     Spectral density estimation  Fourier analysis  Wavelet  Whittle likelihood          Survival      Survival function     Kaplan–Meier estimator (product limit)  Proportional hazards models  Accelerated failure time (AFT) model  First hitting time       Hazard function     Nelson–Aalen estimator       Test     Log-rank test                     Applications         Biostatistics     Bioinformatics  Clinical trials / studies  Epidemiology  Medical statistics       Engineering statistics     Chemometrics  Methods engineering  Probabilistic design  Process / quality control  Reliability  System identification       Social statistics     Actuarial science  Census  Crime statistics  Demography  Econometrics  National accounts  Official statistics  Population statistics  Psychometrics       Spatial statistics     Cartography  Environmental statistics  Geographic information system  Geostatistics  Kriging                Category  Portal  Commons   WikiProject           Authority control     LCCN : sh85038545  NDL : 00564751             Retrieved from " https://en.wikipedia.org/w/index.php?title=Probability_distribution&oldid=833130857 "  Categories : Probability distributions Mathematical and quantitative methods (economics) Hidden categories: Articles lacking in-text citations from July 2011 All articles lacking in-text citations Articles needing additional references from July 2011 All articles needing additional references Articles with multiple maintenance issues All articles with unsourced statements Articles with unsourced statements from March 2018 Wikipedia articles with LCCN identifiers      Navigation menu    Personal tools   Not logged in Talk Contributions Create account Log in      Namespaces   Article Talk       Variants           Views   Read Edit View history      More         Search              Navigation    Main page Contents Featured content Current events Random article Donate to Wikipedia Wikipedia store      Interaction    Help About Wikipedia Community portal Recent changes Contact page      Tools    What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page      Print/export    Create a book Download as PDF Printable version      In other projects    Wikimedia Commons      Languages    العربية বাংলা Беларуская Català Čeština Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն Bahasa Indonesia Italiano עברית ქართული Latina Lietuvių Magyar Македонски Nederlands 日本語 Norsk Norsk nynorsk Polski Português Română Русский Scots සිංහල Simple English Slovenščina Basa Sunda Suomi Svenska Tagalog தமிழ் ไทย Türkçe Українська اردو Tiếng Việt 中文   Edit links        This page was last edited on 29 March 2018, at 20:34.  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Cookie statement  Mobile view                               1.  Exploratory Data Analysis   1.3.  EDA Techniques   1.3.6.  Probability Distributions        1.3.6.1.    What is a Probability Distribution            Discrete Distributions    The mathematical definition of a discrete probability function,
p(x), is a function that satisfies the following properties.  The probability that x can take a specific value is p(x).
       That is  \[ P[X = x] = p(x) = p_{x} \]  p(x) is non-negative for all real x.  The sum of p(x) over all possible values of x is 1, that is  \[ \sum_{j}p_{j} = 1 \] where j represents all possible values that x can have and p j is the
       probability at x j . One consequence of properties 2 and 3 is that 
       0 <= p(x) <= 1. What does this actually mean?  A discrete probability function is a
function that can take a discrete number of values (not necessarily
finite).  This is most often the non-negative integers or some subset
of the non-negative integers.  There is no mathematical restriction
that discrete probability functions only be defined at integers, but
in practice this is usually what makes sense.  For example, if
you toss a coin 6 times, you can get 2 heads or 3 heads but not
2 1/2 heads.  Each of the discrete values has a certain probability
of occurrence that is between zero and one.  That is, a discrete
function that allows negative values or values greater than one is
not a probability function.  The condition that the probabilities
sum to one means that at least one of the values has to occur.        Continuous Distributions    The mathematical definition of a continuous probability function, f(x),
is a function that satisfies the following properties.  The probability that x is between two points a and b is  \[ p[a \le x \le b] = \int_{a}^{b} {f(x)dx} \]  It is non-negative for all real x.  The integral of the probability function is one, that is  \[ \int_{-\infty}^{\infty} {f(x)dx} = 1 \]  What does this actually mean?  Since continuous probability
functions are defined for an infinite number of points over a
continuous interval, the probability at a single point is always
zero.  Probabilities are measured over intervals, not single points.
That is, the area under the curve between two distinct points
defines the probability for that interval.  This means that the
height of the probability function can in fact be greater than one.
The property that the integral must equal one is equivalent to
the property for discrete distributions that the sum of all the
probabilities must equal one.        Probability Mass Functions Versus Probability Density Functions    Discrete probability functions are referred to as probability mass
functions and continuous probability functions are referred to as
probability density functions.  The term probability functions
covers both discrete and continuous distributions. There are a few occasions in the e-Handbook when we use the
term probability density function in a generic sense where it may
apply to either probability density or probability mass functions.
It should be clear from the context whether we are referring only
to continuous distributions or to either continuous or discrete
distributions.                     Jump to navigation     ×  Home  mojo  GCSE  A-Level  Timetable  What's new  Log in / Register         User menu Contact  Log in                       S-cool, the revision website           ≡                 Probability Distribution Tables                 You are here A-level » Maths » Probability Distributions        Register Free  Start revising A-level & GCSE with 7 million other students  FREE Revision guides, questions banks and resources  60% of members achieve a A*-B Grade   Enrol Now »         Expectation and Variance      *Please note: you may not see animations, interactions or images that are potentially on this page because you have not allowed Flash to run on S-cool. To do this, click here. *     Probability Distribution Tables       What is a discrete random variable?  A random variable is a variable which takes numerical values and whose value depends on the outcome of an experiment. It is discrete if it can only take certain values.  Example:  Suppose you roll a die and let X be the number on the uppermost face. Then the values that can be obtained are discrete random variables.  x = 1, 2, 3, 4, 5 or 6  The probability of obtaining a 1 can be written as P(X = 1) = 1/6  The probability of obtaining a 2 can be written as P(X = 2) =  1/6  And so on for all the values of x, P(X = x).  If we list all these probabilities into a table, we get the probability distribution of X .      x  1  2  3  4  5  6    P(X = x)  1/6  1/6  1/6  1/6  1/6  1/6      Note: Capital letters are used to denote the random variables, whereas lower case letters are used to denote the values that can be obtained.  In the above example we covered every possible event when rolling a die. We must get one of the six events listed in the table. These events are exclusive events and always sum to 1.  ∑ P(X = x) = 1  Example:  Suppose you flip 2 coins and let X be the discrete random variable of the number of heads obtained. Write down the probability distribution of X.  First we need to know what values of x can be obtained.  Clearly, x = 0, 1 or 2, as we can either get no heads, 1 head or 2 heads.   Our next step is to calculate the probability of each (if you have trouble with this then go and have a look at the Probability topic).  P(X = 0) = P(tail and tail) = ½ × ½ = ¼  P(X = 1) = P(tail and head) or P(head and tail) = ½ × ½ +  ½ × ½ = ½  P(X = 2) = P(head and head) = ½ × ½ = ¼  We now put these results into a table for the probability distribution of X...      x  0  1  2    P(X = x)  ¼  ½  ¼      Note: The better you are at calculating probabilities, the quicker and easier these problems become. You should be able to write down the probability distribution of a discrete random variable with minimal workings.   Probability density function   Sometimes we are given a formula to calculate probabilities. We call this the probability density function of X or the p.d.f. of X .  Example:  The probability density function of a discrete random variable X is given by...  P(X = x) = kx 3  For x = 0, 1 or 2 where k is a constant.  Find the probability distribution of X.  P(X = 0) = k × 0 3 = 0  P(X = 1) = k × 1 3 = k  P(X = 2) = k × 2 3 = 8k  So:      x  0  1  2    P(X =x)  0  k  8k      From earlier, we know:  ∑ P(X = x) = 1  ( Remember: ∑ means 'sum of')  Therefore, 0 + k + 8k = 1  9k = 1  Hence k =  1/9  So the probability distribution of X is:      x  0  1  2    P(X = x)  0  1/9  8/9       Cumulative distribution function   'Cumulative' gives us a kind of running total so a cumulative distribution function gives us a running total of probabilities within our probability table.  The cumulative distribution function, F(x) of X is defined as:  F(x) = P(X ≤ x)  Example:  The probability distribution of a discrete random variable, X, is given by:      x  1  2  3  4  5    F(x)  0.2  0.43  0.62  0.8  1      Reading the table we see that:  P(X ≤ 3) = 0.62  P(X ≤ 2) = 0.43  This means to calculate a single probability we proceed as follows:      P(X = 3)  = P(X ≤ 3) − P(X ≤ 2)    = 0.62 − 0.43    = 0.19      For probabilities of 'greater than' :      P(X >2)  = 1 - F(2)    = 1 − P(X ≤ 2)    = 1 − 0.43    = 0.57      Question: Try for yourself  For the above example, see if you can work out the following:           Just click "Find out more" and get £10 off your first tutorial             Expectation and Variance                 Show me                                 Main menu Home  mojo  GCSE  A-Level  Timetable  What's new  Log in / Register       Search form   Search                     Advertise  Contact us  Log in  Terms & Conditions                  Copyright © 2018 S-cool Youth Marketing Limited                       The Binomial Distribution In many cases, it is appropriate to summarize a group of independent observations by the 
number of observations in the group that represent one of two outcomes.  For example, the 
proportion of individuals in a random sample who support one of two political candidates fits
this description.  In this case, the statistic  is the count  X of voters who support
the candidate divided by the total number of individuals in the group n .  This provides an 
estimate of the parameter  p , the proportion of 
individuals who support the candidate in the entire population. The binomial distribution X 1: The number of observations n is fixed. 2: Each observation is independent. 3: Each observation represents one of two outcomes ("success" or "failure"). 4: The probability of "success" p is the same for each outcome. X n p B(n,p)  Example  Suppose individuals with a certain gene have a 0.70 probability of eventually contracting 
a certain disease.  If 100 individuals with the gene participate in a lifetime study, then the 
distribution of the random variable describing the number of individuals who will contract the
disease is distributed B(100,0.7) .  Note: The sampling distribution of a count variable is only well-described by the binomial
distribution is cases where the population size is significantly larger than the sample size.  
As a general rule, the binomial distribution should not be applied to observations from 
a simple random sample (SRS) unless the 
population size is at least 10 times larger than the sample size. To find probabilities from a binomial distribution, one may either calculate them directly,
use a binomial table, or use a computer.  The number of sixes rolled by a single die in 20
rolls has a B(20,1/6) distribution.  The probability of rolling more than 2 sixes
in 20 rolls, P(X>2) , is equal to 1 - P(X < 2) = 1 - (P(X=0) + P(X=1) + 
P(X=2)) . Using the MINITAB command "cdf" with subcommand "binomial n=20 p=0.166667" gives the cumulative
distribution function as follows: Binomial with n = 20 and p = 0.166667

         x     P( X <= x)
         0        0.0261
         1        0.1304
         2        0.3287
         3        0.5665
         4        0.7687
         5        0.8982
         6        0.9629
         7        0.9887
         8        0.9972
         9        0.9994 The corresponding graphs for the probability density function and cumulative distribution function
for the B(20,1/6) distribution are shown below:  Since the probability of 2 or fewer sixes is equal to 0.3287, the probability of rolling more than
2 sixes = 1 - 0.3287 = 0.6713.  The probability that a random variable X with binomial distribution B(n,p) is
equal to the value k , where k = 0, 1,....,n , is given by  , where . The latter expression is known as the binomial coefficient n choose k k n one Mean and Variance of the Binomial Distribution X n p n Z Z p mean 1*p + 0*(1-p) = p variance p(1-p). n Z These definitions are intuitively logical.  Imagine, for example, 8 flips
of a coin.  If the coin is fair, then p = 0.5.  One would expect the 
mean number of heads to be half the flips, or np = 8*0.5 = 4.  The
variance is equal to np(1-p) = 8*0.5*0.5 = 2. Sample Proportions If we know that the count X of "successes" in a group of n observations with
sucess probability p has a binomial distribution with mean np and variance np(1-p) , then we are able to derive information about the distribution of the sample proportion X n X/n X n np/n = p unbiased estimator p X/n X n² (np(1-p))/n² = (p(1-p))/n In the example of rolling a six-sided die 20 times, the probability p of rolling
a six on any roll is 1/6, and the count X of sixes has a B(20, 1/6) distribution. 
The mean of this distribution is 20/6 = 3.33, and the variance is 20*1/6*5/6 = 100/36 = 2.78. 
The mean of the proportion of sixes in the 20 rolls, X/20 , is equal to p = 1/6 = 0.167, and the variance of the proportion is equal to (1/6*5/6)/20 = 0.007.  Normal Approximations for Counts and Proportions  For large values of n , the distributions of the count X and the sample proportion are approximately normal .
This result follows from the Central Limit Theorem .
The mean and variance for the approximately normal distribution of X are np and np(1-p) , identical to the mean and variance of the binomial( n,p ) distribution.
Similarly, the mean and variance for the approximately normal distribution of the sample
proportion are p and (p(1-p)/n) .   Note: Because the normal approximation is not accurate for small values of n , a good rule of 
thumb is to use the normal approximation only if np > 10 and np(1-p) > 10.  For example, consider a population of voters in a given state.  The true proportion of voters who 
favor candidate A is equal to 0.40.  Given a sample of 200 voters, what is the probability that
more than half of the voters support candidate A? The count X of voters in the sample of 200 who support candidate A is distributed B(200,0.4) .  The mean of the distribution is equal to 200*0.4 = 80, and the variance is equal
to 200*0.4*0.6 = 48.  The standard deviation is the square root of the variance, 6.93.  The 
probability that more than half of the voters in the sample support candidate A is equal to
the probability that X is greater than 100, which is equal to 1- P(X < 100). To use the normal approximation to calculate this probability, we should first acknowledge that
the normal distribution is continuous and apply the continuity correction .
This means that the probability for a single discrete value, such as 100, is extended to the 
probability of the interval (99.5,100.5).  Because we are interested in the probability
that X is less than or equal to 100, the normal approximation applies to the upper limit
of the interval, 100.5.  If we were interested in the probability that X is strictly less
than 100, then we would apply the normal approximation to the lower end of the interval, 99.5. So, applying the continuity correction and standardizing the variable X gives the following: 1 - P(X < 100) = 1 - P(X < 100.5) = 1 - P(Z < (100.5 - 80)/6.93) = 1 - P(Z < 20.5/6.93) = 1 - P(Z < 2.96) = 1 - (0.9985) = 0.0015.  Since the value 100 is nearly three
standard deviations away from the mean 80, the probability of observing a count this high is 
extremely small.  RETURN TO MAIN PAGE .      Skip navigation       Sign in Search         Loading...            Close           Yeah, keep it  Undo  Close       This video is unavailable.         Watch Queue Queue Watch Queue Queue   Remove all Disconnect         The next video is starting stop    Loading...          Watch Queue  Queue  __count__/__total__                       Tired of ads?       Loading...      Want music and videos with zero ads? Get YouTube Red.         Working...            Not now  Try it free                                                     Find out why Close       The Probability Distribution Function (PDF) of [X]                 MIT OpenCourseWare              Loading...        Unsubscribe from MIT OpenCourseWare?    Cancel  Unsubscribe           Working...            Subscribe Subscribed Unsubscribe 1.5M             Loading...          Loading...              Working...                Add to   Want to watch this again later?  Sign in to add this video to a playlist.  Sign in    Share   More      Report    Need to report the video?  Sign in to report inappropriate content.  Sign in       Transcript      Statistics     Add translations   88,603 views         248   Like this video?  Sign in to make your opinion count.  Sign in     249    11   Don't like this video?  Sign in to make your opinion count.  Sign in     12            Loading...            Loading...      Transcript      The interactive transcript could not be loaded.         Loading...         Loading...       Rating is available when the video has been rented.     This feature is not available right now. Please try again later.      Published on Feb 26, 2014 MIT 6.041SC Probabilistic Systems Analysis and Applied Probability, Fall 2013 View the complete course: http://ocw.mit.edu/6-041SCF13 Instructor: Jimmy Li License: Creative Commons BY-NC-SA More information at http://ocw.mit.edu/terms More courses at http://ocw.mit.edu     Category   Education     License   Standard YouTube License       Show more  Show less       Loading...                 Autoplay    When autoplay is enabled, a suggested video will automatically play next.       Up next       Calculating a Cumulative Distribution Function (CDF)  - Duration: 8:44.  MIT OpenCourseWare  195,515 views     8:44               PDF and CDF Explanations  - Duration: 3:15.  UAMath115  136,421 views     3:15      Probability density functions | Probability and Statistics | Khan Academy  - Duration: 10:02.  Khan Academy  1,410,043 views     10:02      Probability density functions (KristaKingMath)  - Duration: 7:01.  Krista King  84,066 views     7:01      FRM: Terms about distributions: PDF, PMF and CDF  - Duration: 9:58.  Bionic Turtle  96,082 views     9:58      PMF of a Function of a Random Variable  - Duration: 15:26.  MIT OpenCourseWare  65,828 views     15:26      Find the Probability Density Function for Continuous Distribution of Random Variable  - Duration: 9:53.  Anil Kumar  18,342 views     9:53      Continuous Random Variables: Probability Density Functions  - Duration: 23:16.  MrNichollTV  111,584 views     23:16      Probability Mass Function and Probability Density Function  - Duration: 13:15.  ProfessorSerna  9,667 views     13:15      Why I Left My $100,000+ Job at Google  - Duration: 3:00.  CS Dojo  2,657,335 views     3:00      Probability functions: pdf, CDF and inverse CDF (FRM T2-1)  - Duration: 20:34.  Bionic Turtle  3,294 views     20:34      A Derived Distribution Example  - Duration: 9:30.  MIT OpenCourseWare  7,091 views     9:30      Probability Distrubtion Functions and Expected Value  - Duration: 13:46.  V Anusic  4,114 views     13:46      probability density functions and cumulative distribution functions s1  - Duration: 6:25.  Rhys Steele  95,847 views     6:25      Probability Density Functions / Continuous Random Variables  - Duration: 8:32.  patrickJMT  384,462 views     8:32      Continuous Random Variables: Cumulative Distribution Functions  - Duration: 25:47.  MrNichollTV  84,506 views     25:47      Probability Density Functions (Introduction) : ExamSolutions  - Duration: 12:12.  ExamSolutions  118,681 views     12:12      Normal distribution's probability density function derived in 5min  - Duration: 4:50.  acadelivery  41,949 views     4:50      Probability Density Functions (Example 1) : ExamSolutions  - Duration: 7:36.  ExamSolutions  112,055 views     7:36      A Beginner’s Guide To Quantum Computing  - Duration: 17:58.  Coding Tech  325,660 views     17:58     Loading more suggestions...   Show more                Language: English     Location: United States     Restricted Mode: Off    History  Help     Loading...       Loading...       Loading...     About  Press  Copyright  Creators  Advertise  Developers  +YouTube   Terms  Privacy  Policy & Safety  Send feedback   Test new features              Loading...               Working...             Sign in to add this to Watch Later    Add to      Loading playlists...                Skip navigation       Sign in Search         Loading...            Close           Yeah, keep it  Undo  Close       This video is unavailable.         Watch Queue Queue Watch Queue Queue   Remove all Disconnect         The next video is starting stop    Loading...          Watch Queue  Queue  __count__/__total__                       Tired of ads?       Loading...      Want music and videos with zero ads? Get YouTube Red.         Working...            Not now  Try it free                                                     Find out why Close       The Probability Distribution Function (PDF) of [X]                 MIT OpenCourseWare              Loading...        Unsubscribe from MIT OpenCourseWare?    Cancel  Unsubscribe           Working...            Subscribe Subscribed Unsubscribe 1.5M             Loading...          Loading...              Working...                Add to   Want to watch this again later?  Sign in to add this video to a playlist.  Sign in    Share   More      Report    Need to report the video?  Sign in to report inappropriate content.  Sign in       Transcript      Statistics     Add translations   88,603 views         248   Like this video?  Sign in to make your opinion count.  Sign in     249    11   Don't like this video?  Sign in to make your opinion count.  Sign in     12            Loading...            Loading...      Transcript      The interactive transcript could not be loaded.         Loading...         Loading...       Rating is available when the video has been rented.     This feature is not available right now. Please try again later.      Published on Feb 26, 2014 MIT 6.041SC Probabilistic Systems Analysis and Applied Probability, Fall 2013 View the complete course: http://ocw.mit.edu/6-041SCF13 Instructor: Jimmy Li License: Creative Commons BY-NC-SA More information at http://ocw.mit.edu/terms More courses at http://ocw.mit.edu     Category   Education     License   Standard YouTube License       Show more  Show less       Loading...                 Autoplay    When autoplay is enabled, a suggested video will automatically play next.       Up next       Calculating a Cumulative Distribution Function (CDF)  - Duration: 8:44.  MIT OpenCourseWare  195,515 views     8:44               PDF and CDF Explanations  - Duration: 3:15.  UAMath115  136,421 views     3:15      Probability density functions | Probability and Statistics | Khan Academy  - Duration: 10:02.  Khan Academy  1,410,043 views     10:02      Probability density functions (KristaKingMath)  - Duration: 7:01.  Krista King  84,066 views     7:01      FRM: Terms about distributions: PDF, PMF and CDF  - Duration: 9:58.  Bionic Turtle  96,082 views     9:58      PMF of a Function of a Random Variable  - Duration: 15:26.  MIT OpenCourseWare  65,828 views     15:26      Find the Probability Density Function for Continuous Distribution of Random Variable  - Duration: 9:53.  Anil Kumar  18,342 views     9:53      Continuous Random Variables: Probability Density Functions  - Duration: 23:16.  MrNichollTV  111,584 views     23:16      Probability Mass Function and Probability Density Function  - Duration: 13:15.  ProfessorSerna  9,667 views     13:15      Why I Left My $100,000+ Job at Google  - Duration: 3:00.  CS Dojo  2,657,335 views     3:00      Probability functions: pdf, CDF and inverse CDF (FRM T2-1)  - Duration: 20:34.  Bionic Turtle  3,294 views     20:34      A Derived Distribution Example  - Duration: 9:30.  MIT OpenCourseWare  7,091 views     9:30      Probability Distrubtion Functions and Expected Value  - Duration: 13:46.  V Anusic  4,114 views     13:46      probability density functions and cumulative distribution functions s1  - Duration: 6:25.  Rhys Steele  95,847 views     6:25      Probability Density Functions / Continuous Random Variables  - Duration: 8:32.  patrickJMT  384,462 views     8:32      Continuous Random Variables: Cumulative Distribution Functions  - Duration: 25:47.  MrNichollTV  84,506 views     25:47      Probability Density Functions (Introduction) : ExamSolutions  - Duration: 12:12.  ExamSolutions  118,681 views     12:12      Normal distribution's probability density function derived in 5min  - Duration: 4:50.  acadelivery  41,949 views     4:50      Probability Density Functions (Example 1) : ExamSolutions  - Duration: 7:36.  ExamSolutions  112,055 views     7:36      A Beginner’s Guide To Quantum Computing  - Duration: 17:58.  Coding Tech  325,660 views     17:58     Loading more suggestions...   Show more                Language: English     Location: United States     Restricted Mode: Off    History  Help     Loading...       Loading...       Loading...     About  Press  Copyright  Creators  Advertise  Developers  +YouTube   Terms  Privacy  Policy & Safety  Send feedback   Test new features              Loading...               Working...             Sign in to add this to Watch Later    Add to      Loading playlists...                       Stat Trek  Teach yourself statistics                Home    Tutorials    AP statistics    Stat tables    Stat tools    Calculators    Books    Help        Overview  AP statistics  Beyond AP statistics  Simple linear regression  Probability sampling  Matrix algebra    AP tutorial  Test preparation  Practice test  AP formulas  FAQ  AP study guides  AP calculators    Binomial  Chi-square  f Dist  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Dist    Random numbers  Probability  Bayes rule  Combinations/permutations  Factorial  Event counter  Sample planning    Graphing  Scientific  Financial    Statistics  AP study guides  Probability  Survey sampling    Statistics dictionary  Problems and solutions  Formulas  Notation               Home    Tutorials   Overview  AP statistics  Beyond AP statistics  Simple Linear regression  Probability sampling  Matrix algebra     AP statistics   Test preparation  AP tutorial  Practice test  AP formulas  FAQ  AP study guides  AP calculators     Statistical tables   Binomial  Chi-square  f Distribution  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Distribution     Statistical tools   Random number generator  Probability calculator  Bayes rule calculator  Combinations/permutations  Factorial calculator  Event counter  Sample planning wizard     Handheld calculators   Graphing  Scientific  Financial     Books   Statistics  AP study guides  Probability  Survey sampling     Help   Statistics dictionary  Problems and solutions  Formulas  Notation                       Beyond AP Statistics   Probability Basics   Sets and subsets  Statistical experiment  Counting data points  Probability problems  Bayes theorem   Small Samples   Estimate proportion  Test proportion   Distributions   Probability distribution  Discrete/continuous  Hypergeometric  Multinomial  Poisson  f Distribution   Power   Region of acceptance  Power of a test   How to find power            Beyond AP Statistics   Probability Basics   Sets and Subsets  Statistical Experiments  Counting Data Points  Probability Problems  Bayes Theorem   Small Samples   Estimate Proportion  Test Proportion   Distributions   Probability Distribution  Discrete/Continuous  Hypergeometric  Multinomial  Poisson  f Distribution   Power   Region of Acceptance  Power of a Test   How to Find Power        What is a Probability Distribution?  A probability distribution is a table or an equation that links each outcome 
    of a statistical experiment with its probability of occurrence.  Probability Distribution Prerequisites  To understand probability distributions, it is important to understand variables. 
	    random variables, and some notation.   A variable is a symbol ( A , B , x , y , 
	    etc.) that can take on any of a specified set of values.  When the value of a variable is the outcome of a statistical experiment , that variable is a random variable .   Generally, statisticians use a capital letter to represent a random variable and 
	    a lower-case letter, to represent one of its values. For example,   X represents the random variable X.  P(X) represents the probability of X.  P(X = x) refers to the probability that the random variable X is equal to a 
		    particular value, denoted by x. As an example, P(X = 1) refers to the 
		    probability that the random variable X is equal to 1.   Probability Distributions  An example will make clear the relationship between random variables and 
        probability distributions. Suppose you flip a coin two times. This simple 
	    statistical experiment can have four possible outcomes: HH, HT, TH, and TT. 
	    Now, let the variable X represent the number of Heads that result from this 
	    experiment. The variable X can take on the values 0, 1, or 2. In this example, 
	    X is a random variable; because its value is determined by the outcome of a 
	    statistical experiment.  A probability distribution is a table or an equation that links 
	    each outcome of a statistical experiment with its probability of occurrence. 
	    Consider the coin flip experiment described above. The table below, which 
	    associates each outcome with its probability, is an example of a probability 
	    distribution.    Number of heads  Probability    0  0.25    1  0.50    2  0.25    The above table represents the probability distribution of the random variable 
	    X.        Cumulative Probability Distributions  A cumulative probability refers to the probability that the 
	    value of a random variable falls within a specified range.  Let us return to the coin flip experiment. If we flip a coin two times, we might 
	    ask: What is the probability that the coin flips would result in one or fewer 
	    heads? The answer would be a cumulative probability. It would be the 
	    probability that the coin flip experiment results in zero heads plus the 
	    probability that the experiment results in one head.  P(X < 1) = P(X = 0) + P(X = 1) = 0.25 + 0.50 = 0.75  Like a probability distribution, a cumulative probability distribution can be 
	    represented by a table or an equation. In the table below, the cumulative 
	    probability refers to the probability than the random variable X is less than 
	    or equal to x.    Number of heads: x  Probability: P(X = x) Cumulative Probability: P(X < x)  0  0.25  0.25   1  0.50  0.75   2  0.25  1.00      Uniform Probability Distribution The simplest probability distribution occurs when all of the values of a 
	    random variable occur with equal probability. This probability 
	    distribution is called the uniform distribution .  Uniform Distribution. Suppose the 
		    random variable X can assume k different values. Suppose also that the P(X = x k ) 
		    is constant. Then,  P(X = x k ) = 1/k  Example 1 Suppose a die is tossed. What is the probability that the die will land on 5? Solution: When a die is tossed, there are 6 possible outcomes represented 
	    by: S = { 1, 2, 3, 4, 5, 6 }. Each possible outcome is a random variable (X), 
	    and each outcome is equally likely to occur. Thus, we have a uniform 
	    distribution. Therefore, the P(X = 5) = 1/6. Example 2 Suppose we repeat the dice tossing experiment described in Example 1. This 
	    time, we ask what is the probability that the die will land on a number that is 
	    smaller than 5? Solution: When a die is tossed, there are 6 possible outcomes represented 
	    by: S = { 1, 2, 3, 4, 5, 6 }. Each possible outcome is equally likely to occur. 
	    Thus, we have a uniform distribution. This problem involves a cumulative probability. The probability that the die 
	    will land on a number smaller than 5 is equal to: P( X < 5 ) = P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4) P( X < 5 ) = 1/6 + 1/6 + 1/6 + 1/6 = 2/3           Bestsellers Handheld Calculators Updated daily 1. Texas Instruments TI-84 Plus CE Lightning Graphing Calculator $150.00  $150.00 2. Texas Instruments Ti-84 plus Graphing calculator - Black $108.99  $108.99 3. Texas Instruments VOY200/PWB Graphing Calculator $200.00  4. Sharp EL-W535B WriteView Scientific Calculator $24.99  $39.99 Today's Bargain Book Understandable Statistics $319.95  $65.00  80% off See more Statistics books ... Bestsellers Statistics and Probability Updated daily 1. Barron's AP Statistics, 9th Edition $18.99  $12.91 2. Naked Statistics: Stripping the Dread from the Data $16.95  $13.70 3. Statistics For Dummies (For Dummies (Math & Science)) $19.99  $13.11 4. How to Lie with Statistics $13.95  $8.98           About  Contact  Privacy  Terms  Advertising    Advertise on Stat Trek  Copyright © 2018 StatTrek.com             Search       Statistics How To   Statistics for the rest of us!       Home  Tables   Binomial Distribution Table  F Table  PPMC Critical Values  T-Distribution Table (One Tail and Two-Tails)  Chi Squared Table (Right Tail)  Z-table (Right of Curve or Left)    Probability and Statistics   Binomials  Chi-Square Statistic  Expected Value  Hypothesis Testing  Non Normal Distribution  Normal Distributions  Probability  Regression Analysis  Statistics Basics  T-Distribution  Multivariate Analysis  Sampling    Calculators   Variance and Standard Deviation Calculator  Tdist Calculator  Permutation Calculator / Combination Calculator  Interquartile Range Calculator  Linear Regression Calculator  Expected Value Calculator  Binomial Distribution Calculator    Statistics Blog  Calculus   Derivatives  Integrals  Limits    Matrices  Experimental Design  Practically Cheating Statistics Handbook  Navigation             Find the Mean of the Probability Distribution / Binomial    Binomial Theorem > How to find the mean of the probability distribution  Contents:   Mean of a probability distribution  Mean of a binomial (by hand or TI83)   Mean of a probability distribution How to find the mean of the probability distribution : Overview A normal distribution curve is one kind of probability distribution. Finding the mean of a probability distribution is easy in probability and statistics — if you know how. This how to will guide you through a few simple steps necessary to find the mean of the probability distribution or binomial distribution . You’ll often find these types of questions in textbook chapters on binomial probability distribution. The binomial distribution is just a simple trial where there are two outcomes: success or failure. For example, if you are counting how many times you draw an Ace from a deck of cards, you could assign “Success” to “Drawing an Ace” and “Failure” to drawing any other card. You can find the mean of the probability distribution by creating a probability table. How to find the mean of the probability distribution: Steps Sample question : “A grocery store has determined that in crates of tomatoes,  95% carry no rotten tomatoes, 2% carry one rotten tomato, 2% carry two rotten tomatoes, and 1% carry three rotten tomatoes. Find the mean number of rotten tomatoes in the crates.”  Step 1:  Convert all the percentages to decimal probabilities . For example: 95% = .95 2% = .02 2% = .02 1% = .01  Step 2:  Construct a probability distribution table . (If you don’t know how to do this, see how to construct a probability distribution ) .)   Step 3:  Multiply the values in each column. (In other words, multiply each value of X by each probability P(X).) Referring to our probability distribution table: 0 × .95 = 0 1 × .02  = .02 2 × .02  = .04 3 × .01 = .03  Step 4:  Add the results from step 3 together . 0 + .02 + .04 + .03 = .09 is the mean.  You’re done finding the mean for a probability distribution! Mean of Binomial Distribution A coin toss is a simple binomial experiment . A binomial distribution represents the results from a simple experiment where there is “success” or “failure.” For example, if you are polling voters to see who is voting Democrat, the voters that say they will vote Democrat is a “success” and anything else is a failure. One of the simplest binomial experiments you can perform is a coin toss, where “heads” could equal “success” and “tails” could equal “failure.” The mean of binomial distribution is much like the mean of anything else. It answers the question “If you perform this experiment many times, what’s the likely (the average) result?. Formula for Mean of Binomial Distribution The formula for the mean of binomial distribution is:  μ = n *p Where “n” is the number of trials and “p” is the probability of success. For example: if you tossed a coin 10 times to see how many heads come up, your probability is .5 (i.e. you have a 50 percent chance of getting a heads and 50 percent chance of a tails) and “n” is how many trials — 10. Therefore, the mean of this particular binomial distribution is: 10 * .5 = 5. This makes sense: if you toss a coin ten times you would expect heads to show up on average, 5 times. Mean for a Binomial Distribution on the TI-83 Sample problem : Find the mean for a binomial distribution with n = 5 and p = 0.12. Again, the TI 83 doesn’t have a function for this. But if you know the formula (n*p), it’s pretty easy to enter it on the home screen.  Step 1: Multiply n by p. 5 * .12 ENTER  =.6 Hey, that was easy! Something to think about: You may be wondering why it was so easy to calculate the mean. After all being asked to “calculate the mean for a binomial distribution” sounds scary. If you think about what a mean (or average ) is, then you’ll see why it was so easy. In the sample question, n = 5 and p = 0.12. What is “n”? That’s the number of items. So imagine a list of 5 items with a certain score: 1 = 0.12 2 = 0.12 3 = 0.12 4 = 0.12 5 = 0.12 If you were asked to find the average score for those five items, you wouldn’t even have to do the math: it’s just 0.12, right? Finding the mean for a binomial distribution is just a little different: you add up all of the probabilities (0.12 + 0.12 + 0.12 + 0.12 + 0.12). Or a faster way, just multiply n by p. Check out our Youtube channel for more help and stats tips!     If you prefer an online interactive environment to learn R and statistics, this free R Tutorial by Datacamp is a great way to get started. If you're are somewhat comfortable with R and are interested in going deeper into Statistics, try this Statistics with R track . Facebook page Find the Mean of the Probability Distribution / Binomial was last modified: October 15th, 2017 by Stephanie   By Stephanie | August 24, 2009 | Statistics How To |   ← Construct a probability distribution in Easy Steps  Binomial experiment: How to figure out what is and what isn’t →   4 thoughts on “ Find the Mean of the Probability Distribution / Binomial ”      Selin Asar May 30, 2016 at 9:20 am    A building venice is designed by taking 20  years return period water level. a) Calculate the probability of water level exceeding the design value less than 2 times in 20 years with Binomial distrubition. b)Calculate the probability of water level exceeding the design value more than 2 times in 60 years with Poisson distrubition.          Andale May 30, 2016 at 10:39 am    Were you given any probabilities for the water level exceeding the design value? Without this info, the question is impossible to answer.          M N Choudhary May 5, 2017 at 3:48 am    Assume a banks 95% value at risk model is perfectly accurate,If daily losses are independent, what is the probability that the number of daily losses exceeds the Var on exactly 5 days out of the previous 100 trading days.          Madison September 27, 2017 at 7:17 pm    This helps so much! Thanks!         Find an article   Search       Feel like "cheating" at Statistics? Check out the grade-increasing book that's recommended reading at top universities!                 Privacy policy.     Copyright © 2018 Statistics How To Theme by: Theme Horse Powered by: WordPress    Back to Top             Search       Statistics How To   Statistics for the rest of us!       Home  Tables   Binomial Distribution Table  F Table  PPMC Critical Values  T-Distribution Table (One Tail and Two-Tails)  Chi Squared Table (Right Tail)  Z-table (Right of Curve or Left)    Probability and Statistics   Binomials  Chi-Square Statistic  Expected Value  Hypothesis Testing  Non Normal Distribution  Normal Distributions  Probability  Regression Analysis  Statistics Basics  T-Distribution  Multivariate Analysis  Sampling    Calculators   Variance and Standard Deviation Calculator  Tdist Calculator  Permutation Calculator / Combination Calculator  Interquartile Range Calculator  Linear Regression Calculator  Expected Value Calculator  Binomial Distribution Calculator    Statistics Blog  Calculus   Derivatives  Integrals  Limits    Matrices  Experimental Design  Practically Cheating Statistics Handbook  Navigation             Construct a probability distribution in Easy Steps    Probability and Statistics > Probability > Construct a probability distribution   Watch the video or read the steps below:   Construct a probability distribution: Overview  A normal distribution curve is one kind of probability distribution. A probability distribution is basically a chart of what the probability of an event happening is. It’s a way of quickly viewing the event, and the probability of that event. Probability distribution charts can get quite complex in statistics. For example, a normal distribution or t-distribution chart usually requires some form of technology (like Microsoft Excel or the TI-83 calculator ) to create. However, you can construct a basic probability distribution showing events and probabilities in a few easy steps.  Construct a probability distribution: Steps  Sample question : Construct a probability distribution for the following scenario: the probability of a sausage making machine producing 0, 1, 2, 3, 4, or 5 misshapen sausages per day are 0.09, 0.07, 0.1, 0.04,0.12, and 0.02.   Step 1:  Write down the number of widgets (things, items, products or other named thing) given on one horizontal line . In this case, the widgets in this question are the “misshapen sausages”.  Making the first line of the probability distribution chart.  Step 2:  Directly underneath the first line, write the probability of the event happening.   The probability of the sausage machine producing 0 misshapen sausages a day is 0.09. Write “0.09” directly under “0”.  The probability of the sausage machine producing 1 misshapen sausages a day is 0.07. Write “0.07” directly under “1”.  The probability of the sausage machine producing 2 misshapen sausages a day is 0.1. Write “0.1” directly under “2”.  The probability of the sausage machine producing 3 misshapen sausages a day is 0.04. Write “0.04” directly under “3”.  The probability of the sausage machine producing 4 misshapen sausages a day is 0.12. Write “0.12” directly under “4”.  The probability of the sausage machine producing 5 misshapen sausages a day is 0.02. Write “0.02” directly under “5”.     That’s how to Construct a probability distribution!  Check out our YouTube Channel for hundreds more statistics how to videos!      ------------------------------------------------------------------------------ If you prefer an online interactive environment to learn R and statistics, this free R Tutorial by Datacamp is a great way to get started. If you're are somewhat comfortable with R and are interested in going deeper into Statistics, try this Statistics with R track . Comments are now closed for this post. Need help or want to post a correction? Please post a comment on our Facebook page and I'll do my best to help! Construct a probability distribution in Easy Steps was last modified: October 12th, 2017 by Stephanie     By Stephanie | August 24, 2009 | Statistics How To |    ← Probability of selecting a person from a group or committee  Find the Mean of the Probability Distribution / Binomial →         Find an article   Search       Feel like "cheating" at Statistics? Check out the grade-increasing book that's recommended reading at top universities!                 Privacy policy.       Copyright © 2018 Statistics How To Theme by: Theme Horse Powered by: WordPress    Back to Top                   Stat Trek  Teach yourself statistics                Home    Tutorials    AP statistics    Stat tables    Stat tools    Calculators    Books    Help        Overview  AP statistics  Beyond AP statistics  Simple linear regression  Probability sampling  Matrix algebra    AP tutorial  Test preparation  Practice test  AP formulas  FAQ  AP study guides  AP calculators    Binomial  Chi-square  f Dist  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Dist    Random numbers  Probability  Bayes rule  Combinations/permutations  Factorial  Event counter  Sample planning    Graphing  Scientific  Financial    Statistics  AP study guides  Probability  Survey sampling    Statistics dictionary  Problems and solutions  Formulas  Notation               Home    Tutorials   Overview  AP statistics  Beyond AP statistics  Simple Linear regression  Probability sampling  Matrix algebra     AP statistics   Test preparation  AP tutorial  Practice test  AP formulas  FAQ  AP study guides  AP calculators     Statistical tables   Binomial  Chi-square  f Distribution  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Distribution     Statistical tools   Random number generator  Probability calculator  Bayes rule calculator  Combinations/permutations  Factorial calculator  Event counter  Sample planning wizard     Handheld calculators   Graphing  Scientific  Financial     Books   Statistics  AP study guides  Probability  Survey sampling     Help   Statistics dictionary  Problems and solutions  Formulas  Notation                      AP Statistics Tutorial   Exploring Data    The basics   Variables  Population vs sample  Mean and median  Variability  Position     Charts and graphs   Patterns in data  Dotplots  Histograms  Stemplots  Boxplots  Cumulative plots  Scatterplots  Comparing data sets     Regression   Measurement scales  Linear correlation  Linear regression  Regression example  Residual analysis  Transformations  Influential points     Categorical data   One-way tables  Two-way tables     Experimentation    Surveys   Data collection  Sampling methods  Bias in surveys     Experiments   Intro to experiments  Experimental design     Anticipating Patterns    Probability   Probability intro  Rules of probability     Random variables   Introduction  Distributions  Mean and variance  Independence  Combinations  Transformations  Simulation of events     Discrete variables   Binomial distribution  Negative binomial     Continuous variables   Normal distribution  Standard normal  t Distribution  Chi-square     Sampling distributions   Sampling distribution  Diff between props  Diff between means     Statistical Inference    Estimation   Estimation problems  Standard error  Margin of error  Confidence interval     Confidence intervals   Proportions  Diff between props  Means  Diff between means  Diff between pairs  Regression slope     Hypothesis testing   Hypothesis test intro  Power of test  How to test     Hypothesis tests   Proportions  Diff between props  Mean  Diff between means  Diff between pairs  Goodness of fit test  Homogeneity  Independence  Regression slope     Appendices    ■ Practice exam    ■ Notation    ■ AP stat formulas             AP Statistics Lessons   The basics   Variables  Population vs sample  Mean and median  Variability  Position   Charts and graphs   Patterns in data  Dotplots  Histograms  Stemplots  Boxplots  Cumulative plots  Scatterplots  Comparing data sets   Regression   Measurement scales  Linear correlation  Linear regression  Regression example  Residual analysis  Transformations  Influential points   Categorical data   One-way tables  Two-way tables   Surveys   Data collection  Sampling methods  Bias in surveys   Experiments   Intro to experiments  Experimental design   Probability   Probability intro  Rules of probability   Random variables   Introduction  Distributions  Mean and variance  Independence  Combinations  Transformations  Simulation of events   Discrete variables   Binomial distribution  Negative binomial   Continuous variables   Normal distribution  Standard normal  t Distribution  Chi-square   Sampling distributions   Sampling distribution  Diff between props  Diff between means   Estimation   Estimation problems  Standard error  Margin of error  Confidence interval   Confidence intervals   Proportions  Diff between props  Means  Diff between means  Diff between pairs  Regression slope   Hypothesis testing   Hypothesis test intro  Power of test  How to test   Hypothesis tests   Proportions  Diff between props  Mean  Diff between means  Diff between pairs  Goodness of fit test  Homogeneity  Independence  Regression slope   Appendices   Practice exam  Notation  AP stat formulas        Binomial Probability Distribution  To understand binomial distributions and binomial probability, it helps to 
        understand binomial experiments and some associated notation; so we 
        cover those topics first.  Binomial Experiment  A binomial experiment is a statistical experiment that has the following properties:   The experiment consists of n repeated trials.  Each trial can result in just two possible outcomes. We call one of these 
        outcomes a success and the other, a failure.  The probability of success, denoted by P , is the same on every 
        trial.  The trials are independent ; 
	        that is, the outcome on one trial does not affect the outcome on other trials.   Consider the following statistical experiment. You flip a coin 2 times and count 
        the number of times the coin lands on heads. This is a binomial experiment 
        because:   The experiment consists of repeated trials. 
            We flip a coin 2 times.  Each trial can result in just two possible 
            outcomes - heads or tails.  The probability of success is 
            constant - 0.5 on every trial.  The trials are independent; that is, getting heads on one trial does not affect 
	        whether we get heads on other trials.   Notation  The following notation is helpful, when we talk about binomial probability.   x : The number of successes that result 
            from the binomial experiment.  n : The number of trials in the 
            binomial experiment.  P : The probability of success on an 
            individual trial.  Q : The probability of failure on an 
            individual trial. 
	        (This is equal to 1 - P .)  n! : The factorial of n (also known as n factorial).  b( x ; n, P ): Binomial 
            probability - the probability that an n -trial 
	        binomial experiment results in exactly  x successes, 
	        when the 
	        probability of success on an individual trial is P .  n C r : The number of combinations of n things, taken r at a time.        Binomial Distribution  A binomial random variable is the number of successes x in n repeated trials of a binomial experiment. The probability distribution of a binomial random variable is called 
	        a binomial distribution .  Suppose we flip a coin two times and count the number of heads (successes). The 
        binomial random variable is the number of heads, which can take on values of 0, 
        1, or 2. The binomial distribution is presented below.    Number of heads  Probability    0  0.25    1  0.50    2  0.25    The binomial distribution has the following properties:   The mean of the distribution (μ x ) is equal to n * P .  The variance (σ 2 x ) is n * P * ( 1 - P ).  The standard deviation (σ x ) is 
            sqrt[ n * P * ( 1 - P ) ].   Binomial Formula and Binomial Probability  The binomial probability refers to the probability that a 
        binomial experiment results in exactly  x successes. For example, 
        in the above table, we see that the binomial probability of getting exactly one 
        head in two coin flips is 0.50.  Given x , n , and P , we can compute the binomial probability 
        based on the binomial formula:   Binomial Formula. Suppose a binomial 
	        experiment consists of n trials and results in x successes. If 
	        the probability of success on an individual trial is P , then the 
	        binomial probability is:  b( x ; n, P ) = n C x * P x * (1 - P) n - x  or b( x ; n, P ) = { n! / [ x! (n - x)! ] } * P x * (1 - P) n - x     Example 1  Suppose a die is tossed 5 times. What is the probability of getting exactly 2 
        fours?  Solution: This is a binomial experiment in which the number of trials is 
        equal to 5, the number of successes is equal to 2, and the probability of 
        success on a single trial is 1/6 or about 0.167. Therefore, the binomial 
        probability is:  b(2; 5, 0.167) = 5 C 2 * (0.167) 2 * (0.833) 3  b(2; 5, 0.167) = 0.161  Cumulative Binomial Probability  A cumulative binomial probability refers to the probability 
        that the binomial random variable falls within a specified range (e.g., 
        is greater than or equal to a stated lower limit and less than or 
        equal to a stated upper limit).  For example, we might be interested in the cumulative binomial probability of 
        obtaining 45 or fewer heads in 100 tosses of a coin (see Example 1 below). This 
        would be the sum of all these individual binomial probabilities.  b(x < 45; 100, 0.5) = b(x = 0; 100, 0.5) + b(x = 1; 100, 0.5) + ... + 
        b(x = 44; 100, 0.5) + b(x = 45; 100, 0.5)   Binomial Calculator  As you may have noticed, the binomial formula requires many time-consuming 
	        computations. The Binomial Calculator can do this work for you - quickly, 
	        easily, and error-free. Use the Binomial Calculator to compute binomial 
	        probabilities and cumulative binomial probabilities. The  
		    calculator is free. It can found in the Stat Trek
            main menu under the Stat Tools tab. Or you can tap the button below.  Binomial Calculator   Example 2  What is the probability of obtaining 45 or fewer heads in 100 tosses of a coin?  Solution: To solve this problem, we compute 46 individual probabilities, 
        using the binomial formula. The sum of all these probabilities is the answer we 
        seek. Thus,  b(x < 45; 100, 0.5) = b(x = 0; 100, 0.5) + b(x = 1; 100, 0.5) + . . . 
        + b(x = 45; 100, 0.5) b(x < 45; 100, 0.5) = 0.184           Example 3  The probability that a student is accepted to a prestigious college is 0.3. If 
        5 students from the same school apply, what is the probability that at most 2 
        are accepted?  Solution: To solve this problem, we compute 3 individual probabilities, 
        using the binomial formula. The sum of all these probabilities is the answer we 
        seek. Thus,  b(x < 2; 5, 0.3) = b(x = 0; 5, 0.3) + b(x = 1; 5, 0.3) + b(x = 2; 5, 
        0.3) b(x < 2; 5, 0.3) = 0.1681 + 0.3601 + 0.3087 b(x < 2; 5, 0.3) = 0.8369  Example 4  What is the probability that the world series will last 4 games?  5 games?
        6 games? 7 games? Assume that the teams are evenly matched.  Solution: This is a very tricky application of the binomial 
        distribution.  If you can follow the logic of this solution, you have
        a good understanding of the material covered in the tutorial, to this
        point.  In the world series, there are two baseball teams.  The series
        ends when the winning team wins 4 games.  Therefore, we define a success as a 
        win by the team that ultimately becomes the world series champion.  For the purpose of this analysis, we assume that the teams are evenly matched.  
        Therefore, the probability that a particular team wins a particular game is
        0.5.  Let's look first at the simplest case.  What is the probability that the series
        lasts only 4 games.  This can occur if one team wins the first 4 
        games.  The probability of the National League team winning 4 games 
        in a row is:  b(4; 4, 0.5) = 4 C 4 * (0.5) 4 * (0.5) 0 = 0.0625  Similarly, when we compute the probability of the American League team 
        winning 4 games in a row, we find that it is also 0.0625.  
        Therefore, probability that the series ends in
        four games would be 0.0625 + 0.0625 = 0.125; since the series would end if
        either the American or National League team won 4 games in a row.  Now let's tackle the question of finding probability that the world series 
        ends in 5 games.  The trick in finding this solution is to recognize that
        the series can only end in 5 games, if one team has 
        won 3 out of the first 4 games.  So let's first find the probability
        that the American League team wins exactly 3 of the first 4 games.  b(3; 4, 0.5) = 4 C 3 * (0.5) 3 * (0.5) 1 = 0.25  Okay, here comes some more tricky stuff, so listen up.  Given that the 
        American League  team has won 3 of the first 4 games, the American League team 
        has a 50/50 chance of winning the fifth game to end the series.
        Therefore, the probability of the American League team winning the 
        series in 5 games is 0.25 * 0.50 = 0.125.  Since the National League
        team could also win the series in 5 games, the probability that 
        the series ends in 5 games would be 0.125 + 0.125 = 0.25.  The rest of the problem would be solved in the same way.  You should find
        that the probability of the series ending in 6 games is 0.3125; and
        the probability of the series ending in 7 games is also 0.3125.               Bestsellers Advanced Placement Statistics Updated daily 1. Barron's AP Statistics, 9th Edition $18.99  $12.91 2. Barron's AP Statistics, 8th Edition $18.99  $20.00 3. Barron's AP Statistics, 7th Edition $18.99  $10.00 4. Cracking the AP Statistics Exam, 2017 Edition: Proven Techniques to Help You Score a 5 (College Test Preparation) $19.99  $19.99 Today's Bargain Book Understandable Statistics $319.95  $65.00  80% off See more Statistics books ... Bestsellers Handheld Calculators Updated daily 1. Texas Instruments TI-84 Plus CE Lightning Graphing Calculator $150.00  $150.00 2. Texas Instruments Ti-84 plus Graphing calculator - Black $108.99  $108.99 3. Texas Instruments VOY200/PWB Graphing Calculator $200.00  4. Sharp EL-W535B WriteView Scientific Calculator $24.99  $39.99 Today's Bargain Calculator Texas Instruments Nspire CX CAS Graphing Calculator $185.00  $143.49  22% off See more Graphing Calculators ... Bestsellers Statistics and Probability Updated daily 1. Barron's AP Statistics, 9th Edition $18.99  $12.91 2. Naked Statistics: Stripping the Dread from the Data $16.95  $13.70 3. Statistics For Dummies (For Dummies (Math & Science)) $19.99  $13.11 4. How to Lie with Statistics $13.95  $8.98           About  Contact  Privacy  Terms  Advertising    Advertise on Stat Trek  Copyright © 2018 StatTrek.com        Advanced          Show Ads  Hide Ads  About Ads                   Random Variables: Mean, Variance and Standard Deviation  A Random Variable is a set of possible values from  a random experiment.   Example: Tossing a coin: we could get Heads or Tails.  Let's give them the values Heads=0 and Tails=1 and we have a Random Variable "X":    So:   We have an experiment (like tossing a coin)  We give values to each event  The set of values is a Random Variable   Learn more at Random Variables .  Mean, Variance and Standard Deviation   Example: Tossing a single unfair  die  For fun, imagine a weighted die (cheating!) so we have these probabilities:    1  2  3  4  5  6    0.1  0.1  0.1  0.1  0.1  0.5       Mean or Expected Value: μ  When we know the probability p of every value x we can calculate the Expected Value (Mean) of X:   μ = Σxp   Note: Σ is Sigma Notation , and means to sum up.  To calculate the Expected Value:   multiply each value by its probability  sum them up     Example continued:    x  1  2  3  4  5  6    p  0.1  0.1  0.1  0.1  0.1  0.5    xp  0.1  0.2  0.3  0.4  0.5  3    μ = Σxp The expected value is 4.5 Note: this  is a weighted mean :  values with higher probability have higher contribution to the mean.   Variance: Var(X) The Variance is:  Var(X) = Σx 2 p − μ 2  To calculate the Variance :  square each value and multiply by its probability  sum them up and we get Σx 2 p  then subtract the square of the Expected Value μ 2    Example continued:    x  1  2  3  4  5  6    p  0.1  0.1  0.1  0.1  0.1  0.5    x 2 p  0.1  0.4  0.9  1.6  2.5  18    Σx 2 p = 0.1+0.4+0.9+1.6+2.5+18 = 23.5  Var(X) = Σx 2 p − μ 2 = 23.5 - 4.5 2 = 3.25  The variance is 3.25   Standard Deviation: σ The Standard Deviation is the square root of the Variance:  σ = √Var(X)    Example continued:    x  1  2  3  4  5  6    p  0.1  0.1  0.1  0.1  0.1  0.5    x 2 p  0.1  0.4  0.9  1.6  2.5  18    σ = √Var(X) = √3.25 = 1.803...  The Standard Deviation is 1.803...   Let's have another example! (Note that we run the table downwards instead of along this time.)   You plan to open a new McDougals Fried Chicken, and found these stats for similar restaurants:    Percent  Year's Earnings    20%  $50,000 Loss    30%  $0    40%  $50,000 Profit    10%  $150,000 Profit    Using that as probabilities for your new restaurant's profit, what is the Expected Value and Standard Deviation?    The Random Variable is X = 'possible profit'.  Sum up xp and x 2 p :     Probability p  Earnings ($'000s) x  xp  x 2 p    0.2  -50  -10  500    0.3  0  0  0    0.4  50  20  1000    0.1  150  15  2250    Σp = 1    Σxp = 25   Σx 2 p = 3750       μ  = Σxp = 25  Var(X) = Σx 2 p − μ 2  = 3750 − 25 2  = 3750 − 625 = 3125  σ = √3125 = 56 (to nearest whole number)  But remember these are in thousands of dollars, so:   μ = $25,000  σ = $56,000   So you might expect to make $25,000, but with a very wide deviation possible.  Let's try that again, but with a much higher probability for $50,000:  Example (continued):  Now with different probabilities (the $50,000 value has a high probability of 0.7 now):     Probability p  Earnings ($'000s) x  xp  x 2 p    0.1  -50  -5  250    0.1  0  0  0    0.7  50  35  1750    0.1  150  15  2250    Σp = 1  Sums:  Σxp = 45  Σx 2 p = 4250       μ  = Σxp = 45  Var(X) = Σx 2 p − μ 2  = 4250 − 45 2  = 4250 − 2025 = 2225  σ = √2225 = 47 (to nearest whole number)  In thousands of dollars:   μ = $45,000  σ = $47,000   The mean is now much closer to the most probable value.  And the standard deviation is a little smaller (showing that the values are more central.)   Continuous Random Variables can be either Discrete 
	or Continuous :  Discrete Data can only take certain values (such as 1,2,3,4,5)  Continuous Data can take any value within a range (such as a person's height)  Here we looked only at discrete data, as finding the Mean, Variance and Standard Deviation of continuous data  needs Integration .  Summary   A Random 
			Variable is a variable whose possible values are numerical outcomes 
			of a random experiment.  The Mean (Expected 
			Value) is: μ = Σxp  The Variance is: Var(X) = Σx 2 p − μ 2  The Standard Deviation is: σ = √Var(X)      Random Variables Data Index     Copyright © 2016 MathsIsFun.com                        If you're seeing this message, it means we're having trouble loading external resources on our website.  If you're behind a web filter, please make sure that the domains *.kastatic.org and *.kasandbox.org are unblocked.        Main content       To log in and use all the features of Khan Academy, please enable JavaScript in your browser.                                                                                If you're seeing this message, it means we're having trouble loading external resources on our website.  If you're behind a web filter, please make sure that the domains *.kastatic.org and *.kasandbox.org are unblocked.        Main content       To log in and use all the features of Khan Academy, please enable JavaScript in your browser.                                                                     Stat Trek  Teach yourself statistics                Home    Tutorials    AP statistics    Stat tables    Stat tools    Calculators    Books    Help        Overview  AP statistics  Beyond AP statistics  Simple linear regression  Probability sampling  Matrix algebra    AP tutorial  Test preparation  Practice test  AP formulas  FAQ  AP study guides  AP calculators    Binomial  Chi-square  f Dist  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Dist    Random numbers  Probability  Bayes rule  Combinations/permutations  Factorial  Event counter  Sample planning    Graphing  Scientific  Financial    Statistics  AP study guides  Probability  Survey sampling    Statistics dictionary  Problems and solutions  Formulas  Notation               Home    Tutorials   Overview  AP statistics  Beyond AP statistics  Simple Linear regression  Probability sampling  Matrix algebra     AP statistics   Test preparation  AP tutorial  Practice test  AP formulas  FAQ  AP study guides  AP calculators     Statistical tables   Binomial  Chi-square  f Distribution  Hypergeometric  Multinomial  Negative binomial  Normal  Poisson  t Distribution     Statistical tools   Random number generator  Probability calculator  Bayes rule calculator  Combinations/permutations  Factorial calculator  Event counter  Sample planning wizard     Handheld calculators   Graphing  Scientific  Financial     Books   Statistics  AP study guides  Probability  Survey sampling     Help   Statistics dictionary  Problems and solutions  Formulas  Notation                       Beyond AP Statistics   Probability Basics   Sets and subsets  Statistical experiment  Counting data points  Probability problems  Bayes theorem   Small Samples   Estimate proportion  Test proportion   Distributions   Probability distribution  Discrete/continuous  Hypergeometric  Multinomial  Poisson  f Distribution   Power   Region of acceptance  Power of a test   How to find power            Beyond AP Statistics   Probability Basics   Sets and Subsets  Statistical Experiments  Counting Data Points  Probability Problems  Bayes Theorem   Small Samples   Estimate Proportion  Test Proportion   Distributions   Probability Distribution  Discrete/Continuous  Hypergeometric  Multinomial  Poisson  f Distribution   Power   Region of Acceptance  Power of a Test   How to Find Power        What is a Probability Distribution?  A probability distribution is a table or an equation that links each outcome 
    of a statistical experiment with its probability of occurrence.  Probability Distribution Prerequisites  To understand probability distributions, it is important to understand variables. 
	    random variables, and some notation.   A variable is a symbol ( A , B , x , y , 
	    etc.) that can take on any of a specified set of values.  When the value of a variable is the outcome of a statistical experiment , that variable is a random variable .   Generally, statisticians use a capital letter to represent a random variable and 
	    a lower-case letter, to represent one of its values. For example,   X represents the random variable X.  P(X) represents the probability of X.  P(X = x) refers to the probability that the random variable X is equal to a 
		    particular value, denoted by x. As an example, P(X = 1) refers to the 
		    probability that the random variable X is equal to 1.   Probability Distributions  An example will make clear the relationship between random variables and 
        probability distributions. Suppose you flip a coin two times. This simple 
	    statistical experiment can have four possible outcomes: HH, HT, TH, and TT. 
	    Now, let the variable X represent the number of Heads that result from this 
	    experiment. The variable X can take on the values 0, 1, or 2. In this example, 
	    X is a random variable; because its value is determined by the outcome of a 
	    statistical experiment.  A probability distribution is a table or an equation that links 
	    each outcome of a statistical experiment with its probability of occurrence. 
	    Consider the coin flip experiment described above. The table below, which 
	    associates each outcome with its probability, is an example of a probability 
	    distribution.    Number of heads  Probability    0  0.25    1  0.50    2  0.25    The above table represents the probability distribution of the random variable 
	    X.        Cumulative Probability Distributions  A cumulative probability refers to the probability that the 
	    value of a random variable falls within a specified range.  Let us return to the coin flip experiment. If we flip a coin two times, we might 
	    ask: What is the probability that the coin flips would result in one or fewer 
	    heads? The answer would be a cumulative probability. It would be the 
	    probability that the coin flip experiment results in zero heads plus the 
	    probability that the experiment results in one head.  P(X < 1) = P(X = 0) + P(X = 1) = 0.25 + 0.50 = 0.75  Like a probability distribution, a cumulative probability distribution can be 
	    represented by a table or an equation. In the table below, the cumulative 
	    probability refers to the probability than the random variable X is less than 
	    or equal to x.    Number of heads: x  Probability: P(X = x) Cumulative Probability: P(X < x)  0  0.25  0.25   1  0.50  0.75   2  0.25  1.00      Uniform Probability Distribution The simplest probability distribution occurs when all of the values of a 
	    random variable occur with equal probability. This probability 
	    distribution is called the uniform distribution .  Uniform Distribution. Suppose the 
		    random variable X can assume k different values. Suppose also that the P(X = x k ) 
		    is constant. Then,  P(X = x k ) = 1/k  Example 1 Suppose a die is tossed. What is the probability that the die will land on 5? Solution: When a die is tossed, there are 6 possible outcomes represented 
	    by: S = { 1, 2, 3, 4, 5, 6 }. Each possible outcome is a random variable (X), 
	    and each outcome is equally likely to occur. Thus, we have a uniform 
	    distribution. Therefore, the P(X = 5) = 1/6. Example 2 Suppose we repeat the dice tossing experiment described in Example 1. This 
	    time, we ask what is the probability that the die will land on a number that is 
	    smaller than 5? Solution: When a die is tossed, there are 6 possible outcomes represented 
	    by: S = { 1, 2, 3, 4, 5, 6 }. Each possible outcome is equally likely to occur. 
	    Thus, we have a uniform distribution. This problem involves a cumulative probability. The probability that the die 
	    will land on a number smaller than 5 is equal to: P( X < 5 ) = P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4) P( X < 5 ) = 1/6 + 1/6 + 1/6 + 1/6 = 2/3           Bestsellers Handheld Calculators Updated daily 1. Texas Instruments TI-84 Plus CE Lightning Graphing Calculator $150.00  $150.00 2. Texas Instruments Ti-84 plus Graphing calculator - Black $108.99  $108.99 3. Texas Instruments VOY200/PWB Graphing Calculator $200.00  4. Sharp EL-W535B WriteView Scientific Calculator $24.99  $39.99 Today's Bargain Book Understandable Statistics $319.95  $65.00  80% off See more Statistics books ... Bestsellers Statistics and Probability Updated daily 1. Barron's AP Statistics, 9th Edition $18.99  $12.91 2. Naked Statistics: Stripping the Dread from the Data $16.95  $13.70 3. Statistics For Dummies (For Dummies (Math & Science)) $19.99  $13.11 4. How to Lie with Statistics $13.95  $8.98           About  Contact  Privacy  Terms  Advertising    Advertise on Stat Trek  Copyright © 2018 StatTrek.com              Stack Exchange Network  Stack Exchange network consists of 173 Q&A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.  Visit Stack Exchange                   current community        help  chat    Mathematics        Mathematics Meta      your communities     Sign up or log in to customize your list.    more stack exchange communities   company blog           Tour Start here for a quick overview of the site     Help Center Detailed answers to any questions you might have     Meta Discuss the workings and policies of this site     About Us Learn more about Stack Overflow the company     Business Learn more about hiring developers or posting ads with us                                Log In  Sign Up           Mathematics       Questions    Tags    Users    Badges    Unanswered        Ask Question            _  Mathematics Stack Exchange is a question and answer site for people studying math at any level and professionals in related fields. Join them; it only takes a minute:   Sign up    Here's how it works:   Anybody can ask a question  Anybody can answer  The best answers are voted up and rise to the top         Find the probability distribution of X?         up vote  1  down vote  favorite       Suppose Nokia store places 20 of its cell phones on a clearance sale, unknown to anyone 5 of these cell phones are defective. A customer selects 3 cell phones at random for inspection. Let X be the number of defective cell phones in the sample. Find the probability distribution of X?  My attempt:  $$
        \begin{matrix}
        x & 0 & 1 & 2 & 3 \\
        P(X=x) & 0.015625 & 0.140625 & 0.421875 & 0.421875 \\
        \\
        \end{matrix}
$$  I calculated the values using the binomial distribution: $$
P(X = x) = nCx \ p^{n-x} (1-p)^x$$  Where, $n$=5, $x$=0,1,2,3 and $p$=(.25) [from 5/20]  Is this the correct way to do this?    probability     share | cite | improve this question     asked Nov 25 '12 at 8:12       Siyanda   972 3 16 35              1      IF $p$ is the probability of detecting a faulty item ('success'), then it should be $\binom{n}{x}p^{x}(1-p)^{n-x}$ – Alex  Nov 25 '12 at 8:17            @Alex for sampling with replacement – Henry  Nov 25 '12 at 8:34            Still, important to point out that even though Binomial is inappropriate, Binomial was mishandled. – André Nicolas  Nov 25 '12 at 10:03        add a comment |           2 Answers 2     active  oldest  votes            up vote  4  down vote      No it is not correct.  In particular with only a quarter of the phones defective, getting three defectives out of three in the sample should have a low probability.  A second point is that the customer is presumably looking at three different phones (sampling without replacement ), so you not be using the binomial distribution.  As an example, the probability of three phones defective, sampling without replacement, is  $${3 \choose 3} \times \frac{5}{20} \times \frac{4}{19} \times \frac{3}{18} \approx 0.00877$$ rather than your ${3 \choose 3}0.25^0 0.75^3 = 0.421875.$     share | cite | improve this answer     answered Nov 25 '12 at 8:30       Henry   87.9k 3 63 133              add a comment |             up vote  3  down vote      This experiment is described by a hypergeometric distribution . For every $0\leqslant k\leqslant 3$,
$$\mathbb P(X=k)=\frac{\color{red}{{5\choose k}}\color{green}{{15\choose 3-k}}}{{20\choose 3}}$$
hence
$$
\mathbb P(X=3)=\frac{\color{red}{5\cdot4\cdot3}}{20\cdot19\cdot18},
\qquad
\mathbb P(X=2)=\frac{\color{red}{5\cdot4}\cdot3\cdot\color{green}{15}}{20\cdot19\cdot18},
$$
$$
\mathbb P(X=1)=\frac{\color{red}{5}\cdot3\cdot\color{green}{14\cdot15}}{20\cdot19\cdot18},
\qquad
\mathbb P(X=0)=\frac{\color{green}{13\cdot14\cdot15}}{20\cdot19\cdot18}.$$     share | cite | improve this answer      edited Nov 25 '12 at 9:19             answered Nov 25 '12 at 8:43       Did   239k 23 201 429                  (a) I thought you preferred hints rather than the full answer. (b) Your four probabilities do not add up to 1. – Henry  Nov 25 '12 at 9:04             @Henry (a) Maybe so. A naked link to the WP page might have been more educational here. (b) Thanks, misprint corrected. – Did  Nov 25 '12 at 9:25            OK - I will delete those comments – Henry  Nov 25 '12 at 12:09        add a comment |            Your Answer             draft saved  draft discarded             Sign up or log in    Sign up using Google   Sign up using Facebook   Sign up using Email and Password        Post as a guest       Name     Email             Post as a guest       Name     Email            discard  By posting your answer, you agree to the privacy policy and terms of service .     Not the answer you're looking for?                            Browse other questions tagged probability or ask your own question .         asked    5 years, 4 months ago      viewed     17,906 times       active    5 years, 4 months ago       Visit Chat      Linked     2   Assume you toss a fair coin 25 times. How many completed runs do you expect to observe?      Related   2 Probability and Counting 1 Probability $1$ item is defective out of a sample of $10$? 1 A Conditional probability problem and binomial formula 0 Binomial distribution question regarding one after another selection 0 discrete probability distributions; Dish problem 2 Simple calculations of mean, standard deviation, and probability 0 probability of a proportion point estimate 0 Probability of a Normally Distributed Random Sample 0 Find Probability Distribution Parameters For Sample With Percentage Given 1 Clarifying whether hypergeometric distribution can be used here      Hot Network Questions     Distance between two points on the Moon    Does backing up a database shrink the transaction log size?    Paths & Wasting Time    How can hashes be unique if they are limited in number?    Wiping an SSD with Parted Magic seemed too quick    I have a crush on a coworker but won't act on it, how can I tell my boyfriend about it and that I'll remain faithful?    How many arguments were passed?    Can a zeroth order reaction be reversible?    Comma Code - Automate the Boring Stuff    The meaning of "squishy hand-wringer"    To switch back to reg oil (from semi-synthetic) do i have to (or should I) flush the engine 1st?    What to do about a colleague playing pranks on a manager?    Why isn't the bracha "Al Achillat Matzah" said before eating the Afikoman?    My prefix ends fast    How does a device (dashcam) recognize if it is connected to a computer or a power source?    My contract is expiring and it won't be renewed yet I'm in the midst of a crucial project    Sort spelled-out serial numbers    Why would a god tolerate an impostor in his church?    How are we supposed to use debug logs for a specific Apex class only    How do I keep presenting progressively more challenging encounters to my PCs without making them think that the world is gaining Levels as they are?    I have two siblings; we're locked in a war    Why don't all objects bounce like rubber balls?    Is your future tax bracket the ONLY consideration for Roth vs Traditional 401(k) accounts?    Evil Campaigns:How to explain the difference between being evil and being a jerk?    more hot questions     question feed             Mathematics   Tour  Help  Chat  Contact  Feedback  Mobile     Company   Stack Overflow  Stack Overflow Business  Developer Jobs  About  Press  Legal  Privacy Policy       Stack Exchange Network   Technology  Life / Arts  Culture / Recreation  Science  Other        Stack Overflow  Server Fault  Super User  Web Applications  Ask Ubuntu  Webmasters  Game Development   TeX - LaTeX  Software Engineering  Unix & Linux  Ask Different (Apple)  WordPress Development  Geographic Information Systems  Electrical Engineering   Android Enthusiasts  Information Security  Database Administrators  Drupal Answers  SharePoint  User Experience  Mathematica   Salesforce  ExpressionEngine® Answers  Stack Overflow em Português  Blender  Network Engineering  Cryptography  Code Review   Magento  Software Recommendations  Signal Processing  Emacs  Raspberry Pi  Stack Overflow на русском  Programming Puzzles & Code Golf   Stack Overflow en español  Ethereum  Data Science  Arduino  Bitcoin    more (29)        Photography  Science Fiction & Fantasy  Graphic Design  Movies & TV  Music: Practice & Theory  Worldbuilding  Seasoned Advice (cooking)   Home Improvement  Personal Finance & Money  Academia  Law    more (15)        English Language & Usage  Skeptics  Mi Yodeya (Judaism)  Travel  Christianity  English Language Learners  Japanese Language   Arqade (gaming)  Bicycles  Role-playing Games  Anime & Manga  Puzzling  Motor Vehicle Maintenance & Repair    more (33)        MathOverflow  Mathematics  Cross Validated (stats)  Theoretical Computer Science  Physics  Chemistry  Biology   Computer Science  Philosophy    more (10)        Meta Stack Exchange  Stack Apps  API  Data  Area 51        Blog  Facebook  Twitter  LinkedIn   site design / logo © 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required . rev 2018.4.9.29787       Mathematics Stack Exchange works best with JavaScript enabled                  Stack Exchange Network  Stack Exchange network consists of 173 Q&A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.  Visit Stack Exchange                   current community        help  chat    Mathematics        Mathematics Meta      your communities     Sign up or log in to customize your list.    more stack exchange communities   company blog           Tour Start here for a quick overview of the site     Help Center Detailed answers to any questions you might have     Meta Discuss the workings and policies of this site     About Us Learn more about Stack Overflow the company     Business Learn more about hiring developers or posting ads with us                                Log In  Sign Up           Mathematics       Questions    Tags    Users    Badges    Unanswered        Ask Question            _  Mathematics Stack Exchange is a question and answer site for people studying math at any level and professionals in related fields. Join them; it only takes a minute:   Sign up    Here's how it works:   Anybody can ask a question  Anybody can answer  The best answers are voted up and rise to the top         Find the probability distribution of the random variable X.         up vote  2  down vote  favorite       A fair coin is flipped $3$ times. Consider a random variable $X$ which is the number of runs. Number of runs is the number of changes of letter $H$ and $T$. For example, $HHH$ has one run, $TTH$ has two runs and $THT$ has three runs. Find the probability distribution of the random variable $X$.  My work: I don't understand the phrasing of this question. In examples in my textbook and online $X$ is defined as the number of heads or tails. But I can't follow where the example in this question is going. I would think that $TTH$ and $THT$ would both have 2 runs since $HHH$ only has one. I don't know what zero runs would be either. Can anyone give me guidance on what exactly this question means? I'm pretty sure I can solve it once I understand what the number of runs means.  The outcomes would be:  $HHH$ $X=1$  $HTH$  $HHT$  $THH$  $TTH$ $X=2$  $HTT$  $THT$ $X=3$  $TTT$  I don't know what number of $X$ would correspond with each.    probability  probability-theory  statistics     share | cite | improve this question      edited Feb 28 '16 at 1:07             asked Feb 28 '16 at 1:00       Koalafications   45 9                  Why should you think that there must be an outcome described by the event zero runs?  In my interpretation, $\text{Onerun}=\{HHH,TTT\}, \text{Tworuns}=\{HHT,HTT,TTH,THH\}, \text{Threeruns}=\{HTH,THT\}$.  Something like $TTHHHHTTHTHHHT$ would have $7$ runs.  $\underbrace{TT}_1 \underbrace{HHHH}_2 \underbrace{TT}_3 \underbrace{H}_4 \underbrace{T}_5 \underbrace{HHH}_6 \underbrace{T}_7$ – JMoravitz  Feb 28 '16 at 1:13             Thank you that makes sense to me! – Koalafications  Feb 28 '16 at 1:20        add a comment |           1 Answer 1     active  oldest  votes            up vote  1  down vote  accepted      I think you are just missing what the book means by a "run". E.g.  HHHHHHHHHHHHTTTTTTHHHHHTTTTTT  has 4 runs: a run of heads, followed by a run of tails, followed by a run of heads, followed by a run of tails. All the H's or T's that are next to each other get lumped together to form a single "run", regardless of how many there are. Thus HHH and TTT have 1 run, HTT, HHT, THH, TTH have 2 runs, and HTH and THT have 3 runs.  Thus $P(X=1) = 2/8, P(X=2) = 4/8, P(X=3) = 2/8$ is the distribution of $X$.     share | cite | improve this answer     answered Feb 28 '16 at 1:17       nullUser   15.9k 4 33 96                  Yep that was it! Haha I was looking at it like some sort of pattern but it was really that simple. Guess I like to over complicate things. Thank you though! – Koalafications  Feb 28 '16 at 1:20        add a comment |            Your Answer             draft saved  draft discarded             Sign up or log in    Sign up using Google   Sign up using Facebook   Sign up using Email and Password        Post as a guest       Name     Email             Post as a guest       Name     Email            discard  By posting your answer, you agree to the privacy policy and terms of service .     Not the answer you're looking for?                            Browse other questions tagged probability  probability-theory  statistics or ask your own question .         asked    2 years, 1 month ago      viewed     174 times       active    2 years, 1 month ago       Related   8 Probability about a coin games 0 Chance on winning by throwing a head on first toss. 2 Conditional probability or Bayes' theorem 0 Random variables probability-infinite coin toss 2 Probability with coins 1 Confusion - Expected Waiting Times for pattern HHH 2 expected waiting time for HTT - alternative way 0 Possible outcomes on a three coin flip. 3 How to rationalize through coin flip probability? 1 Outcome space of 3 biased coins      Hot Network Questions     Single Word : Cannot be resolved by waiting and trying again    Distance between two points on the Moon    New "professional" branding perceived as "dull"    Why is this shared-neutral wiring bad?    Crash during startup on a recent corporate computer    Why should a software QA engineer need to learn JavaScript?    My prefix ends fast    How can hashes be unique if they are limited in number?    Is the quotient of a toric variety by a finite group still toric    A Mathematical Paradox About Probabilities    How old is the oldest light visible from Earth?    What can I do to get models to take my small camera more seriously?    I have two siblings; we're locked in a war    Does backing up a database shrink the transaction log size?    Why will the BFS reenter broadside rather than engine first?    How to make people spread over the earth?    How are we supposed to use debug logs for a specific Apex class only    Why is first order logic not categorical, as Löwenheim-Skolem just reduces the infinity to infinitely countable    What is the power / energy of a discrete time constant signal?    I have a crush on a coworker but won't act on it, how can I tell my boyfriend about it and that I'll remain faithful?    What is the most effective use of phoenixes in battle?    How do I keep presenting progressively more challenging encounters to my PCs without making them think that the world is gaining Levels as they are?    Stat block from Animate Dead    Having trouble getting my friends to get invested in the game    more hot questions     question feed             Mathematics   Tour  Help  Chat  Contact  Feedback  Mobile     Company   Stack Overflow  Stack Overflow Business  Developer Jobs  About  Press  Legal  Privacy Policy       Stack Exchange Network   Technology  Life / Arts  Culture / Recreation  Science  Other        Stack Overflow  Server Fault  Super User  Web Applications  Ask Ubuntu  Webmasters  Game Development   TeX - LaTeX  Software Engineering  Unix & Linux  Ask Different (Apple)  WordPress Development  Geographic Information Systems  Electrical Engineering   Android Enthusiasts  Information Security  Database Administrators  Drupal Answers  SharePoint  User Experience  Mathematica   Salesforce  ExpressionEngine® Answers  Stack Overflow em Português  Blender  Network Engineering  Cryptography  Code Review   Magento  Software Recommendations  Signal Processing  Emacs  Raspberry Pi  Stack Overflow на русском  Programming Puzzles & Code Golf   Stack Overflow en español  Ethereum  Data Science  Arduino  Bitcoin    more (29)        Photography  Science Fiction & Fantasy  Graphic Design  Movies & TV  Music: Practice & Theory  Worldbuilding  Seasoned Advice (cooking)   Home Improvement  Personal Finance & Money  Academia  Law    more (15)        English Language & Usage  Skeptics  Mi Yodeya (Judaism)  Travel  Christianity  English Language Learners  Japanese Language   Arqade (gaming)  Bicycles  Role-playing Games  Anime & Manga  Puzzling  Motor Vehicle Maintenance & Repair    more (33)        MathOverflow  Mathematics  Cross Validated (stats)  Theoretical Computer Science  Physics  Chemistry  Biology   Computer Science  Philosophy    more (10)        Meta Stack Exchange  Stack Apps  API  Data  Area 51        Blog  Facebook  Twitter  LinkedIn   site design / logo © 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required . rev 2018.4.9.29787       Mathematics Stack Exchange works best with JavaScript enabled                  Stack Exchange Network  Stack Exchange network consists of 173 Q&A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.  Visit Stack Exchange                   current community        help  chat    Mathematics        Mathematics Meta      your communities     Sign up or log in to customize your list.    more stack exchange communities   company blog           Tour Start here for a quick overview of the site     Help Center Detailed answers to any questions you might have     Meta Discuss the workings and policies of this site     About Us Learn more about Stack Overflow the company     Business Learn more about hiring developers or posting ads with us                                Log In  Sign Up           Mathematics       Questions    Tags    Users    Badges    Unanswered        Ask Question            _  Mathematics Stack Exchange is a question and answer site for people studying math at any level and professionals in related fields. Join them; it only takes a minute:   Sign up    Here's how it works:   Anybody can ask a question  Anybody can answer  The best answers are voted up and rise to the top         find the mean value of x if The probability distribution of a discrete random variable x is given         up vote  0  down vote  favorite       The probability distribution of a discrete random variable x is
 $$f (x)=   \begin{pmatrix}3 \\ x \end{pmatrix}   (1/4)^x (3/4)^{3-x} $$   Find the mean value of x.
Construct a cumulative distribution function for f (x).
i find out$$ P(X=o) = 0.421875$$ $$ P(X=1) = 0.421875$$ $$ P(X=2) = 0.140625$$ $$ P(X=3) = 0.015625$$ put the value in Binomial distribution.    probability  statistics  probability-theory  probability-distributions     share | cite | improve this question      edited May 22 '14 at 8:15             asked May 21 '14 at 16:32       bill   37 1 5              3      The distribution is Binomial, the expected value (aka mean) is $np$, what is $p$ and what is $n$ in your formula? I'll give you a hint, $n  = 3,$ now find $p$ (probability of success) yourself. – Nameless  May 21 '14 at 16:35             The essence of the probability argument is that we have 3 independent trials, and on each trial, outcome 1 occurs with probability p and some other outcome with probability 1−p. help me for finding mean value – bill  May 21 '14 at 17:08        add a comment |           2 Answers 2     active  oldest  votes            up vote  1  down vote      "The essence of the probability argument is that we have 3 independent trials, and on each trial, outcome 1 occurs with probability p and some other outcome with probability 1−p. help me for finding mean value"  I would say that
x = {0,1}, P(0) = 1-p, P(1) = p, E(x) =0*(1-p)+1*p = p  When $X = x_1+x_2+\cdots + x_n$, then $E(X) = E(x_1)+E(x_2)+\cdots+E(x_n)=np$ (generally apply to both dependent and independent experiments).     share | cite | improve this answer     answered May 22 '14 at 8:06       georg   2,185 1 7 9              add a comment |             up vote  0  down vote      This discrete random variable follows Binomial distribution, i.e.
$$f(x)=\binom{n}{x}p^x(1-p)^{n-x}$$
where $n$ is the number of trials and $p$ is the proportion. What this distribution mean is that when there are $n$ trials and every trial follows $p$ proportion(success rate), the probability that $x$ successes occur is $f(x)$. And for Binomial distribution, the expect(mean) value and variance are
$$E(X)=np$$
$$Var(X)=np(1-p)$$
respectively.   From above, you can know how to calculate the mean value. For cumulative distribution function, it is just the the sum of probability of first k outcomes, which can be expressed as
$$F(k;n,p)=P(X\le \lfloor k\rfloor)=\sum_{i=0}^{\lfloor k \rfloor} \binom{n}{i}p^i(1-p)^{n-i}$$
where $\lfloor k\rfloor$ is the largest integer that is less than or equal to $k$. For this problem, I think this form is enough because of the small $n$ that equals to 3.     share | cite | improve this answer      edited May 21 '14 at 17:32             answered May 21 '14 at 17:15       sdg   161 1 7                  firsly find the probablity – bill  May 21 '14 at 17:18        add a comment |            Your Answer             draft saved  draft discarded             Sign up or log in    Sign up using Google   Sign up using Facebook   Sign up using Email and Password        Post as a guest       Name     Email             Post as a guest       Name     Email            discard  By posting your answer, you agree to the privacy policy and terms of service .     Not the answer you're looking for?                            Browse other questions tagged probability  statistics  probability-theory  probability-distributions or ask your own question .         asked    3 years, 10 months ago      viewed     3,777 times       active    3 years, 10 months ago       Visit Chat      Related   0 Construct a distribution for discrete random variable from its mean 0 Random variable X has the following discrete distribution 1 Discrete Random Variables Probability Exercise - How to approach this 0 Probability distribution of discrete random variable $X^2$ 0 Random variables/probability distribution table 0 Finding the expected value of a particular discrete random variable. 0 Pdf for discrete vs continuous random variables. 2 Given the cumulative distribution function find a random variable that has this distribution. 0 Given the joint distribution of discrete random variables $x$ and $y$ , find the probability distribution of a “centred” version of $x$. 1 Find the mean of the discrete random variable      Hot Network Questions     Why was UNIX never backported to the PDP-7?    To switch back to reg oil (from semi-synthetic) do i have to (or should I) flush the engine 1st?    In US universities, are the sport coaches typically considered tenured professors?    Sort spelled-out serial numbers    Why is this shared-neutral wiring bad?    Can animated undead wear armor and use weapons?    How to build a trap to last the ages?    What technique is best for moving two large (20 kg) suitcases, reasonably short distances (<1km)?    New "professional" branding perceived as "dull"    Steaming with oil instead of water    What are healthy, productive ways to encourage students to progress to more advanced constructs as opposed to staying with the familiar?    I want to leave Islam, but they would execute and kill me. What do I do?    Evil Campaigns:How to explain the difference between being evil and being a jerk?    Why will the BFS reenter broadside rather than engine first?    Paths & Wasting Time    How many arguments were passed?    Talmud passage relevant to #metoo    Could a cave-in or avalanche in low gravity be dangerous?    How can hashes be unique if they are limited in number?    French A roads, Spanish E- roads, Dutch E roads and German roads    Taxonomy of genetically engineered species    A Mathematical Paradox About Probabilities    Git clone only works with ssh://git@.. and not with git@    I have two siblings; we're locked in a war    more hot questions     question feed             Mathematics   Tour  Help  Chat  Contact  Feedback  Mobile     Company   Stack Overflow  Stack Overflow Business  Developer Jobs  About  Press  Legal  Privacy Policy       Stack Exchange Network   Technology  Life / Arts  Culture / Recreation  Science  Other        Stack Overflow  Server Fault  Super User  Web Applications  Ask Ubuntu  Webmasters  Game Development   TeX - LaTeX  Software Engineering  Unix & Linux  Ask Different (Apple)  WordPress Development  Geographic Information Systems  Electrical Engineering   Android Enthusiasts  Information Security  Database Administrators  Drupal Answers  SharePoint  User Experience  Mathematica   Salesforce  ExpressionEngine® Answers  Stack Overflow em Português  Blender  Network Engineering  Cryptography  Code Review   Magento  Software Recommendations  Signal Processing  Emacs  Raspberry Pi  Stack Overflow на русском  Programming Puzzles & Code Golf   Stack Overflow en español  Ethereum  Data Science  Arduino  Bitcoin    more (29)        Photography  Science Fiction & Fantasy  Graphic Design  Movies & TV  Music: Practice & Theory  Worldbuilding  Seasoned Advice (cooking)   Home Improvement  Personal Finance & Money  Academia  Law    more (15)        English Language & Usage  Skeptics  Mi Yodeya (Judaism)  Travel  Christianity  English Language Learners  Japanese Language   Arqade (gaming)  Bicycles  Role-playing Games  Anime & Manga  Puzzling  Motor Vehicle Maintenance & Repair    more (33)        MathOverflow  Mathematics  Cross Validated (stats)  Theoretical Computer Science  Physics  Chemistry  Biology   Computer Science  Philosophy    more (10)        Meta Stack Exchange  Stack Apps  API  Data  Area 51        Blog  Facebook  Twitter  LinkedIn   site design / logo © 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required . rev 2018.4.9.29787       Mathematics Stack Exchange works best with JavaScript enabled                  Stack Exchange Network  Stack Exchange network consists of 173 Q&A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.  Visit Stack Exchange                   current community        help  chat    Mathematics        Mathematics Meta      your communities     Sign up or log in to customize your list.    more stack exchange communities   company blog           Tour Start here for a quick overview of the site     Help Center Detailed answers to any questions you might have     Meta Discuss the workings and policies of this site     About Us Learn more about Stack Overflow the company     Business Learn more about hiring developers or posting ads with us                                Log In  Sign Up           Mathematics       Questions    Tags    Users    Badges    Unanswered        Ask Question            _  Mathematics Stack Exchange is a question and answer site for people studying math at any level and professionals in related fields. Join them; it only takes a minute:   Sign up    Here's how it works:   Anybody can ask a question  Anybody can answer  The best answers are voted up and rise to the top         Probability $X$ is odd in a geometric distribution [duplicate]         up vote  1  down vote  favorite        This question already has an answer here:    Find the probability that a geometric random variable $X$ is an even number  2 answers     I have this problem:   Let $X$ be a Geometric distribution with parameter $p = \frac1{10}$.   What is the value of $P(x\text{ is odd})$?   What I got was $P(X = i) = pq^{i-1}$ where $p = \frac{1}{10}$ and $q = (1-p)$   Is this right?    probability  probability-distributions     share | cite | improve this question      edited Apr 24 '16 at 16:45       G. Sassatelli   22.4k 11 30 54        asked Mar 10 '14 at 0:10       userunknown   133 2 12           marked as duplicate by Meta, G. Sassatelli , Daniel Robert-Nicoud , Stefan4024 , Edward Jiang  Apr 24 '16 at 19:46   This question has been asked before and already has an answer. If those answers do not fully address your question, please ask a new question .          add a comment |           2 Answers 2     active  oldest  votes            up vote  3  down vote  accepted      $$P(\text{X is odd}) = P(X=1) + P(X=3) + P(X=5) + \ldots$$  The pmf of a geometric distribution is $q^{k-1}p$.  So $$P(\text{X is odd}) = q^{0} p + q^{2}p + q^{4} p + \ldots = p \sum_{k=0}^{\infty} q^{2k} = \frac{p}{1-q^{2}}$$  EDIT:  which equals $ \displaystyle\frac{1}{q+1}$ as pointed out by Dilip Sarwate in the comments     share | cite | improve this answer      edited Mar 10 '14 at 1:57             answered Mar 10 '14 at 1:43       Random Variable   25k 1 68 134              1      and $\displaystyle \frac{p}{1-q^2} = \frac{p}{(1-q)(1+q)} = \frac{1}{1+q}$ since $p = 1-q$. – Dilip Sarwate  Mar 10 '14 at 1:47         add a comment |             up vote  0  down vote      Imagine that if $X$ is odd then Alphonse wins the game, while if $X$ is even then Beti wins the game. It is clear that with probability $1$ one of Alphonse or Beti wins. Let $p$ be the probability A wins.  If A fails to win on the first trial, then the roles of A and B are reversed, and B has probability $p$ of winning. It follows that
$$p+\frac{9}{10}p=1.$$
Solve for $p$.     share | cite | improve this answer     answered Mar 10 '14 at 6:42       André Nicolas   442k 35 401 779              add a comment |       Not the answer you're looking for?                            Browse other questions tagged probability  probability-distributions or ask your own question .         asked    4 years, 1 month ago      viewed     3,330 times       active    1 year, 11 months ago       Visit Chat      Linked     3   Find the probability that a geometric random variable $X$ is an even number      Related   1 Calculate Expectancy of Poisson distribution with Poisson parameter 1 help on geometric probability distribution problem? 3 Probability, geometric distribution 0 Testing if distribution is geometric and finding is parameter. 0 Minimum of two geometric distributions 1 Probability and Name of Distribution 0 Finding the Probability of a multiple using geometric distribution 0 Tossing a coin. X is the number tosses. What is the probability that X is an odd number? 2 Simple Probability - Enumeration and Geometric Distributions 1 Interesting probability distribution of a mixed type random variable $Y$      Hot Network Questions     How many arguments were passed?    Why do we need so many classes in design patterns    What technique is best for moving two large (20 kg) suitcases, reasonably short distances (<1km)?    Is your future tax bracket the ONLY consideration for Roth vs Traditional 401(k) accounts?    How can we teach good naming practice for students learning Java?    Was the Twin Pines Mall scene at 1:15am for in-universe or out-of-universe reasons?    Is the quotient of a toric variety by a finite group still toric    How old is the oldest light visible from Earth?    Why is first order logic not categorical, as Löwenheim-Skolem just reduces the infinity to infinitely countable    French A roads, Spanish E- roads, Dutch E roads and German roads    Two lasers between two mirrors    Why isn't the bracha "Al Achillat Matzah" said before eating the Afikoman?    Print the Previous Answer!    Why use baking powder instead of yeast?    How to make a Tree diagram using images as vertices, that goes left to right    What is the power / energy of a discrete time constant signal?    Splitting equilateral triangle with shortest curve    What are healthy, productive ways to encourage students to progress to more advanced constructs as opposed to staying with the familiar?    Why should a software QA engineer need to learn JavaScript?    Employees are gaming their performance metrics, instead of doing what's best for the company    Is there a way to avoid the recurring cost of microbes, when composting at home?    How does a device (dashcam) recognize if it is connected to a computer or a power source?    The meaning of "squishy hand-wringer"    My contract is expiring and it won't be renewed yet I'm in the midst of a crucial project    more hot questions             Mathematics   Tour  Help  Chat  Contact  Feedback  Mobile     Company   Stack Overflow  Stack Overflow Business  Developer Jobs  About  Press  Legal  Privacy Policy       Stack Exchange Network   Technology  Life / Arts  Culture / Recreation  Science  Other        Stack Overflow  Server Fault  Super User  Web Applications  Ask Ubuntu  Webmasters  Game Development   TeX - LaTeX  Software Engineering  Unix & Linux  Ask Different (Apple)  WordPress Development  Geographic Information Systems  Electrical Engineering   Android Enthusiasts  Information Security  Database Administrators  Drupal Answers  SharePoint  User Experience  Mathematica   Salesforce  ExpressionEngine® Answers  Stack Overflow em Português  Blender  Network Engineering  Cryptography  Code Review   Magento  Software Recommendations  Signal Processing  Emacs  Raspberry Pi  Stack Overflow на русском  Programming Puzzles & Code Golf   Stack Overflow en español  Ethereum  Data Science  Arduino  Bitcoin    more (29)        Photography  Science Fiction & Fantasy  Graphic Design  Movies & TV  Music: Practice & Theory  Worldbuilding  Seasoned Advice (cooking)   Home Improvement  Personal Finance & Money  Academia  Law    more (15)        English Language & Usage  Skeptics  Mi Yodeya (Judaism)  Travel  Christianity  English Language Learners  Japanese Language   Arqade (gaming)  Bicycles  Role-playing Games  Anime & Manga  Puzzling  Motor Vehicle Maintenance & Repair    more (33)        MathOverflow  Mathematics  Cross Validated (stats)  Theoretical Computer Science  Physics  Chemistry  Biology   Computer Science  Philosophy    more (10)        Meta Stack Exchange  Stack Apps  API  Data  Area 51        Blog  Facebook  Twitter  LinkedIn   site design / logo © 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required . rev 2018.4.9.29787       Mathematics Stack Exchange works best with JavaScript enabled                  Stack Exchange Network  Stack Exchange network consists of 173 Q&A communities including Stack Overflow , the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.  Visit Stack Exchange                   current community        help  chat    Mathematics        Mathematics Meta      your communities     Sign up or log in to customize your list.    more stack exchange communities   company blog           Tour Start here for a quick overview of the site     Help Center Detailed answers to any questions you might have     Meta Discuss the workings and policies of this site     About Us Learn more about Stack Overflow the company     Business Learn more about hiring developers or posting ads with us                                Log In  Sign Up           Mathematics       Questions    Tags    Users    Badges    Unanswered        Ask Question            _  Mathematics Stack Exchange is a question and answer site for people studying math at any level and professionals in related fields. Join them; it only takes a minute:   Sign up    Here's how it works:   Anybody can ask a question  Anybody can answer  The best answers are voted up and rise to the top         probability distribution $X, Y$ and $X+Y$         up vote  1  down vote  favorite       A box contains $5$ ticket, $\{ 0 , 0 , 0 , 4 , 4\}$.  Drawing two tickets at random w/o replacement.  $X$ be the sum of the first two draws and Y be the outcome of the first draw.  Question: Find distribution of $X + Y$?   What I did --
I found the distribution of $X$. There are only three possible sums: $0 , 4, 8$. And there are $10$ ways to get these sums:  $(0,0), (0,0), (0,4),(0,4),(0,0),(0,4),(0,4),(0,4),(0,4) (4,4)$. Therefore $P(0) = 3/10 , P(4) = 6/10, P(8) = 1/10$.   My other question:  Is there another way (different from what I did) to get the distribution of $X$?  And I have no idea how to get distribution of $Y$.    statistics  probability-distributions     share | cite | improve this question      edited Jan 30 '15 at 16:10       Daniel W. Farlow   17.1k 11 41 86        asked Feb 7 '13 at 5:46       hibc   48 2 9                  The distribution of $Y$ is given in the problem! I imagine you want the distribution of $X+Y$. – André Nicolas  Feb 7 '13 at 5:49             @AndréNicolas Yes I want to get X+Y, but I thought I need to first get the distribution of Y.  Y is given ?? really ?????  I'm confused,, – hibc  Feb 7 '13 at 5:51            Yes @hibc, it is given. There are 2 choices for the draw 0 or 4. It can be 0 with probabiliy _____ and 4 with probability _______. Also, it follows ________ distribution since it takes one of two values. – Inquest  Feb 7 '13 at 5:57        add a comment |           1 Answer 1     active  oldest  votes            up vote  1  down vote  accepted      For the distribution of $X$, I do not think there is a substantially better way than yours.  For the distribution of $X+Y$, we do something similar. If it helps to keep track of things, draw a tree diagram. Let $W=X+Y$.  Case $1$: Maybe the first pick was $0$. This has probability $\frac{3}{5}$. Given that this happened, the probability of picking a $0$ next is $\frac{2}{4}$. Then $W=0$. The probability of piking a $4$ is  $\frac{2}{4}$. Then $W=4$.  Case $2$: Maybe the first pick was a $4$. This has probability $\frac{2}{5}$. In that case, the probability of next getting $0$ is $\frac{3}{4}$, and we get $W=8$. The probability of getting a $4$ is $\frac{1}{4}$. In that case $W=12$.  So $W$ takes on values $0$, $4$, $8$, $12$.  For the probability that $W=0$, locate all the ways $W$ can be $0$. This only happens in one way, $0$ then $0$. We can see that the probability is $\frac{3}{5}\frac{2}{4}$.  Similarly, the probability that $W=4$ is $\frac{3}{5}\frac{2}{4}$.  Similarly, $\Pr(W=8)=\frac{2}{5}\frac{3}{4}$ and the probability that $W=12$ is $\frac{2}{5}\frac{1}{4}$.  As a partial correctness check, add up our four numbers. We should get $1$, and we do.     share | cite | improve this answer      edited Feb 7 '13 at 6:12             answered Feb 7 '13 at 6:06       André Nicolas   442k 35 401 779                  Thank you very much!! very detailed and clear ! – hibc  Feb 7 '13 at 6:09            You are welcome. I went into the gruesome detail in order to show that if one stays organized, things are likely to turn out well. – André Nicolas  Feb 7 '13 at 6:14        add a comment |            Your Answer             draft saved  draft discarded             Sign up or log in    Sign up using Google   Sign up using Facebook   Sign up using Email and Password        Post as a guest       Name     Email             Post as a guest       Name     Email            discard  By posting your answer, you agree to the privacy policy and terms of service .     Not the answer you're looking for?                            Browse other questions tagged statistics  probability-distributions or ask your own question .         asked    5 years, 2 months ago      viewed     733 times       active    3 years, 2 months ago       Related   2 Another textbook problem on probability 0 Hypergeometric Distribution Definition 1 Finding the distribution of a random vector in a conditional probability problem 1 Standard Error - Statistics 1 Statistics - Chebychev's Inequalities 0 Null Hypothesis and Binomial Distribution 0 Expected value and standard error question. 0 How to calculate the expected value in the following box? 0 Replacement Problem 1 Sampling distribution of random sample      Hot Network Questions     Why doesn’t the IRS just send me a bill for the taxes I owe based on the info they already have?    How does the title 'The Imitation Game' justify the story of the movie?    Does writing matter a lot in research?    What technique is best for moving two large (20 kg) suitcases, reasonably short distances (<1km)?    Comma Code - Automate the Boring Stuff    Wiping an SSD with Parted Magic seemed too quick    Automated way to create a directory tree    What type of aircraft is depicted in the Taylor Swift music video "Look What You Made Me Do"?    What's the probable cause for extremely low inbound traffic and high outbound traffic?    What is the most effective use of phoenixes in battle?    Count the number of lines in macro argument    How to make people spread over the earth?    Should I prepare new homework exercises each year?    Why would a god tolerate an impostor in his church?    Could a cave-in or avalanche in low gravity be dangerous?    Why do we need so many classes in design patterns    For n nodes that are connected to at most m and at least 2 other nodes, what values of n and m always allow the connections to not intersect?    My prefix ends fast    How to ensure two standalone documents have same dimensions    “GOD is real, unless declared integer.”    What are healthy, productive ways to encourage students to progress to more advanced constructs as opposed to staying with the familiar?    A Mathematical Paradox About Probabilities    What am I talking about here?    How can hashes be unique if they are limited in number?    more hot questions     question feed             Mathematics   Tour  Help  Chat  Contact  Feedback  Mobile     Company   Stack Overflow  Stack Overflow Business  Developer Jobs  About  Press  Legal  Privacy Policy       Stack Exchange Network   Technology  Life / Arts  Culture / Recreation  Science  Other        Stack Overflow  Server Fault  Super User  Web Applications  Ask Ubuntu  Webmasters  Game Development   TeX - LaTeX  Software Engineering  Unix & Linux  Ask Different (Apple)  WordPress Development  Geographic Information Systems  Electrical Engineering   Android Enthusiasts  Information Security  Database Administrators  Drupal Answers  SharePoint  User Experience  Mathematica   Salesforce  ExpressionEngine® Answers  Stack Overflow em Português  Blender  Network Engineering  Cryptography  Code Review   Magento  Software Recommendations  Signal Processing  Emacs  Raspberry Pi  Stack Overflow на русском  Programming Puzzles & Code Golf   Stack Overflow en español  Ethereum  Data Science  Arduino  Bitcoin    more (29)        Photography  Science Fiction & Fantasy  Graphic Design  Movies & TV  Music: Practice & Theory  Worldbuilding  Seasoned Advice (cooking)   Home Improvement  Personal Finance & Money  Academia  Law    more (15)        English Language & Usage  Skeptics  Mi Yodeya (Judaism)  Travel  Christianity  English Language Learners  Japanese Language   Arqade (gaming)  Bicycles  Role-playing Games  Anime & Manga  Puzzling  Motor Vehicle Maintenance & Repair    more (33)        MathOverflow  Mathematics  Cross Validated (stats)  Theoretical Computer Science  Physics  Chemistry  Biology   Computer Science  Philosophy    more (10)        Meta Stack Exchange  Stack Apps  API  Data  Area 51        Blog  Facebook  Twitter  LinkedIn   site design / logo © 2018 Stack Exchange Inc; user contributions licensed under cc by-sa 3.0 with attribution required . rev 2018.4.9.29787       Mathematics Stack Exchange works best with JavaScript enabled     ]>>
startxref
0
%%EOF
             
466 0 obj< �v��w�)�c\H�2����	��� >stream
H���=o�0�w��S���M0<���!@�c3�ZE2$����{�7b���Z����xD�cA	�LA��/�HT����p��M�8�(�����&<��?}�~��	J��\g�ן���L*9�k%r77lF0����C���é	�=B膡n�����>>��[���pH���[���4Y���pa�̼|1>Iۿ���SZMDIK��%�]p�79VÈ1V��$�]��l��l5#X"��N��¦*6�k(�`�\ܯ��a��Z����۪X��wL�ȉ}ҩO�P KF4�h�&K;��Z�Ǣ��%��!H�c�M�U���?�̳">FVc2Ϳ!+����<��4"N�:�]x����3\_iy�#�8��"K� ���J�	��X+0��|!�Ӿv��~���ƻ���������w7��@��)�d��.e�fF��'0.��f��њQsAp�D̚|�?�e�Kt��E�f�= �0�����\P���G���>�N�C�� {B��k}�����'t����"/3��? ��su
endstreamendobj479 0 obj< >stream
H��UMk�@����ޙ��B�!�B�ѡ�^6��(�������	��U(�X���{o�}��l�	#�}�}�\�*�w� mx��Ĺ���ŕ$� �\�>�X<��I�������P��,D���8�&�q���^�zj�=LM��㸭�0ő1tпd}�d����8�k;5�+b��������!��n�'q�?����B)KnY�O!�퓡��1�)�Z8Y*\�g��%����a���q�~��	�ͻ�.��@�ĞWՎP�������?H��e��|�zf1�j�5�W�Q���qc#�C���w�{�j$��͈TY#��@�=�(�l'Q�9�CF�`�VY���LM\�}����X�(��Q'p�	�d��M���m����m�S[���Mq����#���O�F�+� ��p"
endstreamendobj482 0 obj< ���#:�
�v��yf����G�P/Y�1	�?E�qu��n������0�I��!���ŉwM[��%ҭ��]�!���	�{���6*�[���� �` ��jw
endstreamendobj487 0 obj< K���Jg����PC8�L�m6J:�E��R`.�Óp�$(W(��Ze�7E������9�Ið��R.�������u�Ϝ���E��+������5��%��;�;��{��'�״�v�y���#�ߨ|�6��Ͽ�������n�xsV�9�����C���3�V���z�R��eg�3B�r�F|�lKc��w���������\�4>q��ƍg��ħ#�0Y(��H	��vg��%�Eg6?h(��t-��	�2�����h>�ɫ���u%���9��j��^y����b��%��]Ʃ��6p�Pq�����\��2g<��V4W��̶���[��:?g���Y�������V�W,x����Ó�P������x�>���י�1��9��2��p��_7H�a�����s/w��N�-�/χ�����<�	�+n������꟥�B��tJ��n7n]T�Xy��{���X_ɕ���(�4�,���i�ꭵ�o=�2(S��؏��b���l���'L��R�[�)m{Dq���E{���]��Q�X��y�O���H�p�d]ԃ�M���>�/X��s�q�ĳ�w1�_Hh˴�U����թ"{�>`H<��-��0�|��4l𔆲�kȣO.]�Uu}a�o;�#�ܪ��_x�gI3�[`:�{��_���pzT�C���G��Z�\�����-�yך��%���ѳk�.���<;"�}q��4�=A,��9�6��Y�b2�L�¬*��m��X�������(�u�p�o�P8&�x�P�,�a0��Pcˢ�u����&�H8H�����(l�4�^�$-���"e��1'���8	i�k���/�C�$2�l�Pԁm���9
C�z��.��B�&�$":�%I�x:�#����^?�xS��B��Pa֑$�b�l28l$���#]g6e�(��
�¤|̍��
�i� �Wo��4�ңlV���{.j"�b�fJ�`���%��s�@m��/��o�G߬"'� �+��`��ʓ�o���M�/,E��S[pN��W��aWN�"���i��"�k�p��G��-�n�j[0�:,c���3�W5]���s�ǎ��[����e�\��'�))�l�j���ąh��]o^�����.��H?�s4`p����:��l�*#�Yt�M��������n�l_ޙvX�iÁ���Kﲆ����dK��Ew��|Թ_r?w����lY�sx_���Z����3��/�h��m�����U���=ne���vDe^|6��<K��m�.��7891'�\Ot�������'��brj��rZb�KK��%^EU=��6��?�5�'�s��PHN�}b~46��=b��_ߗ�@ӥ�h#���y�X�Ɏ��ѦNv R�	*f�(;�/
c������j�M$4ʔe2�(�z
�N6����Ad�j DPg̈́&���$����&e.䑹�鄁��M)#��_�^R:E7DԚ(�BX)81	�!�$�J0�H��d���4�׽�m �(��6M�V�-��A���A��$E�}gs��%����`�,2�Q�:�<�@�d[���!V�&"_�B#G`#�yi9���l#E7Iidd�� ���PM#H�CD��LdJ�B���qo�\�����R�q�"I�'+�\߯'��8j��V��(���i%LI@������+�Z%D�F��ڤIP�7^��Bm
m�KW�q��J����`�Z���
%�C��*-�M/�k4g��â8�����T���"j����1r���ˍ(0�0���	����BM!+芫+�5DMD�U?�x��\v���������Tu�������}q=��w����"�1Rك@p��vw}k�b���B��P!	��Y�zeh������=(�|'�E�Zu'�o'xۡ���v>������R1�k�Ů��"����$wE�����R���w�s��*���\����녨|G;;g��RP*2-N�~���Q��ܕȴ�
s?]�
���qX�B�U�6��#�B�݉`����	$KB���N�J�VaĨ��0!D��B�&	��BBb�30\ݕ3����G`�j-p����VO�oҼ�?J��������V�K?*W����M.d�K����*<�t���8?�$�I�~r����+�h^O�Y\��};qoM�"��-��x̅>]B�%�N�3�t��v��w�ў�v�H�m�_�ٴ���U�E�Sʾ�ug�J\���[�\h�����YI�y�u����]v_�Y2e���b��Y48O��n�s]��NG����n+���q��j�Ď���Ŝ�	I���h�ocΛ�M3�-ێ̚i{��_2L�2��X�� ty��`��7����nR(����c�Q�,`V�n,�1�Z�V����>E�J���!nj�Gy�����b���{�3|���|����᠄d��`PA��1_2A=^Mo�)��P�M�	a��k�~��H�eXZoP��movЏg�<�/�U�>*��V�{qé���y9����q���׍:�S�1���\Q���k-�?��W'��Z�}�)�AW��o]�ِ��WJ6*9{��c��`�QI	��U�6ߨ�o
?[sϑ��B�b�N���e�S�~;&���s��=�Z�d4f׌[���~'rI�B7z��ӏemzhsG��z[s}KԤg��[y��,��bZc�����Q����I�k��'{�6i\W�"T�;2R���ւ���s[2�^�;���|&������K2�5<�5��Bb�A�b�im��W�߿�RC8;�����7NK2��씕o�dX`�@���|7Nu{R�a�����ӾzX��]Cn5c��������n�U��.�0ة��?��Պ����3�S�{ps�+án�Mi5b��mܳ�[�3�����G�볅�E������i�/���:�xh� ��͗�eNMVs�����>5+ ��MѸtG�mFN�������K֏������=6_��ٺ�,N[�w����Ӱi����C8��x�P"�P^
�.U�
�w"�E�D���Zu�S~9X��0��l�w���5��c~5�hV����p�o��+LaX���8G&�7��A,���؟��<�G
F$���D� �a6g$V�;<�у�0�H�"`����s؀3dC,~/��d>! ������/���-�	]�B�
���@
��;�=�� �B)��bI�v�D����X�5`�	�mP���ej��Fb��G�"�F��u\������z�%k^�1b�0mF�Q��'��a�o� �*�h�f�8�0+�d��+�zȂ�����<$���
yBQ5�潥�R�՝ߊ��s\c �Ck�jHB�m�v�f	��'��$�Ć�W�C��&r;������������Y*ka�t�N/�n�5�[LB.	r.CO: �E��@|��F�rQ�2�r����PZ�!<���LB��ؐ�d�%�'$��&'I9G.�v�N��t6��>4����4���
ZM��_��9L���G��ղ��:k�[�q�\��+��:�����Œ��wиiĉ��,n�Q"�c�D0C<����Q"�X����mDD�`r�e�$T������:�͈�6��x��h��q�}b���#�(K�O�$��I.)D�+H%�yrQj���+h"M�[�n�����izBd��H����b�V�x��}�>a�X1�b�YG�9�7�m���	��k�n�rކ�A)�+��|�D_b,�.QJ���d�6|�P��s�d�!�>#m�cj�@�P]�D2���=0� �����h�{�:�I���� �$ ���� [ |Q2o
Jn7��A|��Q>�u�W�VB.]�Y*����$����4��4���ibJ-h��8�[���fst��u��C3�:z�~c�zc����������c�Ǳq�܇Ǆ������C�0?��',A�`j����L�\Ub�6iՄ:�N�u�Su,������i�(��Ci�����s_��thڳ��{�=��{�ﾛ���s��}�	w�ߕ�1�%���4t�K�*�W�I�d3�D�.�]������I-}`�b)L#���f�5�~�����F��񭑶v�'����o���~���q����K��Ύ��g���ڲ�y[�)�ذ��οEڧy��lv�j�5�U������Jm%�"K"~�BSL�Kj�.��:}�� ��)���LCQ��6LKZʹ�-ly�-���jKb�B
6i1]c�Fu-O&���M�ݵ�}/�Y�r��|h�ŜQ���c}S�X2���Jm=2n6A�V�l)r�F?�#5��bhM�3GA-Ǩ�K��X��!0�K����D,����`#��>�@�eV�Xn�a��F��Á�Z�i1�j���@٘>�:�`B��>*�7�j���q;wD��j�B&��x5�9��7k�>^�&����ߗ����W1��a��i3��4���H����7�Ǹ$yTc%z�>�9�Ĺqe��͹\�B��bZf$��X�[7S�͹J����5����`S�^���܆�E��|-3���8�9��C��%<"�9\LKkIB�1��b�2�vl��IЊ��L��H2c��rn�$�]�2�� ���%��D��?��u���P�³@�56�%�DpN1�n��l�i*O��q����۔�ٌ������0�vf0�\�`�=Fs�d4�5�+���\sfE�j��q%_�����u�����M��NF���z|Y��Z,�,�6>����o_�9�)�ܴ�Q�`iqQ^m�+�2&��/[�z,���*-	���=�{�4m>��i�/|̭,�ج&���w����,#`�b���dlku����>������nS_�Ҹ��&���*>��C����H� ~q�Y�%w�n�,t�nD�k��c�o�k��P@��ǈ&�0BC�"L�^�w��~�8��!nϩp�����x�C�.���n�-^eu��T�����P�x�M�X�+��͇ËP'��h�%MC5ƾu�R�ʇ��-��~~)��E�G��
� ���8N"��{C��� �����C��"Hc��B��?�1`+�<�6�M��ئm�߅�c��'pi3�{H�3\'?�KH�����&��7Dp��f'�
dd�lG� q_= �m�c�/�Pa��~��ӓh������b�9�p_*��:�P�pǮ�q�OCs��6����EH!�ǁ��#�]EtJW�Q��a� ͡x�m����6P��P�o1~�b�͘������& 8`x`��{��s,J.��	��-x:M�\DG�u�A_X��#�}��-���U�����up�T����U>o�=V��K�<����鶹78i��4 �b��ty[��P=��]_o�ޘ���D�U��}%��=�hFLa����m��?��c? ���g�����/��"Y,�g`�8�x�	~�"/�+��.Fi֒f�4kI�v1>�`���r��1�|��q�$��8"��OM8ٙQM�W�u�����qvU�2C�j��'�OquX���Tl$�;e�G��F8�������\��:w?Zq� �d��{̓Q�e#��?��3\��=�p�3�c��oy�M���@"�B����E�i��*��������ּu��/���\�R<����U�p0�U�`�j��+��/w��o��E��z/N���p�&��Ba���sx[&?g(?�����'��xJV�(�Ĵ 6YJu�(b�@��������-����C��K!�	-�8Z��*|~,pm�CMX|hH�9�8�|}��,;ފH|��j��B��t�m]v��C��ݽ��忖��*����}�z�A��; |(> l�3j�R��'O�e�_�L��w	|a�<=c�ʸ� ��\wHb��WQ���Dd�<� �3��,���8Q �U�l� 뱲K�B�s���u�]��r�ȭ�%b���F}UlT�e�񜳓���8Szg��4���қ��B�h8VG��?��\%8����V��fq���4����r�Z̓Y+��\�]��,��ୣ?��Ѽ`����<�i��U�Y���'�9m��W�[����v�2������s_�}�'6���؉oc���|�����{��8�`���t�܂T�L��<*�L���HQ�j�YP;���*� �}�(�5��ox�o�0��â��RF`�H���#�	fx�g��L��F�dZ��"M,*�VGS��J�ZJ��hhQ�ai��J�V�X<,��A<����k��L]�l;���z� ^�oc]P�G#��9-U��ad�����W�gF��qЈ�aÈ��6r�z����=��o/fLN���43D߄�	b�*30��ɡh3������"�蛔cn��:#�������>}cC�ه&�	m2i�o�we�Ev�RN�$�ES��l2ć:����~��C��H�h8��qfN�)�w�x�{1�������kX��&��bɾ�x��s�%�MgR�)��ڨ}%� 7�1u3���d/b��6���C���$�M�,��O�g3C�0�uE�a���Hx"�$�EnCuJ�x]Z�Ā{����N�"�J��
7n��^�~��~��)��kf��|<�{��fNV�K�`W=>�H��P�4�d1>��*n"�a� ��e3b�R�>EGD_F&���,DZ<�P;c�cӑ�"N�sm�o��m�(!V���<"�@O�� �p�(�g!�d�n0s%� ���ƿ�R,{�����<s.mDmJ*-�+���g�Y�uY�''����}F���^��,WG�#P F��p<���KӔ�� H9n���=�0�X�����sT;��gH�U��]ϔzK�yӵ�>)� V����<�{�6ۜ�k��>�}�^Y�,ty�L��x�tr $�Ut�b�ㄌQ>��	R�2��L�I�,�!�@�ږV���?��[x�3�*�2Ե6�%�t�:����"�J�3�&�B������u�<��]\G?;��;��{����-+�������w͊P���&
�^���V$�f�<:w �K��P'�=�Q<��V��S }�4�M��T%�'�d���G�D�'�t��<�����QaBrH��0#�洁=Ҭz��w� ϕ�y�j��A�i�נ�j~X_�����ǌ}��8:/�Z��F������L���ɋ��]W���ߗg�%��y祊s�滟��c��w�
h*�@�� ���

endstreamendobj489 0 obj< ����y�7�ȖK_m�#k����{�r�R5�$�7#.�y?F�RCXed��`p��-�靽�o�������������pv]mz���a���k{�'*�m "Jҽr���Һ!F+i�H�g�'�J>�oJ�����%%7B�y�.����Y`!|��D�B���t�����o���AF��+��vZ=�0z�)�9����j�i�mg��Xs(�Ms!�o����USAA�����=\yP]	Q�R-�M��6����2�+[��ɚ�r�_,��O�#��Pk�7�<� 0�����CQV��%JR+���n�N*GY�X�;��0�6����킍���s4'�-�F� 0�H�U����qyMLr���Fҩw���� �&����0�ǎ�#sce(lw��q�vԌxz7�[�n@�N��(Q�l��V��;��.T����?��A�8�E��0�HhH�A�0Һen��mHiM!�άCv�|ۙ��WƇ�R��8�F�·`�&�`�o��Ivo16�AW�,1�� �Sg��8c�s������*� *X0X۫-�O�iA�`D08i�^�=��",5"�� �4[��Cb��sm������t���>=�%:I%-�50̛�D���ʸJ+��u�Vd7� ��q:�Q�m�txF��dQ��*=$k�%���P��+���?�&�׉�밾*�Ǭ�QOF)1/�ٯv{�g�C4%HܸR�Q��}��n�Lm:	�R����qU�s/����,���?Od|���!K�A&���~W�O|�x���َ�.g�a��$>YvX����WҮ[Jm��������(W����ۧ�w�0��%?Qap(cs����}ʕQP?,l�1���r9�<�2�&E;"w�O�"��d�n�戮J�f>��P�ǚ�	�tbj��� �~t�4�y��CXdX�:RxZ;e��Xl�p��R!�|�L��g�)�����F;YrF9��{�w��r�yU���,�����O>���Hޚ8'�8ۙc@��o������n��8�wy�7���yT���z���H<�(6G1&Z;�s]��C -�8���Ou��B�QZ �����Y�>vzKo��N�>",�C��4q=�NX���~,F��d[���;[M�25,I�i7,|�!�����N��e�G	�"��eB��ݝ�P;�]�ΦfdY��'�ڞ5���ʏ�}B�����w(��x"vJ���o�m�=���,|�\"&�T>mNHM����"��zwf�������Va��f*�T(DwϦ�g�"-l���T�F��2)�ghG���H �
�z��V�O0/A�z�v̻.��5�Paēk���V�d}��r�dSV?|U��#��/�����gQ�>�-�rz���EPk����0i�#|�muf���kPC�`㩣ػy�o�*�ϐtMp8l�'t_pIAW@��d�΍�"r�m��Ϋ��i����Ab��؉��>�'�xZ'�u��4U�Mݿ�\;I��Ni������>��sI����Hv��?GH���gp�r��)Gh���`|m뼡��$w��2bgD+'�!lf����up7�j�pW��.�+We��.���ؑ��v%8P��b9a��r�Ly$���Hh�Y�)�(��ȶ������h��tt�4g�|���e�'�}�ZQ��s{�|Z/(��;
Aime��Z���{�Eh[���� @��kp��`<�B�5 ެ��&�?Ÿ�7�Y��e��DSP�85|��A��(`�C��4�@#S����S�A--y�i1�6��)<�8�3�v�/�ŏ�M�fW�C�v�bv~^��ͨ:�Z�	���p/�p�f���5d�x3;�����n�J̾_0T���_"0xE{���(�F��ƭ�hq��7��'{�/���M
!c���{�H�l�`_��0�P�jv�P���Qy��? oS}
endstreamendobj3 0 obj< S(Dѥ�XfL�����g�1�%8���x	�2�;���X��:��Yg4�*r\��fk8>��mZH�[UE���7D�Xk��{q�p�>��G���o�#β,��N�ju��]~V��1p�x�3�or�K�Hn)� ���;ddvOW7_2qbL�?�!}��מp�Q+�3W~g�c!���!ɼ����8adX�Q��T�]�_3�32��9�=Ծ�Q���Ӭv�WWl�����ۢ�vhtZ���4�Zs�WXU�n���k�"<�/�%�?���q�Y�1K#e�\��[�D�J��,��$�����c�� �;�r�dI�r�c	�&E;Ʊ���!TĒ�GŊ ��O��
ey�����Ly�0����s��-��^�!U�Ř���L
�"c��<��_tˉ@V��V��F*��JO[�_Z�,�G�/:�&�d ����[�L/��Y��4�OU��T��/z�_�3mhPK�}�|�ђ�"���Bʮ�99���*zoC}k��ˬ��ˣ�F=ݬ���?�N/''�`]@;��>�T�E���S��m������ 5���`���8�	"���HK���D�O��B�!m^Y�C	O�qxɨ\<_Z��e Y�"J�	8���G�Bh�����۠o��BDT�������/tM��E�Ǖ�yƟN��=3SV��mY[�ee~}S�������;�/�ʡ�g�5^ݔ��O��
�p��J�e���	���r�hU:��E���C�E��c�S��=�U�&�2�%[��X��=*m��|0��*i�׎�3�v���!�.׳��J:V���{�5ԻqWV��SFQ��iiWР�Y�hb �b8��k��]�f�^Wő�{�a�c_;G/��'/�*�9ѵ�O�#X�r�e\�!l$m�hDR��x[�5P|�+e�����@�j�-�Gpx�ڭ�����<"���O�'�rX ً.?-Y4�0>Iy������G��7��S��T����؋��������z�s�o{0��&���lA߸B�ֳ�~eo1�1T�:���Jt��k���r��D����ЈC5���6��4��3��=��2�}�h���u��Sʥ�XW������@f����n��7Kv�W���FJ$Q�!(��*9�)L��6�U���Wy �[?�ld;ߙB�ȼ3���}�!���B��NiH))�}"�Q	�M"ו��KIE�Ң�Oh�wP�-ߟ���Ǚ3�y�9s�<����o�O�o�#���F���G��	�o0�L\2s��R_r�WKʼ: ��H0ɀ��S$��$�Lb"��G���������/���zb�s�D�Pl�Ԓr�j����^�<���U/�d�h�|]���,�d�_п�&�hªn�����{�\�ZW��耥�n��ϗ�o�rQ�
���9��uQ�>��`�]�]ZU=��Vh� t���H9=ybb9ϯ�I� ړ-��uA�|;y�b�綉��+Z�u�9G��8H� �	�W���6�/�����;*cI�W���2�W ���+B>�3+y�:a,&#            Search       Statistics How To   Statistics for the rest of us!       Home  Tables   Binomial Distribution Table  F Table  PPMC Critical Values  T-Distribution Table (One Tail and Two-Tails)  Chi Squared Table (Right Tail)  Z-table (Right of Curve or Left)    Probability and Statistics   Binomials  Chi-Square Statistic  Expected Value  Hypothesis Testing  Non Normal Distribution  Normal Distributions  Probability  Regression Analysis  Statistics Basics  T-Distribution  Multivariate Analysis  Sampling    Calculators   Variance and Standard Deviation Calculator  Tdist Calculator  Permutation Calculator / Combination Calculator  Interquartile Range Calculator  Linear Regression Calculator  Expected Value Calculator  Binomial Distribution Calculator    Statistics Blog  Calculus   Derivatives  Integrals  Limits    Matrices  Experimental Design  Practically Cheating Statistics Handbook  Navigation             Find the Mean of the Probability Distribution / Binomial    Binomial Theorem > How to find the mean of the probability distribution  Contents:   Mean of a probability distribution  Mean of a binomial (by hand or TI83)   Mean of a probability distribution How to find the mean of the probability distribution : Overview A normal distribution curve is one kind of probability distribution. Finding the mean of a probability distribution is easy in probability and statistics — if you know how. This how to will guide you through a few simple steps necessary to find the mean of the probability distribution or binomial distribution . You’ll often find these types of questions in textbook chapters on binomial probability distribution. The binomial distribution is just a simple trial where there are two outcomes: success or failure. For example, if you are counting how many times you draw an Ace from a deck of cards, you could assign “Success” to “Drawing an Ace” and “Failure” to drawing any other card. You can find the mean of the probability distribution by creating a probability table. How to find the mean of the probability distribution: Steps Sample question : “A grocery store has determined that in crates of tomatoes,  95% carry no rotten tomatoes, 2% carry one rotten tomato, 2% carry two rotten tomatoes, and 1% carry three rotten tomatoes. Find the mean number of rotten tomatoes in the crates.”  Step 1:  Convert all the percentages to decimal probabilities . For example: 95% = .95 2% = .02 2% = .02 1% = .01  Step 2:  Construct a probability distribution table . (If you don’t know how to do this, see how to construct a probability distribution ) .)   Step 3:  Multiply the values in each column. (In other words, multiply each value of X by each probability P(X).) Referring to our probability distribution table: 0 × .95 = 0 1 × .02  = .02 2 × .02  = .04 3 × .01 = .03  Step 4:  Add the results from step 3 together . 0 + .02 + .04 + .03 = .09 is the mean.  You’re done finding the mean for a probability distribution! Mean of Binomial Distribution A coin toss is a simple binomial experiment . A binomial distribution represents the results from a simple experiment where there is “success” or “failure.” For example, if you are polling voters to see who is voting Democrat, the voters that say they will vote Democrat is a “success” and anything else is a failure. One of the simplest binomial experiments you can perform is a coin toss, where “heads” could equal “success” and “tails” could equal “failure.” The mean of binomial distribution is much like the mean of anything else. It answers the question “If you perform this experiment many times, what’s the likely (the average) result?. Formula for Mean of Binomial Distribution The formula for the mean of binomial distribution is:  μ = n *p Where “n” is the number of trials and “p” is the probability of success. For example: if you tossed a coin 10 times to see how many heads come up, your probability is .5 (i.e. you have a 50 percent chance of getting a heads and 50 percent chance of a tails) and “n” is how many trials — 10. Therefore, the mean of this particular binomial distribution is: 10 * .5 = 5. This makes sense: if you toss a coin ten times you would expect heads to show up on average, 5 times. Mean for a Binomial Distribution on the TI-83 Sample problem : Find the mean for a binomial distribution with n = 5 and p = 0.12. Again, the TI 83 doesn’t have a function for this. But if you know the formula (n*p), it’s pretty easy to enter it on the home screen.  Step 1: Multiply n by p. 5 * .12 ENTER  =.6 Hey, that was easy! Something to think about: You may be wondering why it was so easy to calculate the mean. After all being asked to “calculate the mean for a binomial distribution” sounds scary. If you think about what a mean (or average ) is, then you’ll see why it was so easy. In the sample question, n = 5 and p = 0.12. What is “n”? That’s the number of items. So imagine a list of 5 items with a certain score: 1 = 0.12 2 = 0.12 3 = 0.12 4 = 0.12 5 = 0.12 If you were asked to find the average score for those five items, you wouldn’t even have to do the math: it’s just 0.12, right? Finding the mean for a binomial distribution is just a little different: you add up all of the probabilities (0.12 + 0.12 + 0.12 + 0.12 + 0.12). Or a faster way, just multiply n by p. Check out our Youtube channel for more help and stats tips!     If you prefer an online interactive environment to learn R and statistics, this free R Tutorial by Datacamp is a great way to get started. If you're are somewhat comfortable with R and are interested in going deeper into Statistics, try this Statistics with R track . Facebook page Find the Mean of the Probability Distribution / Binomial was last modified: October 15th, 2017 by Stephanie   By Stephanie | August 24, 2009 | Statistics How To |   ← Construct a probability distribution in Easy Steps  Binomial experiment: How to figure out what is and what isn’t →   4 thoughts on “ Find the Mean of the Probability Distribution / Binomial ”      Selin Asar May 30, 2016 at 9:20 am    A building venice is designed by taking 20  years return period water level. a) Calculate the probability of water level exceeding the design value less than 2 times in 20 years with Binomial distrubition. b)Calculate the probability of water level exceeding the design value more than 2 times in 60 years with Poisson distrubition.          Andale May 30, 2016 at 10:39 am    Were you given any probabilities for the water level exceeding the design value? Without this info, the question is impossible to answer.          M N Choudhary May 5, 2017 at 3:48 am    Assume a banks 95% value at risk model is perfectly accurate,If daily losses are independent, what is the probability that the number of daily losses exceeds the Var on exactly 5 days out of the previous 100 trading days.          Madison September 27, 2017 at 7:17 pm    This helps so much! Thanks!         Find an article   Search       Feel like "cheating" at Statistics? Check out the grade-increasing book that's recommended reading at top universities!                 Privacy policy.     Copyright © 2018 Statistics How To Theme by: Theme Horse Powered by: WordPress    Back to Top          Probability distribution   From Wikipedia, the free encyclopedia   Jump to: navigation , search   This article is about probability distributions. For generalized functions in mathematical analysis, see Distribution (mathematics) . For other uses, see Distribution .         This article has multiple issues. Please help improve it or discuss these issues on the talk page . ( Learn how and when to remove these template messages )         This article includes a list of references , related reading or external links , but its sources remain unclear because it lacks inline citations . Please help to improve this article by introducing more precise citations.  (July 2011)  ( Learn how and when to remove this template message )           This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed.  (July 2011)  ( Learn how and when to remove this template message )       ( Learn how and when to remove this template message )     In probability theory and statistics , a probability distribution is a mathematical function that, stated in simple terms, can be thought of as providing the probabilities of occurrence of different possible outcomes in an experiment . For instance, if the random variable  X is used to denote the outcome of a coin toss ("the experiment"), then the probability distribution of X would take the value 0.5 for X = heads , and 0.5 for X = tails (assuming the coin is fair).  In more technical terms, the probability distribution is a description of a random phenomenon in terms of the probabilities of events . Examples of random phenomena can include the results of an experiment or survey . A probability distribution is defined in terms of an underlying sample space , which is the set of all possible outcomes of the random phenomenon being observed. The sample space may be the set of real numbers or a higher-dimensional vector space , or it may be a list of non-numerical values; for example, the sample space of a coin flip would be {heads, tails} .  Probability distributions are generally divided into two classes. A discrete probability distribution (applicable to the scenarios where the set of possible outcomes is discrete , such as a coin toss or a roll of dice) can be encoded by a discrete list of the probabilities of the outcomes, known as a probability mass function . On the other hand, a continuous probability distribution (applicable to the scenarios where the set of possible outcomes can take on values in a continuous range (e.g. real numbers), such as the temperature on a given day) is typically described by probability density functions (with the probability of any individual outcome actually being 0). The normal distribution is a commonly encountered continuous probability distribution. More complex experiments, such as those involving stochastic processes defined in continuous time , may demand the use of more general probability measures .  A probability distribution whose sample space is the set of real numbers is called univariate , while a distribution whose sample space is a vector space is called multivariate . A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution ) gives the probabilities of a random vector – a list of two or more random variables – taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution , the hypergeometric distribution , and the normal distribution . The multivariate normal distribution is a commonly encountered multivariate distribution.     Contents    1  Introduction  2  Terminology   2.1  Basic terms    3  Cumulative distribution function  4  Discrete probability distribution   4.1  Measure theoretic formulation  4.2  Cumulative distribution function  4.3  Delta-function representation  4.4  Indicator-function representation    5  Continuous probability distribution  6  Some properties  7  Kolmogorov definition  8  Random number generation  9  Applications  10  Common probability distributions   10.1  Related to real-valued quantities that grow linearly (e.g. errors, offsets)  10.2  Related to positive real-valued quantities that grow exponentially (e.g. prices, incomes, populations)  10.3  Related to real-valued quantities that are assumed to be uniformly distributed over a (possibly unknown) region  10.4  Related to Bernoulli trials (yes/no events, with a given probability)  10.5  Related to categorical outcomes (events with K possible outcomes, with a given probability for each outcome)  10.6  Related to events in a Poisson process (events that occur independently with a given rate)  10.7  Related to the absolute values of vectors with normally distributed components  10.8  Related to normally distributed quantities operated with sum of squares (for hypothesis testing)  10.9  Useful as conjugate prior distributions in Bayesian inference    11  See also  12  References  13  External links     Introduction [ edit ]     The probability mass function (pmf) p ( S ) specifies the probability distribution for the sum S of counts from two dice . For example, the figure shows that p (11) = 1/18. The pmf allows the computation of probabilities of events such as P ( S > 9) = 1/12 + 1/18 + 1/36 = 1/6, and all other probabilities in the distribution.    To define probability distributions for the simplest cases, one needs to distinguish between discrete and continuous  random variables . In the discrete case, it is sufficient to specify a probability mass function      p    {\displaystyle p}   assigning a probability to each possible outcome: for example, when throwing a fair dice , each of the six values 1 to 6 has the probability 1/6. The probability of an event is then defined to be the sum of the probabilities of the outcomes that satisfy the event; for example, the probability of the event "the dice rolls an even value" is       p  (  2  )  +  p  (  4  )  +  p  (  6  )  =  1   /   6  +  1   /   6  +  1   /   6  =  1   /   2.    {\displaystyle p(2)+p(4)+p(6)=1/6+1/6+1/6=1/2.}     In contrast, when a random variable takes values from a continuum then typically, any individual outcome has probability zero and only events that include infinitely many outcomes, such as intervals, can have positive probability. For example, the probability that a given object weighs exactly 500 g is zero, because the probability of measuring exactly 500 g tends to zero as the accuracy of our measuring instruments increases. Nevertheless, in quality control one might demand that the probability of a "500 g" package containing between 490 g and 510 g should be no less than 98%, and this demand is less sensitive to the accuracy of measurement instruments.  Continuous probability distributions can be described in several ways. The probability density function describes the infinitesimal probability of any given value, and the probability that the outcome lies in a given interval can be computed by integrating the probability density function over that interval. On the other hand, the cumulative distribution function describes the probability that the random variable is no larger than a given value; the probability that the outcome lies in a given interval can be computed by taking the difference between the values of the cumulative distribution function at the endpoints of the interval. The cumulative distribution function is the antiderivative of the probability density function provided that the latter function exists.     The probability density function (pdf) of the normal distribution , also called Gaussian or "bell curve", the most important continuous random distribution. As notated on the figure, the probabilities of intervals of values correspond to the area under the curve.    Terminology [ edit ]  As probability theory is used in quite diverse applications, terminology is not uniform and sometimes confusing. The following terms are used for non-cumulative probability distribution functions:   Frequency distribution : A frequency distribution is a table that displays the frequency of various outcomes in a sample .  Relative frequency distribution : A frequency distribution where each value has been divided (normalized) by a number of outcomes in a sample i.e. sample size.  Probability distribution : Sometimes used as an alias for Relative frequency distribution but most books use it as a limit to which Relative frequency distribution tends when sample size tends to population size. It's a general term to indicate the way the total probability of 1 is distributed over all various possible outcomes (i.e. over entire population). It may for instance refer to a table that displays the probabilities of various outcomes in a finite population or to the probability density of an uncountably infinite population.  Cumulative distribution function : is a general functional form to describe a probability distribution.  Probability distribution function : somewhat ambiguous term sometimes referring to a functional form of probability distribution table. Could be called a "normalized frequency distribution function", where area under the graph equals to 1.  Probability mass , Probability mass function , p.m.f. , Discrete probability distribution function : for discrete random variables.  Categorical distribution : for discrete random variables with a finite set of values.  Probability density , Probability density function , p.d.f. , Continuous probability distribution function : most often reserved for continuous random variables.   The following terms are somewhat ambiguous as they can refer to non-cumulative or cumulative distributions, depending on authors' preferences:   Probability distribution function : continuous or discrete, non-cumulative or cumulative .  Probability function : even more ambiguous, can mean any of the above or other things.   Basic terms [ edit ]   Mode : for a discrete random variable, the value with highest probability (the location at which the probability mass function has its peak); for a continuous random variable, a location at which the probability density function has a local peak.  Support : the smallest closed set whose complement has probability zero.  Head : the range of values where the pmf or pdf is relatively high.  Tail : the complement of the head within the support; the large set of values where the pmf or pdf is relatively low.  Expected value or mean : the weighted average of the possible values, using their probabilities as their weights; or the continuous analog thereof.  Median : the value such that the set of values less than the median, and the set greater than the median, each have probabilities no greater than one-half.  Variance : the second moment of the pmf or pdf about the mean; an important measure of the dispersion of the distribution.  Standard deviation : the square root of the variance, and hence another measure of dispersion.  Symmetry : a property of some distributions in which the portion of the distribution to the left of a specific value is a mirror image of the portion to its right.  Skewness : a measure of the extent to which a pmf or pdf "leans" to one side of its mean. The third standardized moment of the distribution.  Kurtosis : a measure of the "fatness" of the tails of a pmf or pdf. The fourth standardized moment of the distribution.   Cumulative distribution function [ edit ]  Because a probability distribution P on the real line is determined by the probability of a scalar random variable X being in a half-open interval (−∞, x ], the probability distribution is completely characterized by its cumulative distribution function :       F  (  x  )  =  P  ⁡  [  X  ≤  x  ]    for all   x  ∈   R   .    {\displaystyle F(x)=\operatorname {P} [X\leq x]\qquad {\text{ for all }}x\in \mathbb {R} .}     Discrete probability distribution [ edit ]  See also: Probability mass function and Categorical distribution     The probability mass function of a discrete probability distribution. The probabilities of the singletons {1}, {3}, and {7} are respectively 0.2, 0.5, 0.3. A set not containing any of these points has probability zero.       The cdf of a discrete probability distribution, ...       ... of a continuous probability distribution, ...       ... of a distribution which has both a continuous part and a discrete part.    A discrete probability distribution is a probability distribution characterized by a probability mass function . Thus, the distribution of a random variable  X is discrete, and X is called a discrete random variable , if        ∑   u    P  ⁡  (  X  =  u  )  =  1    {\displaystyle \sum _{u}\operatorname {P} (X=u)=1}     as u runs through the set of all possible values of X . A discrete random variable can assume only a finite or countably infinite number of values. For the number of potential values to be countably infinite, even though their probabilities sum to 1, the probabilities have to decline to zero fast enough. For example, if     P  ⁡  (  X  =  n  )  =     1   2   n         {\displaystyle \operatorname {P} (X=n)={\tfrac {1}{2^{n}}}}   for n = 1, 2, ..., we have the sum of probabilities 1/2 + 1/4 + 1/8 + ... = 1.  Well-known discrete probability distributions used in statistical modeling include the Poisson distribution , the Bernoulli distribution , the binomial distribution , the geometric distribution , and the negative binomial distribution . Additionally, the discrete uniform distribution is commonly used in computer programs that make equal-probability random selections between a number of choices.  When a sample (a set of observations) is drawn from a larger population, the sample points have an empirical distribution that is discrete and that provides information about the population distribution.  Measure theoretic formulation [ edit ]  A measurable function      X  :  A  →  B    {\displaystyle X\colon A\to B}   between a probability space      (  A  ,    A    ,  P  )    {\displaystyle (A,{\mathcal {A}},P)}   and a measurable space      (  B  ,    B    )    {\displaystyle (B,{\mathcal {B}})}   is called a discrete random variable provided its image is a countable set and the pre-image of singleton sets are measurable, i.e.,      X   −  1    (  b  )  ∈    A      {\displaystyle X^{-1}(b)\in {\mathcal {A}}}   for all     b  ∈  B    {\displaystyle b\in B}   . The latter requirement induces a probability mass function       f   X    :  X  (  A  )  →   R     {\displaystyle f_{X}\colon X(A)\to \mathbb {R} }   via      f   X    (  b  )  :=  P  (   X   −  1    (  b  )  )    {\displaystyle f_{X}(b):=P(X^{-1}(b))}   . Since the pre-images of disjoint sets are disjoint        ∑   b  ∈  X  (  A  )     f   X    (  b  )  =   ∑   b  ∈  X  (  A  )    P  (   X   −  1    (  b  )  )  =  P   (    ⋃   b  ∈  X  (  A  )     X   −  1    (  b  )   )   =  P  (  A  )  =  1.    {\displaystyle \sum _{b\in X(A)}f_{X}(b)=\sum _{b\in X(A)}P(X^{-1}(b))=P\left(\bigcup _{b\in X(A)}X^{-1}(b)\right)=P(A)=1.}     This recovers the definition given above.  Cumulative distribution function [ edit ]  Equivalently to the above, a discrete random variable can be defined as a random variable whose cumulative distribution function (cdf) increases only by jump discontinuities —that is, its cdf increases only where it "jumps" to a higher value, and is constant between those jumps. The points where jumps occur are precisely the values which the random variable may take.  Delta-function representation [ edit ]  Consequently, a discrete probability distribution is often represented as a generalized probability density function involving Dirac delta functions , which substantially unifies the treatment of continuous and discrete distributions. This is especially useful when dealing with probability distributions involving both a continuous and a discrete part.  Indicator-function representation [ edit ]  For a discrete random variable X , let u 0 , u 1 , ... be the values it can take with non-zero probability. Denote        Ω   i    =   X   −  1    (   u   i    )  =  {  ω  :  X  (  ω  )  =   u   i    }  ,   i  =  0  ,  1  ,  2  ,  …    {\displaystyle \Omega _{i}=X^{-1}(u_{i})=\{\omega :X(\omega )=u_{i}\},\,i=0,1,2,\dots }     These are disjoint sets , and for such sets       P   (    ⋃   i     Ω   i     )   =   ∑   i    P  (   Ω   i    )  =   ∑   i    P  (  X  =   u   i    )  =  1.    {\displaystyle P\left(\bigcup _{i}\Omega _{i}\right)=\sum _{i}P(\Omega _{i})=\sum _{i}P(X=u_{i})=1.}     It follows that the probability that X takes any value except for u 0 , u 1 , ... is zero, and thus one can write X as       X  (  ω  )  =   ∑   i     u   i     1    Ω   i      (  ω  )    {\displaystyle X(\omega )=\sum _{i}u_{i}1_{\Omega _{i}}(\omega )}     except on a set of probability zero, where      1   A      {\displaystyle 1_{A}}   is the indicator function of A . This may serve as an alternative definition of discrete random variables.  Continuous probability distribution [ edit ]  See also: Probability density function  A continuous probability distribution is a probability distribution that has a cumulative distribution function that is continuous. Most often they are generated by having a probability density function . Mathematicians call distributions with probability density functions absolutely continuous , since their cumulative distribution function is absolutely continuous with respect to the Lebesgue measure  λ . If the distribution of X is continuous, then X is called a continuous random variable . There are many examples of continuous probability distributions: normal , uniform , chi-squared , and others .  Intuitively, a continuous random variable is the one which can take a continuous range of values —as opposed to a discrete distribution, where the set of possible values for the random variable is at most countable . While for a discrete distribution an event with probability zero is impossible [ citation needed ] (e.g., rolling π on a standard die has probability zero and is impossible), this is not so in the case of a continuous random variable. For example, if one measures the width of an oak leaf, the result of 3½ cm is possible; however, it has probability zero because uncountably many other potential values exist even between 3 cm and 4 cm. Each of these individual outcomes has probability zero, yet the probability that the outcome will fall into the interval  (3 cm, 4 cm) is nonzero. This apparent paradox is resolved by the fact that the probability that X attains some value within an infinite set, such as an interval, cannot be found by naively adding the probabilities for individual values. Formally, each value has an infinitesimally small probability, which statistically is equivalent to zero. [ citation needed ]  Formally, if X is a continuous random variable, then it has a probability density function  ƒ ( x ), and therefore its probability of falling into a given interval, say [ a , b ] is given by the integral       P  ⁡  [  a  ≤  X  ≤  b  ]  =   ∫   a    b    f  (  x  )   d  x    {\displaystyle \operatorname {P} [a\leq X\leq b]=\int _{a}^{b}f(x)\,dx}     In particular, the probability for X to take any single value a (that is a ≤ X ≤ a ) is zero, because an integral with coinciding upper and lower limits is always equal to zero.  The definition states that a continuous probability distribution must possess a density, or equivalently, its cumulative distribution function be absolutely continuous. This requirement is stronger than simple continuity of the cumulative distribution function, and there is a special class of distributions, singular distributions , which are neither continuous nor discrete nor a mixture of those. An example is given by the Cantor distribution . Such singular distributions however are never encountered in practice.  Note on terminology: some authors use the term "continuous distribution" to denote the distribution with continuous cumulative distribution function. Thus, their definition includes both the (absolutely) continuous and singular distributions.  By one convention, a probability distribution      μ    {\displaystyle \,\mu }   is called continuous if its cumulative distribution function     F  (  x  )  =  μ  (  −  ∞  ,  x  ]    {\displaystyle F(x)=\mu (-\infty ,x]}   is continuous and, therefore, the probability measure of singletons     μ  {  x  }   =   0    {\displaystyle \mu \{x\}\,=\,0}   for all      x    {\displaystyle \,x}   .  Another convention reserves the term continuous probability distribution for absolutely continuous distributions. These distributions can be characterized by a probability density function : a non-negative Lebesgue integrable function      f    {\displaystyle \,f}   defined on the real numbers such that       F  (  x  )  =  μ  (  −  ∞  ,  x  ]  =   ∫   −  ∞    x    f  (  t  )   d  t  .    {\displaystyle F(x)=\mu (-\infty ,x]=\int _{-\infty }^{x}f(t)\,dt.}     Discrete distributions and some continuous distributions (like the Cantor distribution ) do not admit such a density.  Some properties [ edit ]   The probability distribution of the sum of two independent random variables is the convolution of each of their distributions.  Probability distributions are not a vector space —they are not closed under linear combinations , as these do not preserve non-negativity or total integral 1—but they are closed under convex combination , thus forming a convex subset of the space of functions (or measures).   Kolmogorov definition [ edit ]  Main articles: Probability space and Probability measure  In the measure-theoretic formalization of probability theory , a random variable is defined as a measurable function  X from a probability space       (  Ω  ,    F    ,  P  )     {\displaystyle \scriptstyle (\Omega ,{\mathcal {F}},\operatorname {P} )}   to measurable space      (    X    ,    A    )     {\displaystyle \scriptstyle ({\mathcal {X}},{\mathcal {A}})}   . A probability distribution of X is the pushforward measure  X * P  of X , which is a probability measure on      (    X    ,    A    )     {\displaystyle \scriptstyle ({\mathcal {X}},{\mathcal {A}})}   satisfying X * P = P X  −1 .  Random number generation [ edit ]  Main article: Pseudo-random number sampling  A frequent problem in statistical simulations (the Monte Carlo method ) is the generation of pseudo-random numbers that are distributed in a given way. Most algorithms are based on a pseudorandom number generator that produces numbers X that are uniformly distributed in the half-open interval [0,1). These random variates  X are then transformed via some algorithm to create a new random variate having the required probability distribution.  Applications [ edit ]  The concept of the probability distribution and the random variables which they describe underlies the mathematical discipline of probability theory, and the science of statistics. There is spread or variability in almost any value that can be measured in a population (e.g. height of people, durability of a metal, sales growth, traffic flow, etc.); almost all measurements are made with some intrinsic error; in physics many processes are described probabilistically, from the kinetic properties of gases to the quantum mechanical description of fundamental particles . For these and many other reasons, simple numbers are often inadequate for describing a quantity, while probability distributions are often more appropriate.  As a more specific example of an application, the cache language models and other statistical language models used in natural language processing to assign probabilities to the occurrence of particular words and word sequences do so by means of probability distributions.  Common probability distributions [ edit ]  Main article: List of probability distributions  The following is a list of some of the most common probability distributions, grouped by the type of process that they are related to. For a more complete list, see list of probability distributions , which groups by the nature of the outcome being considered (discrete, continuous, multivariate, etc.)  Note also that all of the univariate distributions below are singly peaked; that is, it is assumed that the values cluster around a single point. In practice, actually observed quantities may cluster around multiple values. Such quantities can be modeled using a mixture distribution .  Related to real-valued quantities that grow linearly (e.g. errors, offsets) [ edit ]   Normal distribution (Gaussian distribution), for a single such quantity; the most common continuous distribution   Related to positive real-valued quantities that grow exponentially (e.g. prices, incomes, populations) [ edit ]   Log-normal distribution , for a single such quantity whose log is normally distributed  Pareto distribution , for a single such quantity whose log is exponentially distributed; the prototypical power law distribution   Related to real-valued quantities that are assumed to be uniformly distributed over a (possibly unknown) region [ edit ]   Discrete uniform distribution , for a finite set of values (e.g. the outcome of a fair die)  Continuous uniform distribution , for continuously distributed values   Related to Bernoulli trials (yes/no events, with a given probability) [ edit ]   Basic distributions:  Bernoulli distribution , for the outcome of a single Bernoulli trial (e.g. success/failure, yes/no)  Binomial distribution , for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed total number of independent occurrences  Negative binomial distribution , for binomial-type observations but where the quantity of interest is the number of failures before a given number of successes occurs  Geometric distribution , for binomial-type observations but where the quantity of interest is the number of failures before the first success; a special case of the negative binomial distribution    Related to sampling schemes over a finite population:  Hypergeometric distribution , for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, using sampling without replacement  Beta-binomial distribution , for the number of "positive occurrences" (e.g. successes, yes votes, etc.) given a fixed number of total occurrences, sampling using a Polya urn scheme (in some sense, the "opposite" of sampling without replacement )     Related to categorical outcomes (events with K possible outcomes, with a given probability for each outcome) [ edit ]   Categorical distribution , for a single categorical outcome (e.g. yes/no/maybe in a survey); a generalization of the Bernoulli distribution  Multinomial distribution , for the number of each type of categorical outcome, given a fixed number of total outcomes; a generalization of the binomial distribution  Multivariate hypergeometric distribution , similar to the multinomial distribution , but using sampling without replacement ; a generalization of the hypergeometric distribution   Related to events in a Poisson process (events that occur independently with a given rate) [ edit ]   Poisson distribution , for the number of occurrences of a Poisson-type event in a given period of time  Exponential distribution , for the time before the next Poisson-type event occurs  Gamma distribution , for the time before the next k Poisson-type events occur   Related to the absolute values of vectors with normally distributed components [ edit ]   Rayleigh distribution , for the distribution of vector magnitudes with Gaussian distributed orthogonal components. Rayleigh distributions are found in RF signals with Gaussian real and imaginary components.  Rice distribution , a generalization of the Rayleigh distributions for where there is a stationary background signal component. Found in Rician fading of radio signals due to multipath propagation and in MR images with noise corruption on non-zero NMR signals.   Related to normally distributed quantities operated with sum of squares (for hypothesis testing) [ edit ]   Chi-squared distribution , the distribution of a sum of squared standard normal variables; useful e.g. for inference regarding the sample variance of normally distributed samples (see chi-squared test )  Student's t distribution , the distribution of the ratio of a standard normal variable and the square root of a scaled chi squared variable; useful for inference regarding the mean of normally distributed samples with unknown variance (see Student's t-test )  F-distribution , the distribution of the ratio of two scaled chi squared variables; useful e.g. for inferences that involve comparing variances or involving R-squared (the squared correlation coefficient )   Useful as conjugate prior distributions in Bayesian inference [ edit ]  Main article: Conjugate prior   Beta distribution , for a single probability (real number between 0 and 1); conjugate to the Bernoulli distribution and binomial distribution  Gamma distribution , for a non-negative scaling parameter; conjugate to the rate parameter of a Poisson distribution or exponential distribution , the precision (inverse variance ) of a normal distribution , etc.  Dirichlet distribution , for a vector of probabilities that must sum to 1; conjugate to the categorical distribution and multinomial distribution ; generalization of the beta distribution  Wishart distribution , for a symmetric non-negative definite matrix; conjugate to the inverse of the covariance matrix of a multivariate normal distribution ; generalization of the gamma distribution   See also [ edit ]    Statistics portal     Copula (statistics)  Empirical probability  Histogram  Joint probability distribution  Likelihood function  List of statistical topics  Kirkwood approximation  Moment-generating function  Quasiprobability distribution  Riemann–Stieltjes integral application to probability theory   References [ edit ]    B. S. Everitt: The Cambridge Dictionary of Statistics , Cambridge University Press , Cambridge (3rd edition, 2006). ISBN  0-521-69027-7  Bishop: Pattern Recognition and Machine Learning , Springer , ISBN  0-387-31073-8  den Dekker A. J., Sijbers J., (2014) "Data distributions in magnetic resonance images: a review", Physica Medica , [1]   External links [ edit ]     Wikimedia Commons has media related to Probability distribution .     Hazewinkel, Michiel , ed. (2001) [1994], "Probability distribution" , Encyclopedia of Mathematics , Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN  978-1-55608-010-4   Field Guide to Continuous Probability Distributions , Gavin E. Crooks.         v  t  e    Probability distributions      List     Discrete univariate with finite support     Benford  Bernoulli  beta-binomial  binomial  categorical  hypergeometric  Poisson binomial  Rademacher  discrete uniform  Zipf  Zipf–Mandelbrot       Discrete univariate with infinite support     beta negative binomial  Borel  Conway–Maxwell–Poisson  discrete phase-type  Delaporte  extended negative binomial  Gauss–Kuzmin  geometric  logarithmic  negative binomial  parabolic fractal  Poisson  Skellam  Yule–Simon  zeta       Continuous univariate supported on a bounded interval     arcsine  ARGUS  Balding–Nichols  Bates  beta  beta rectangular  Irwin–Hall  Kumaraswamy  logit-normal  noncentral beta  raised cosine  reciprocal  triangular  U-quadratic  uniform  Wigner semicircle       Continuous univariate supported on a semi-infinite interval     Benini  Benktander 1st kind  Benktander 2nd kind  beta prime  Burr  chi-squared  chi  Dagum  Davis  exponential-logarithmic  Erlang  exponential  F  folded normal  Flory–Schulz  Fréchet  gamma  gamma/Gompertz  generalized inverse Gaussian  Gompertz  half-logistic  half-normal  Hotelling's T -squared  hyper-Erlang  hyperexponential  hypoexponential  inverse chi-squared   scaled inverse chi-squared    inverse Gaussian  inverse gamma  Kolmogorov  Lévy  log-Cauchy  log-Laplace  log-logistic  log-normal  Lomax  matrix-exponential  Maxwell–Boltzmann  Maxwell–Jüttner  Mittag-Leffler  Nakagami  noncentral chi-squared  Pareto  phase-type  poly-Weibull  Rayleigh  relativistic Breit–Wigner  Rice  shifted Gompertz  truncated normal  type-2 Gumbel  Weibull   Discrete Weibull    Wilks's lambda       Continuous univariate supported on the whole real line     Cauchy  exponential power  Fisher's z  Gaussian q  generalized normal  generalized hyperbolic  geometric stable  Gumbel  Holtsmark  hyperbolic secant  Johnson's S U  Landau  Laplace  asymmetric Laplace  logistic  noncentral t  normal (Gaussian)  normal-inverse Gaussian  skew normal  slash  stable  Student's t  type-1 Gumbel  Tracy–Widom  variance-gamma  Voigt       Continuous univariate with support whose type varies     generalized extreme value  generalized Pareto  Marchenko–Pastur  q-exponential  q-Gaussian  q-Weibull  shifted log-logistic  Tukey lambda       Mixed continuous-discrete univariate     rectified Gaussian       Multivariate (joint)     Discrete  Ewens  multinomial  Dirichlet-multinomial  negative multinomial  Continuous  Dirichlet  generalized Dirichlet  multivariate Laplace  multivariate normal  multivariate stable  multivariate t  normal-inverse-gamma  normal-gamma  Matrix-valued  inverse matrix gamma  inverse-Wishart  matrix normal  matrix t  matrix gamma  normal-inverse-Wishart  normal-Wishart  Wishart       Directional     Univariate (circular) directional  Circular uniform  univariate von Mises  wrapped normal  wrapped Cauchy  wrapped exponential  wrapped asymmetric Laplace  wrapped Lévy  Bivariate (spherical)  Kent  Bivariate (toroidal)  bivariate von Mises  Multivariate  von Mises–Fisher  Bingham       Degenerate and singular     Degenerate  Dirac delta function  Singular  Cantor       Families     Circular  compound Poisson  elliptical  exponential  natural exponential  location–scale  maximum entropy  mixture  Pearson  Tweedie  wrapped              v  t  e    Theory of probability distributions        probability mass function (pmf)  probability density function (pdf)  cumulative distribution function (cdf)  quantile function             raw moment  central moment  mean  variance  standard deviation  skewness  kurtosis  L-moment          moment-generating function (mgf)  characteristic function  probability-generating function (pgf)  cumulant  combinant              v  t  e    Statistics        Outline  Index            Descriptive statistics         Continuous data      Center     Mean   arithmetic  geometric  harmonic    Median  Mode       Dispersion     Variance  Standard deviation  Coefficient of variation  Percentile  Range  Interquartile range       Shape     Central limit theorem  Moments   Skewness  Kurtosis  L-moments            Count data     Index of dispersion       Summary tables     Grouped data  Frequency distribution  Contingency table       Dependence     Pearson product-moment correlation  Rank correlation   Spearman's rho  Kendall's tau    Partial correlation  Scatter plot       Graphics     Bar chart  Biplot  Box plot  Control chart  Correlogram  Fan chart  Forest plot  Histogram  Pie chart  Q–Q plot  Run chart  Scatter plot  Stem-and-leaf display  Radar chart                  Data collection         Study design     Population  Statistic  Effect size  Statistical power  Sample size determination  Missing data       Survey methodology     Sampling   stratified  cluster    Standard error  Opinion poll  Questionnaire       Controlled experiments     Design   control  optimal    Controlled trial  Randomized  Random assignment  Replication  Blocking  Interaction  Factorial experiment       Uncontrolled studies     Observational study  Natural experiment  Quasi-experiment                  Statistical inference         Statistical theory     Population  Statistic  Probability distribution  Sampling distribution   Order statistic    Empirical distribution   Density estimation    Statistical model   L p space    Parameter   location  scale  shape    Parametric family   Likelihood  (monotone)  Location–scale family  Exponential family    Completeness  Sufficiency  Statistical functional   Bootstrap  U  V    Optimal decision   loss function    Efficiency  Statistical distance   divergence    Asymptotics  Robustness       Frequentist inference      Point estimation     Estimating equations   Maximum likelihood  Method of moments  M-estimator  Minimum distance    Unbiased estimators   Mean-unbiased minimum-variance   Rao–Blackwellization  Lehmann–Scheffé theorem    Median unbiased    Plug-in       Interval estimation     Confidence interval  Pivot  Likelihood interval  Prediction interval  Tolerance interval  Resampling   Bootstrap  Jackknife         Testing hypotheses     1- & 2-tails  Power   Uniformly most powerful test    Permutation test   Randomization test    Multiple comparisons       Parametric tests     Likelihood-ratio  Wald  Score          Specific tests         Z -test (normal)  Student's t -test  F -test       Goodness of fit     Chi-squared  G -test  Kolmogorov–Smirnov  Anderson–Darling  Lilliefors  Jarque–Bera  Normality (Shapiro–Wilk)  Likelihood-ratio test  Model selection   Cross validation  AIC  BIC         Rank statistics     Sign   Sample median    Signed rank (Wilcoxon)   Hodges–Lehmann estimator    Rank sum (Mann–Whitney)  Nonparametric  anova   1-way (Kruskal–Wallis)  2-way (Friedman)  Ordered alternative (Jonckheere–Terpstra)            Bayesian inference     Bayesian probability   prior  posterior    Credible interval  Bayes factor  Bayesian estimator   Maximum posterior estimator                       Correlation  Regression analysis            Correlation     Pearson product-moment  Partial correlation  Confounding variable  Coefficient of determination       Regression analysis     Errors and residuals  Regression model validation  Mixed effects models  Simultaneous equations models  Multivariate adaptive regression splines (MARS)       Linear regression     Simple linear regression  Ordinary least squares  General linear model  Bayesian regression       Non-standard predictors     Nonlinear regression  Nonparametric  Semiparametric  Isotonic  Robust  Heteroscedasticity  Homoscedasticity       Generalized linear model     Exponential families  Logistic (Bernoulli) / Binomial / Poisson regressions       Partition of variance     Analysis of variance (ANOVA, anova)  Analysis of covariance  Multivariate ANOVA  Degrees of freedom                  Categorical / Multivariate / Time-series / Survival analysis         Categorical     Cohen's kappa  Contingency table  Graphical model  Log-linear model  McNemar's test       Multivariate     Regression  Manova  Principal components  Canonical correlation  Discriminant analysis  Cluster analysis  Classification  Structural equation model   Factor analysis    Multivariate distributions   Elliptical distributions   Normal           Time-series      General     Decomposition  Trend  Stationarity  Seasonal adjustment  Exponential smoothing  Cointegration  Structural break  Granger causality       Specific tests     Dickey–Fuller  Johansen  Q-statistic (Ljung–Box)  Durbin–Watson  Breusch–Godfrey       Time domain     Autocorrelation (ACF)   partial (PACF)    Cross-correlation (XCF)  ARMA model  ARIMA model (Box–Jenkins)  Autoregressive conditional heteroskedasticity (ARCH)  Vector autoregression (VAR)       Frequency domain     Spectral density estimation  Fourier analysis  Wavelet  Whittle likelihood          Survival      Survival function     Kaplan–Meier estimator (product limit)  Proportional hazards models  Accelerated failure time (AFT) model  First hitting time       Hazard function     Nelson–Aalen estimator       Test     Log-rank test                     Applications         Biostatistics     Bioinformatics  Clinical trials / studies  Epidemiology  Medical statistics       Engineering statistics     Chemometrics  Methods engineering  Probabilistic design  Process / quality control  Reliability  System identification       Social statistics     Actuarial science  Census  Crime statistics  Demography  Econometrics  National accounts  Official statistics  Population statistics  Psychometrics       Spatial statistics     Cartography  Environmental statistics  Geographic information system  Geostatistics  Kriging                Category  Portal  Commons   WikiProject           Authority control     LCCN : sh85038545  NDL : 00564751             Retrieved from " https://en.wikipedia.org/w/index.php?title=Probability_distribution&oldid=833130857 "  Categories : Probability distributions Mathematical and quantitative methods (economics) Hidden categories: Articles lacking in-text citations from July 2011 All articles lacking in-text citations Articles needing additional references from July 2011 All articles needing additional references Articles with multiple maintenance issues All articles with unsourced statements Articles with unsourced statements from March 2018 Wikipedia articles with LCCN identifiers      Navigation menu    Personal tools   Not logged in Talk Contributions Create account Log in      Namespaces   Article Talk       Variants           Views   Read Edit View history      More         Search              Navigation    Main page Contents Featured content Current events Random article Donate to Wikipedia Wikipedia store      Interaction    Help About Wikipedia Community portal Recent changes Contact page      Tools    What links here Related changes Upload file Special pages Permanent link Page information Wikidata item Cite this page      Print/export    Create a book Download as PDF Printable version      In other projects    Wikimedia Commons      Languages    العربية বাংলা Беларуская Català Čeština Deutsch Eesti Ελληνικά Español Esperanto Euskara فارسی Français Galego 한국어 Հայերեն Bahasa Indonesia Italiano עברית ქართული Latina Lietuvių Magyar Македонски Nederlands 日本語 Norsk Norsk nynorsk Polski Português Română Русский Scots සිංහල Simple English Slovenščina Basa Sunda Suomi Svenska Tagalog தமிழ் ไทย Türkçe Українська اردو Tiếng Việt 中文   Edit links        This page was last edited on 29 March 2018, at 20:34.  Text is available under the Creative Commons Attribution-ShareAlike License ;
additional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.    Privacy policy  About Wikipedia  Disclaimers  Contact Wikipedia  Developers  Cookie statement  Mobile view                               1.  Exploratory Data Analysis   1.3.  EDA Techniques   1.3.6.  Probability Distributions        1.3.6.1.    What is a Probability Distribution            Discrete Distributions    The mathematical definition of a discrete probability function,
p(x), is a function that satisfies the following properties.  The probability that x can take a specific value is p(x).
       That is  \[ P[X = x] = p(x) = p_{x} \]  p(x) is non-negative for all real x.  The sum of p(x) over all possible values of x is 1, that is  \[ \sum_{j}p_{j} = 1 \] where j represents all possible values that x can have and p j is the
       probability at x j . One consequence of properties 2 and 3 is that 
       0 <= p(x) <= 1. What does this actually mean?  A discrete probability function is a
function that can take a discrete number of values (not necessarily
finite).  This is most often the non-negative integers or some subset
of the non-negative integers.  There is no mathematical restriction
that discrete probability functions only be defined at integers, but
in practice this is usually what makes sense.  For example, if
you toss a coin 6 times, you can get 2 heads or 3 heads but not
2 1/2 heads.  Each of the discrete values has a certain probability
of occurrence that is between zero and one.  That is, a discrete
function that allows negative values or values greater than one is
not a probability function.  The condition that the probabilities
sum to one means that at least one of the values has to occur.        Continuous Distributions    The mathematical definition of a continuous probability function, f(x),
is a function that satisfies the following properties.  The probability that x is between two points a and b is  \[ p[a \le x \le b] = \int_{a}^{b} {f(x)dx} \]  It is non-negative for all real x.  The integral of the probability function is one, that is  \[ \int_{-\infty}^{\infty} {f(x)dx} = 1 \]  What does this actually mean?  Since continuous probability
functions are defined for an infinite number of points over a
continuous interval, the probability at a single point is always
zero.  Probabilities are measured over intervals, not single points.
That is, the area under the curve between two distinct points
defines the probability for that interval.  This means that the
height of the probability function can in fact be greater than one.
The property that the integral must equal one is equivalent to
the property for discrete distributions that the sum of all the
probabilities must equal one.        Probability Mass Functions Versus Probability Density Functions    Discrete probability functions are referred to as probability mass
functions and continuous probability functions are referred to as
probability density functions.  The term probability functions
covers both discrete and continuous distributions. There are a few occasions in the e-Handbook when we use the
term probability density function in a generic sense where it may
apply to either probability density or probability mass functions.
It should be clear from the context whether we are referring only
to continuous distributions or to either continuous or discrete
distributions.                     Jump to navigation     ×  Home  mojo  GCSE  A-Level  Timetable  What's new  Log in / Register         User menu Contact  Log in                       S-cool, the revision website           ≡                 Probability Distribution Tables                 You are here A-level » Maths » Probability Distributions        Register Free  Start revising A-level & GCSE with 7 million other students  FREE Revision guides, questions banks and resources  60% of members achieve a A*-B Grade   Enrol Now »         Expectation and Variance      *Please note: you may not see animations, interactions or images that are potentially on this page because you have not allowed Flash to run on S-cool. To do this, click here. *     Probability Distribution Tables       What is a discrete random variable?  A random variable is a variable which takes numerical values and whose value depends on the outcome of an experiment. It is discrete if it can only take certain values.  Example:  Suppose you roll a die and let X be the number on the uppermost face. Then the values that can be obtained are discrete random variables.  x = 1, 2, 3, 4, 5 or 6  The probability of obtaining a 1 can be written as P(X = 1) = 1/6  The probability of obtaining a 2 can be written as P(X = 2) =  1/6  And so on for all the values of x, P(X = x).  If we list all these probabilities into a table, we get the probability distribution of X .      x  1  2  3  4  5  6    P(X = x)  1/6  1/6  1/6  1/6  1/6  1/6      Note: Capital letters are used to denote the random variables, whereas lower case letters are used to denote the values that can be obtained.  In the above example we covered every possible event when rolling a die. We must get one of the six events listed in the table. These events are exclusive events and always sum to 1.  ∑ P(X = x) = 1  Example:  Suppose you flip 2 coins and let X be the discrete random variable of the number of heads obtained. Write down the probability distribution of X.  First we need to know what values of x can be obtained.  Clearly, x = 0, 1 or 2, as we can either get no heads, 1 head or 2 heads.   Our next step is to calculate the probability of each (if you have trouble with this then go and have a look at the Probability topic).  P(X = 0) = P(tail and tail) = ½ × ½ = ¼  P(X = 1) = P(tail and head) or P(head and tail) = ½ × ½ +  ½ × ½ = ½  P(X = 2) = P(head and head) = ½ × ½ = ¼  We now put these results into a table for the probability distribution of X...      x  0  1  2    P(X = x)  ¼  ½  ¼      Note: The better you are at calculating probabilities, the quicker and easier these problems become. You should be able to write down the probability distribution of a discrete random variable with minimal workings.   Probability density function   Sometimes we are given a formula to calculate probabilities. We call this the probability density function of X or the p.d.f. of X .  Example:  The probability density function of a discrete random variable X is given by...  P(X = x) = kx 3  For x = 0, 1 or 2 where k is a constant.  Find the probability distribution of X.  P(X = 0) = k × 0 3 = 0  P(X = 1) = k × 1 3 = k  P(X = 2) = k × 2 3 = 8k  So:      x  0  1  2    P(X =x)  0  k  8k      From earlier, we know:  ∑ P(X = x) = 1  ( Remember: ∑ means 'sum of')  Therefore, 0 + k + 8k = 1  9k = 1  Hence k =  1/9  So the probability distribution of X is:      x  0  1  2    P(X = x)  0  1/9  8/9       Cumulative distribution function   'Cumulative' gives us a kind of running total so a cumulative distribution function gives us a running total of probabilities within our probability table.  The cumulative distribution function, F(x) of X is defined as:  F(x) = P(X ≤ x)  Example:  The probability distribution of a discrete random variable, X, is given by:      x  1  2  3  4  5    F(x)  0.2  0.43  0.62  0.8  1      Reading the table we see that:  P(X ≤ 3) = 0.62  P(X ≤ 2) = 0.43  This means to calculate a single probability we proceed as follows:      P(X = 3)  = P(X ≤ 3) − P(X ≤ 2)    = 0.62 − 0.43    = 0.19      For probabilities of 'greater than' :      P(X >2)  = 1 - F(2)    = 1 − P(X ≤ 2)    = 1 − 0.43    = 0.57      Question: Try for yourself  For the above example, see if you can work out the following:           Just click "Find out more" and get £10 off your first tutorial             Expectation and Variance                 Show me                                 Main menu Home  mojo  GCSE  A-Level  Timetable  What's new  Log in / Register       Search form   Search                     Advertise  Contact us  Log in  Terms & Conditions                  Copyright © 2018 S-cool Youth Marketing Limited                       The Binomial Distribution In many cases, it is appropriate to summarize a group of independent observations by the 
number of observations in the group that represent one of two outcomes.  For example, the 
proportion of individuals in a random sample who support one of two political candidates fits
this description.  In this case, the statistic  is the count  X of voters who support
the candidate divided by the total number of individuals in the group n .  This provides an 
estimate of the parameter  p , the proportion of 
individuals who support the candidate in the entire population. The binomial distribution X 1: The number of observations n is fixed. 2: Each observation is independent. 3: Each observation represents one of two outcomes ("success" or "failure"). 4: The probability of "success" p is the same for each outcome. X n p B(n,p)  Example  Suppose individuals with a certain gene have a 0.70 probability of eventually contracting 
a certain disease.  If 100 individuals with the gene participate in a lifetime study, then the 
distribution of the random variable describing the number of individuals who will contract the
disease is distributed B(100,0.7) .  Note: The sampling distribution of a count variable is only well-described by the binomial
distribution is cases where the population size is significantly larger than the sample size.  
As a general rule, the binomial distribution should not be applied to observations from 
a simple random sample (SRS) unless the 
population size is at least 10 times larger than the sample size. To find probabilities from a binomial distribution, one may either calculate them directly,
use a binomial table, or use a computer.  The number of sixes rolled by a single die in 20
rolls has a B(20,1/6) distribution.  The probability of rolling more than 2 sixes
in 20 rolls, P(X>2) , is equal to 1 - P(X < 2) = 1 - (P(X=0) + P(X=1) + 
P(X=2)) . Using the MINITAB command "cdf" with subcommand "binomial n=20 p=0.166667" gives the cumulative
distribution function as follows: Binomial with n = 20 and p = 0.166667

         x     P( X <= x)
         0        0.0261
         1        0.1304
         2        0.3287
         3        0.5665
         4        0.7687
         5        0.8982
         6        0.9629
         7        0.9887
         8        0.9972
         9        0.9994 The corresponding graphs for the probability density function and cumulative distribution function
for the B(20,1/6) distribution are shown below:  Since the probability of 2 or fewer sixes is equal to 0.3287, the probability of rolling more than
2 sixes = 1 - 0.3287 = 0.6713.  The probability that a random variable X with binomial distribution B(n,p) is
equal to the value k , where k = 0, 1,....,n , is given by  , where . The latter expression is known as the binomial coefficient n choose k k n one Mean and Variance of the Binomial Distribution X n p n Z Z p mean 1*p + 0*(1-p) = p variance p(1-p). n Z These definitions are intuitively logical.  Imagine, for example, 8 flips
of a coin.  If the coin is fair, then p = 0.5.  One would expect the 
mean number of heads to be half the flips, or np = 8*0.5 = 4.  The
variance is equal to np(1-p) = 8*0.5*0.5 = 2. Sample Proportions If we know that the count X of "successes" in a group of n observations with
sucess probability p has a binomial distribution with mean np and variance np(1-p) , then we are able to derive information about the distribution of the sample proportion X n X/n X n np/n = p unbiased estimator p X/n X n² (np(1-p))/n² = (p(1-p))/n In the example of rolling a six-sided die 20 times, the probability p of rolling
a six on any roll is 1/6, and the count X of sixes has a B(20, 1/6) distribution. 
The mean of this distribution is 20/6 = 3.33, and the variance is 20*1/6*5/6 = 100/36 = 2.78. 
The mean of the proportion of sixes in the 20 rolls, X/20 , is equal to p = 1/6 = 0.167, and the variance of the proportion is equal to (1/6*5/6)/20 = 0.007.  Normal Approximations for Counts and Proportions  For large values of n , the distributions of the count X and the sample proportion are approximately normal .
This result follows from the Central Limit Theorem .
The mean and variance for the approximately normal distribution of X are np and np(1-p) , identical to the mean and variance of the binomial( n,p ) distribution.
Similarly, the mean and variance for the approximately normal distribution of the sample
proportion are p and (p(1-p)/n) .   Note: Because the normal approximation is not accurate for small values of n , a good rule of 
thumb is to use the normal approximation only if np > 10 and np(1-p) > 10.  For example, consider a population of voters in a given state.  The true proportion of voters who 
favor candidate A is equal to 0.40.  Given a sample of 200 voters, what is the probability that
more than half of the voters support candidate A? The count X of voters in the sample of 200 who support candidate A is distributed B(200,0.4) .  The mean of the distribution is equal to 200*0.4 = 80, and the variance is equal
to 200*0.4*0.6 = 48.  The standard deviation is the square root of the variance, 6.93.  The 
probability that more than half of the voters in the sample support candidate A is equal to
the probability that X is greater than 100, which is equal to 1- P(X < 100). To use the normal approximation to calculate this probability, we should first acknowledge that
the normal distribution is continuous and apply the continuity correction .
This means that the probability for a single discrete value, such as 100, is extended to the 
probability of the interval (99.5,100.5).  Because we are interested in the probability
that X is less than or equal to 100, the normal approximation applies to the upper limit
of the interval, 100.5.  If we were interested in the probability that X is strictly less
than 100, then we would apply the normal approximation to the lower end of the interval, 99.5. So, applying the continuity correction and standardizing the variable X gives the following: 1 - P(X < 100) = 1 - P(X < 100.5) = 1 - P(Z < (100.5 - 80)/6.93) = 1 - P(Z < 20.5/6.93) = 1 - P(Z < 2.96) = 1 - (0.9985) = 0.0015.  Since the value 100 is nearly three
standard deviations away from the mean 80, the probability of observing a count this high is 
extremely small.  RETURN TO MAIN PAGE .      Skip navigation       Sign in Search         Loading...            Close           Yeah, keep it  Undo  Close       This video is unavailable.         Watch Queue Queue Watch Queue Queue   Remove all Disconnect         The next video is starting stop    Loading...          Watch Queue  Queue  __count__/__total__                       Tired of ads?       Loading...      Want music and videos with zero ads? Get YouTube Red.         Working...            Not now  Try it free                                                     Find out why Close       The Probability Distribution Function (PDF) of [X]                 MIT OpenCourseWare              Loading...        Unsubscribe from MIT OpenCourseWare?    Cancel  Unsubscribe           Working...            Subscribe Subscribed Unsubscribe 1.5M             Loading...          Loading...              Working...                Add to   Want to watch this again later?  Sign in to add this video to a playlist.  Sign in    Share   More      Report    Need to report the video?  Sign in to report inappropriate content.  Sign in       Transcript      Statistics     Add translations   88,603 views         248   Like this video?  Sign in to make your opinion count.  Sign in     249    11   Don't like this video?  Sign in to make your opinion count.  Sign in     12            Loading...            Loading...      Transcript      The interactive transcript could not be loaded.         Loading...         Loading...       Rating is available when the video has been rented.     This feature is not available right now. Please try again later.      Published on Feb 26, 2014 MIT 6.041SC Probabilistic Systems Analysis and Applied Probability, Fall 2013 View the complete course: http://ocw.mit.edu/6-041SCF13 Instructor: Jimmy Li License: Creative Commons BY-NC-SA More information at http://ocw.mit.edu/terms More courses at http://ocw.mit.edu     Category   Education     License   Standard YouTube License       Show more  Show less       Loading...                 Autoplay    When autoplay is enabled, a suggested video will automatically play next.       Up next       Calculating a Cumulative Distribution Function (CDF)  - Duration: 8:44.  MIT OpenCourseWare  195,515 views     8:44               PDF and CDF Explanations  - Duration: 3:15.  UAMath115  136,421 views     3:15      Probability density functions | Probability and Statistics | Khan Academy  - Duration: 10:02.  Khan Academy  1,410,043 views     10:02      Probability density functions (KristaKingMath)  - Duration: 7:01.  Krista King  84,066 views     7:01      FRM: Terms about distributions: PDF, PMF and CDF  - Duration: 9:58.  Bionic Turtle  96,082 views     9:58      PMF of a Function of a Random Variable  - Duration: 15:26.  MIT OpenCourseWare  65,828 views     15:26      Find the Probability Density Function for Continuous Distribution of Random Variable  - Duration: 9:53.  Anil Kumar  18,342 views     9:53      Continuous Random Variables: Probability Density Functions  - Duration: 23:16.  MrNichollTV  111,584 views     23:16      Probability Mass Function and Probability Density Function  - Duration: 13:15.  ProfessorSerna  9,667 views     13:15      Why I Left My $100,000+ Job at Google  - Duration: 3:00.  CS Dojo  2,657,335 views     3:00      Probability functions: pdf, CDF and inverse CDF (FRM T2-1)  - Duration: 20:34.  Bionic Turtle  3,294 views     20:34      A Derived Distribution Example  - Duration: 9:30.  MIT OpenCourseWare  7,091 views     9:30      Probability Distrubtion Functions and Expected Value  - Duration: 13:46.  V Anusic  4,114 views     13:46      probability density functions and cumulative distribution functions s1  - Duration: 6:25.  Rhys Steele  95,847 views     6:25      Probability Density Functions / Continuous Random Variables  - Duration: 8:32.  patrickJMT  384,462 views     8:32      Continuous Random Variables: Cumulative Distribution Functions  - Duration: 25:47.  MrNichollTV  84,506 views     25:47      Probability Density Functions (Introduction) : ExamSolutions  - Duration: 12:12.  ExamSolutions  118,681 views     12:12      Normal distribution's probability density function derived in 5min  - Duration: 4:50.  acadelivery  41,949 views     4:50      Probability Density Functions (Example 1) : ExamSolutions  - Duration: 7:36.  ExamSolutions  112,055 views     7:36      A Beginner’s Guide To Quantum Computing  - Duration: 17:58.  Coding Tech  325,660 views     17:58     Loading more suggestions...   Show more                Language: English     Location: United States     Restricted Mode: Off    History  Help     Loading...       Loading...       Loading...     About  Press  Copyright  Creators  Advertise  Developers  +YouTube   Terms  Privacy  Policy & Safety  Send feedback   Test new features              Loading...               Working...             Sign in to add this to Watch Later    Add to      Loading playlists...                Skip navigation       Sign in Search         Loading...            Close           Yeah, keep it  Undo  Close       This video is unavailable.         Watch Queue Queue Watch Queue Queue   Remove all Disconnect         The next video is starting stop    Loading...          Watch Queue  Queue  __count__/__total__                       Ditch the ads.       Loading...      Want music and videos with zero ads? Get YouTube Red.         Working...            Not now  Try it free                                                     Find out why Close       The Probability Distribution Function (PDF) of [X]                 MIT OpenCourseWare              Loading...        Unsubscribe from MIT OpenCourseWare?    Cancel  Unsubscribe           Working...            Subscribe Subscribed Unsubscribe 1.5M             Loading...          Loading...              Working...                Add to   Want to watch this again later?  Sign in to add this video to a playlist.  Sign in    Share   More      Report    Need to report the video?  Sign in to report inappropriate content.  Sign in       Transcript      Statistics     Add translations   88,603 views         248   Like this video?  Sign in to make your opinion count.  Sign in     249    11   Don't like this video?  Sign in to make your opinion count.  Sign in     12            Loading...            Loading...      Transcript      The interactive transcript could not be loaded.         Loading...         Loading...       Rating is available when the video has been rented.     This feature is not available right now. Please try again later.      Published on Feb 26, 2014 MIT 6.041SC Probabilistic Systems Analysis and Applied Probability, Fall 2013 View the complete course: http://ocw.mit.edu/6-041SCF13 Instructor: Jimmy Li License: Creative Commons BY-NC-SA More information at http://ocw.mit.edu/terms More courses at http://ocw.mit.edu     Category   Education     License   Standard YouTube License       Show more  Show less       Loading...                 Autoplay    When autoplay is enabled, a suggested video will automatically play next.       Up next       Calculating a Cumulative Distribution Function (CDF)  - Duration: 8:44.  MIT OpenCourseWare  195,515 views     8:44               Probability density functions | Probability and Statistics | Khan Academy  - Duration: 10:02.  Khan Academy  1,410,043 views     10:02      FRM: Terms about distributions: PDF, PMF and CDF  - Duration: 9:58.  Bionic Turtle  96,082 views     9:58      PDF and CDF Explanations  - Duration: 3:15.  UAMath115  136,421 views     3:15      Probability density functions (KristaKingMath)  - Duration: 7:01.  Krista King  84,066 views     7:01      PMF of a Function of a Random Variable  - Duration: 15:26.  MIT OpenCourseWare  65,828 views     15:26      Find the Probability Density Function for Continuous Distribution of Random Variable  - Duration: 9:53.  Anil Kumar  18,342 views     9:53      Continuous Random Variables: Probability Density Functions  - Duration: 23:16.  MrNichollTV  111,584 views     23:16      Probability functions: pdf, CDF and inverse CDF (FRM T2-1)  - Duration: 20:34.  Bionic Turtle  3,294 views     20:34      Probability Mass Function and Probability Density Function  - Duration: 13:15.  ProfessorSerna  9,667 views     13:15      Why I Left My $100,000+ Job at Google  - Duration: 3:00.  CS Dojo  2,657,335 views     3:00      Probability Distrubtion Functions and Expected Value  - Duration: 13:46.  V Anusic  4,114 views     13:46      Probability Density Functions / Continuous Random Variables  - Duration: 8:32.  patrickJMT  384,462 views     8:32      Stats: Finding Probability Using a Normal Distribution Table  - Duration: 11:23.  poysermath  849,697 views     11:23      Finding a CDF from a pdf  - Duration: 12:00.  Mr Camilleri  70,272 views     12:00      5. Discrete Random Variables I  - Duration: 50:35.  MIT OpenCourseWare  121,059 views     50:35      Continuous Random Variables: Cumulative Distribution Functions  - Duration: 25:47.  MrNichollTV  84,506 views     25:47      probability density functions and cumulative distribution functions s1  - Duration: 6:25.  Rhys Steele  95,847 views     6:25      A Derived Distribution Example  - Duration: 9:30.  MIT OpenCourseWare  7,091 views     9:30      Normal distribution's probability density function derived in 5min  - Duration: 4:50.  acadelivery  41,949 views     4:50     Loading more suggestions...   Show more                Language: English     Location: United States     Restricted Mode: Off    History  Help     Loading...       Loading...       Loading...     About  Press  Copyright  Creators  Advertise  Developers  +YouTube   Terms  Privacy  Policy & Safety  Send feedback   Test new features              Loading...               Working...             Sign in to add this to Watch Later    Add to      Loading playlists...          